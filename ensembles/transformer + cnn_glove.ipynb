{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer + cnn_glove.ipynb","provenance":[{"file_id":"1IpfMkmtFHT_w4HBS3uNOGM26Apw0aZVz","timestamp":1582798698410},{"file_id":"1Om7GVDag19A4DMFLrCW1n1Lq1XjSr_j9","timestamp":1580323733414}],"collapsed_sections":[],"authorship_tag":"ABX9TyM7/qwIOwIdRMOVojS4jc1W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yi1b9Svhk-Qy","colab_type":"code","outputId":"ff50fac9-8280-487d-b8c8-0f824eca2516","executionInfo":{"status":"ok","timestamp":1582798765504,"user_tz":-330,"elapsed":25873,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WN6w7wL-b-eI","colab_type":"code","outputId":"a2fced69-9bb8-49cc-f410-24f8cc11f058","executionInfo":{"status":"ok","timestamp":1582809812072,"user_tz":-330,"elapsed":799,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/My\\ Drive/NLP/cnn-text-classification-pytorch/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP/cnn-text-classification-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1BWsbXx2sziz","colab_type":"code","outputId":"3abe0ee9-060a-4eba-84e4-e16a7850cae4","executionInfo":{"status":"ok","timestamp":1582809818153,"user_tz":-330,"elapsed":5370,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["custom_mapping.pickle  model.py                        rt-polaritydata.tar\n","LICENSE                mydatasets.py                   \u001b[0m\u001b[01;34msnapshot\u001b[0m/\n","main.py                \u001b[01;34m__pycache__\u001b[0m/                    train.py\n","mapping.pickle         README.md                       \u001b[01;34mtry1\u001b[0m/\n","model_glove_2.pth      \u001b[01;34mrt-polaritydata\u001b[0m/                val_glove.pickle\n","model_glove.pth        rt-polaritydata.README.1.0.txt  val_set.pickle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hy0G-z3qtS7W","colab_type":"code","outputId":"eabdea2f-f10b-4c44-fac9-26f3020f3bc5","executionInfo":{"status":"ok","timestamp":1582809818880,"user_tz":-330,"elapsed":5791,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from collections import defaultdict\n","from nltk.corpus import wordnet as wn\n","import numpy as np\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","\n","np.random.seed(500)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zeZbGHV0vqq1","colab_type":"code","colab":{}},"source":["path = \"../data/Subtask-1-master/train.csv\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fYrLUqwM0Sn","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","train_corpus = pd.read_csv(\"../train_train_1.csv\", encoding='utf-8')\n","test_corpus = pd.read_csv(\"../train_val_1.csv\", encoding='utf-8')\n","corpus = pd.read_csv(path, encoding='utf-8')\n","\n","\n","train_indices = []\n","test_indices = []\n","for sent in train_corpus['sentenceID']:\n","  train_indices.append(sent%100000)\n","for sent in test_corpus['sentenceID']:\n","  test_indices.append(sent%100000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3xrkiWBTTRf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"outputId":"dd57534d-de1b-43f4-9aae-8e25eed2e770","executionInfo":{"status":"ok","timestamp":1582809818884,"user_tz":-330,"elapsed":5077,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["corpus['sentenceID']%100000"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0            0\n","1            1\n","2            2\n","3            3\n","4            4\n","         ...  \n","12995    12995\n","12996    12996\n","12997    12997\n","12998    12998\n","12999    12999\n","Name: sentenceID, Length: 13000, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"MM8beogbv6Pp","colab_type":"code","colab":{}},"source":["corpus['sentence'].dropna(inplace=True)\n","corpus['sentence'] = [sent.lower() for sent in corpus['sentence']]\n","corpus['sentence'] = [word_tokenize(word) for word in corpus['sentence']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBFmeMBJwG-K","colab_type":"code","outputId":"09bc2817-5f23-466d-bb5d-a84e23b56180","executionInfo":{"status":"ok","timestamp":1582809821879,"user_tz":-330,"elapsed":7671,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["corpus['sentence']"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        [goodfellow, 's, theory, has, been, questioned...\n","1        [however, ,, both, campaigners, and, pro-peopl...\n","2        [things, could, have, been, even, better, if, ...\n","3        [the, new, request, ,, if, approved, ,, would,...\n","4        [companies, in, financial, difficulty, can, cu...\n","                               ...                        \n","12995    [the, department, said, it, conducts, thorough...\n","12996    [the, mccain, episode, may, sound, largely, ha...\n","12997    [merkel, said, in, a, speech, to, her, conserv...\n","12998    [``, ``, the, entire, drug, scene, has, change...\n","12999    [in, 2009, ,, the, group, said, adults, should...\n","Name: sentence, Length: 13000, dtype: object"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yGyKQsjswY-0","colab_type":"code","colab":{}},"source":["# glove\n","\n","w2v = []\n","w2i = {}\n","words = []\n","\n","idx = 0\n","with open('../data/glove.6B.300d.txt','rb') as f:\n","  for l in f:\n","    line = l.decode().split()\n","    word =  line[0]\n","    words.append(word)\n","    w2i[word] = idx\n","    idx +=1\n","    vect = np.array(line[1:]).astype(np.float)\n","    w2v.append(vect)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYvk9ssU4_mS","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '...' in sent[j]:\n","      corpus['sentence'][i][j] = sent[j].replace('...','')\n","    if '..' in sent[j]:\n","      corpus['sentence'][i][j] = sent[j].replace('..','')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kff8lmP5BThE","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '-' in sent[j]:\n","      x = sent[j].split('-')\n","      corpus['sentence'][i][j] = x[0]\n","      if j+1 == len(sent):\n","        corpus['sentence'][i].append(x[1])\n","      else:\n","        corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"668r-osICj4s","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '/' in sent[j]:\n","      x = sent[j].split('/')\n","      corpus['sentence'][i][j] = x[0]\n","      if len(x)>1:\n","        corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8WVCaYeC_rl","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if \"'\" in sent[j]:\n","      if sent[j] not in words:\n","        corpus['sentence'][i][j] = sent[j].replace(\"'\",\"\")\n","      # x = sent[j].split('-')\n","      # corpus['sentence'][i][j] = x[0]\n","      # corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKHBegoNEXZa","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if \".\" in sent[j]:\n","      if sent[j] not in words:\n","        if len(sent[j].split('.')) > 2 or len(sent[j].split('.')) == 1:\n","          corpus['sentence'][i][j] = sent[j].replace(\".\",\"\")\n","        else:\n","          if len(sent[j].split('.')) == 2:\n","            x = sent[j].split('.')\n","            corpus['sentence'][i][j] = x[0]\n","            corpus['sentence'][i].insert(j+1, x[1])\n","            # print(corpus['sentence'][i][j], corpus['sentence'][i][j+1], '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcYTE0JpxTs7","colab_type":"code","colab":{}},"source":["not_in = []\n","for sent in corpus['sentence']:\n","  for word in sent:\n","    if word not in words:\n","      if word not in not_in:\n","        not_in.append(word)\n","        # print(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dspSDLi08Lz","colab_type":"code","outputId":"347817aa-d7bf-49fa-d5bf-c3464479444a","executionInfo":{"status":"ok","timestamp":1582809933659,"user_tz":-330,"elapsed":118142,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(not_in)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1381"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"kicQpyKfE2Ps","colab_type":"code","outputId":"0d14282e-1e95-4445-f4a5-0dd31599fb6d","executionInfo":{"status":"ok","timestamp":1582809933662,"user_tz":-330,"elapsed":117966,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(not_in)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["['kingdomof', '', 'forwardnot', 'stakethe', 'nonemergencies', 'circumastance', 'r-nc', 'reynaga', 'irisin', 'cuntry', 'shkreli', 'thatdespite', 'forcehe', 'ntgr', 'bfsr', 'ceramides', 'otillar', 'scratchpad', 'mcritchie', 'enria', 'watchwordalthough', 'mosychuk', 'aedpa', 'dipierro', 'panelo', 'arconic', 'paynet', 'gorsuchs', 'suicidenow', 'kyrsten', 'freiborg', '5bn', 'depomed', 'amsellem', 'overperformances', 'thcb', 'privitera', 'legimately', 'burkan', 'zonszein', '10,554', 'brochez', 'pfapa', 'bingener', 'entresto', 'candidateand', 'zarxio', 'tdn2k', 'confounders', 'overcounted', 'rebirthed', 'altmaier', 'barcap', 'shithole', 'lignos', 'sciple', 'morens', 'reclosing', 'roadfor', 'unpeg', 'currencythe', 'somepart', 'allthat', 'staehr', 'baumblit', 'khorsand', 'innereye', 'topkill', '1+1', 'kilimnik', 'fralick', 'krautzberger', 'truog', 'luján', 'screenos', 'unbooked', 'ohions', 'sowanick', 'neurobiologically', 'killino', 'incel', 'nn27303398', 'rcep', 'piampongsant', 'oubina', 'nontraumatic', '21e', 'coeure', 'rabten', 'reichlin', 'klipp', 'theranos', 'brexit', 'ionis', 'msct', '≥25kg', '≥30kg', 'müller', 'issakainen', 'blasey', 'elugardo', 'unrigging', '40bn', '100tn', 'deprescribing', 'mulye', 'ilbd', 'torba', 'shiono', 'vemurafenib', 'dacarbazine', 'lynparza', 'cholnoky', 'jeoff', 'gangwisch', 'bernanke*', 'pended', 'rejoiceit', 'staikouras', 'lewandowskis', 'carcinoembryonic', 'bosiers', 'haridy', 'urodynamic', 'alembik', 'dvn', 'melland', 'florange', 'brexiteer', 'colectomy', 'pay+', 'rosanoff', 'knols', 'jobsbut', 'siloed', 'preauthorization', 'gaddie', 'mansplaining', 'esiner', 'beggaring', 'iezzi', '60tn', 'townswick', 'guidancesome', 'icagen', 'ecohelmet', 'world�', '�', 'fscai', 'scai', 'schnautz', 'proration', 'incentivised', 'mdv3100', 'zytiga', 'medivation', 'melsen', 'nunamaker', 'tudose', 'shingrix', 'indexuniverse', 'vxx', 'washpost', 'karoun', 'securityholder', 'yger', 'google+', '11,493', 'certifiied', 'orkambi', 'en+', 'pshg_p', '112bn', 'slavedriver', 'isrg', 'sharice', 'jfcom', 'woahhhh', 'nktr', 'opdivo', 'yervoy', 'immunotherapies', 'recapitalizating', 'dierksen', 'ambarella', 'herefor', 'gamblersor', 'contestantsthat', 'schreckreports', 'erdogans', 'beencooked', 'golbon', 'eubut', 'clintonness', 'giunchigliani', 'eteplirsen', 'storiesseasonal', 'viprinex', 'avenatti', 'jarris', 'gilholm', 'nontradeable', 'oanda', 'prizeroot', 'systemthe', 'pescanova', 'weekdo', 'unsterilised', 'them�', 'zemcik', 'immunochemical', 'papyracea', 'morcellation', 'houte', 'firdapse', 'kronthal', 'passporting', 'rotasiil', 'patrikis', 'cyad', 'comeyi', 'misdemeanourand', 'lomen', 'hoerth', 'baxalta', 'engag', 'percec', 'dovere', 'provention', 't1d', '**a', 'zavecz', 'hoepa', 'inquicker', 'baytex', 'do�', 'whcra', 'wharen', 'randomising', 'analogise', 'thoughtssome', '630m', 'assetswhich', 'banksthe', 'fibrillating', 'defibrillated', 'conlumino', 'slutkin', 'tjokrosaputro', 'dailyfx', 'ohiohealth', 'littlefinger', 'monologuing', 'dontos', 'lemtrada', 'presal', 'hematospermia', 'guindos', 'continueon', 'trueness', 'queered', 'nfld', 'venoplasty', 'unbuyable', 'btig', 'kopola', 'mäkinen', '33bn', '7bn', 'mythaholic', 'venge', 'scardina', 'isis8', 'whadya', '£200m', '£420m', '£7', '6bn', '£23bn', 'misgoverning', 'tyrannically', 'sobanko', 'filmalter', '£3', 'investmentwhile', 'wordelectoral', 'healthkit', 'parscale', 'ransomware', 'zulily', 'léonora', 'chiappelli', 'vestager', 'agentswho', 'jobfeared', 'dehejia', 'idfc', 'thepast', 'thatbehaviourright', '£500m', 'wauthier', 'cubicin', 'otezla', 'receptos', 'lunacies', 'coxibs', 'xme', 'thiswhile', 'hq2', 'catellanos', 'indivior', 'durata', 'sciutto', 'bigotted', 'lamestream', 'starndard', 'lifepoint', 'lpnt', 'ridaforolimus', 'mortifications', 'chozick', 'cyberespionage', 'alperovitch', 'crowdstrike', 'hackable', 'nkc', 'jcar015', '£69', '4bn', '13057', 'fornero', 'circumsized', 'pneumoliner', 'morcellated', 'inácio', 'bromiley', 'storiesempty', 'bakries', 'daca', 'kanefield', 'shugerman', 'strategised', 'reznikova', 'liptovsky', 'hradok', 'patheon', 'highertwitter', 'nyag', 'unremedied', 'molepost', 'solanezumab', 'pmis', 'believehigh', 'grexit', 'broemer', 'reince', '£28m', 'affo', 'puigdemont', 'trumpism', 'sportsafe', 'redtech', 'ursano', '16bn', 'warmbrodt', '204k', 'esformes', 'toigo', 'valvuloplasty', 'usmca', 'wrongi', 'klepin', 'worldchecking', 'amzn', 'tdjc', 'lockmaker', 'timesour', 'pinebridge', 'valuationsdiageo', 'themchinese', 'solarfun', 'curveglobal', 'margina', 'governmetn', 'tmunity', 'realised�', 'pr+', 'hospitalinspections', 'geltzeiler', 'bloomgren', 'nicolás', 'werent', 'okuwobi', 'riskscore', 'numberedat', 'premorbid', 'porchia', 'nifla', 'examination-rooms', 'overprescribe', 'espriet', 'irmat', 'optum', 'mfglobal', 'cadburys', 'earliestthough', 'verlinvest', 'mulacek', 'kotagal', 'schrder', 'perritano', 'flesvig', 'argumentsindeed', 'nymann', 'cuccinellis', 'lipus', 'vapers', 'akorn', 'putcha', 'sier', 'ultrastrong', 'kitakura', 'hntes', 'in�', 'wechat', 'wateringly', 'skylanders', 'entegra', 'nn25160928', 'prough', 'inzitari', 'yarros', 'charanpreet', 'dabhia', 'cellseven', 'licencethen', 'darwinistic', 'infinera', 'espp', '17bn', 'whaled', 'farkouh', 'restuccia', 'deconditioned', 'manouch', 'moshayedi', 'phylopatric', 'altarum', 'yieldco', 'gilvarry', 'docmorris', 'spokeo', 'retweeted', 'wasendorf', 'keytruda', 'non-executives', 'odendahl', 'saysthat', 'malvertising', 'camcorp', 'egzzz', 'medpage', 'ahier', 'generalisable', 'discussant', 'ticc', 'makuch', 'marketmakers', 'kolfage', 'hruksa', 'as�', 'overpromises', 'unspared', 'douda', 'spikily', 'egert', 'poloz', 'bolsonaro', 'ungallant', '7,042', 'litonjua', 'self-judgments', 'kondik', 'arshamian', 'lumentum', 'vcsels', 'rohret', 'capitala', 'amcap', 'emanuels', 'resolutionsmeaning', 'levelshave', 'semaya', 'healthtour', 't1c', 'vipshop', 'betterof', 'alreadyyou', 'cirmo', 'epidiolex', 'dravet', 'asnider', 'wyen', 'himselfwell', 'educatedthe', 'jbarro', 'trauttenberg', 'moonshoti', 'mmscf', 'lobley', 'point72', 'cakeshop', 'juurlink', 'qualifyand', 'mnn', 'gerspach', 'atypia', 'sefl', '55bn', '£69bn', '£14bn', 'rednecky', 'iiea', 'phenotyping', 'tijirit', 'preyss', 'plco', '2618', 'rebastinib', '3014', 'andhe', 'whatyou', 'thespecialist', 'takenplace', 'assistantsfollows', 'saikawa', 'inflam', 'workvery', 'anothervendor', 'trns', 'knighthead', 'rgy', 'obfr', 'enlightnment', 'doubleline', 'vidscription', 'ycmnet', 'yoshikami', 'mitigo', 'background-check', 'ap7', 'ca125', 'acquiror', 'nayda', 'pancioli', 'fischell', 'pcmh', 'leoning', 'threatsnixons', 'allegationseven', 'avoidably', 'under-estimation', 'obozo', 'blizzardy', 'entrec', 'treatmenteven', 'likelywould', 'weensure', 'oversimplifyingthe', 'thecataract', 'distanceor', 'matons', 'cithian', 'meiburg', 'mdbi', 'simps', 'flotus', 'stabbe', 'efsm', 'superinfection', 'sedgh', 'laclair', 'design\\xaded', 'rel\\xadeased', 'an\\xadnoying', 'bec\\xadause', 'lyxor', 'profitslindsay', 'jrgen', 'microgrid', 'vitalmedix', 'dexcom', 'missika', 'stefin', 'sherbin', 'serper', 'timbouctou', 'distributory', 'antiatherogenic', 'marinuzzi', '*minneapolis', '*fed', 'expiredmeaning', 'e004', 'emmanouilidis', 'devincow', 'doomsters', 'bowone', 'lowflationary', '456,789', 'witlessly', '65bn', 'pricings', 'dtegn', 'interspersal', 'nadella', 'ibrutinib', 'attackonly', 'hollyfrontier', '59,480', 'peña', 'deprescribe', 'tradereporter', 'goitein', 'iome', 'vissers', 'nppc', 'tanona', 'morbidities', 'biamonte', 'rmds', 'ellmers', 'airfarewatchdog', 'excisional', 'cryptocurrency', 'coincheck', 'cataltepe', 'biomarin', '2x1012', '6,862', '72,000,000', 'flowback', 'vbp', 'bisaro', 'hollak', 'leadiant', 'prevenion', 'newsnew', 'sederer', 'medicareon', 'aflappin', 'wesee', 'november1', '34,517,250', 'skierka', 'scavino', 'kobre', 'goate', 'younkin', 'vengroff', 'answersis', 'lbbb', '28bn', 'weekwant', 'steidley', 'gonzález', '58bn', 'protégés', 'marsupialization', 'trutina', 'delreal', 'aideven', 'leronlimab', 'cd02', 'pcatest', 'hemauer', 'savolitinib', 'maoa', 'luthman', 'liontrust', 'emph', 'trumponomics', 'dismuke', 'cassivi', 'insurancenot', 'intrapartum', 'shiarly', 'hepatologist', 'gracas', 'heran', 'gphin', 'tedprize', 'faltersremember', 'tmfdeej', 'gothamist', 'kashoggi', 'reguire', 'cringeworthy', 'underwhelm', 'flavanol', 'donationsthis', 'nccn', 'mcaleenan', 'coalitionperhaps', 'springloaded', '35:34', 'reluctances', 'orexo', 'ltip', 'indexiq', 'overexert', 'leftciti', 'lagorio', 'sedran', 'gevo', 'tarnef', 'atiqi', 'grandparental', 'trembowicz', 'wyderko', 'botheration', 'creuzfeldt', 'bauler', 'mainfirst', 'soleoutcome', 'whenall', 'anecklace', 'thebest', 'ourchest', 'feedingtube', 'regionrussia', 'itone', 'corpbanca', 'oxybate', '12bn', '9bn', 'forward‐looking', '+2349051208634when', 'srouji', 'pehub', 'corbat', 'corrollary', 'casamassimo', 'lnder', 'answershow', 'nondevelopmental', 'glitazones', 'hyperinfection', 'banaei', 'kartsotis', 'rerunor', 'sherzan', 'ezechiel', 'copic', 'bcma', 'haughom', '2000including', 'oklahomathe', 'sivignon', 'bdsi', 'detasseling', 'cornpicking', 'judgy', 'meetingwithout', 'evidencethat', 'dumain', 'kapla', 'frideres', 'ossoff', '£2,800', 'contentthis', 'romneyworld', 'bbby', 'partcipate', 'evalulation', 'nematicides', 'sowhile', 'plansout', 'orderto', 'furthermarket', 'lesscompetition', 'thelegislation', 'propafenone', 'sotalol', 'dotard', 'avanir', 'nuedexta', '7751', '7733', '8035', '8036', 'exercycle', 'linewhile', 'shitstains', 'nonrenewal', 'somebodyie', 'avav', 'lukashevich', 'oosterveld', 'awfulizing', 'janumet', 'dixhouse', 'ingel', 'hb56', 'kwarteng', 'tv+', 'nflx', 'medcity', 'traister', 'redocument', 'chondrocyte', 'loncon', '£903m', 'aquadvantage', 'glennbeck', 'gbtv', 'douchiness', 'policywise', 'friedmen', 'ferreyr', 'flexitime', 'shaub', 'posthurricane', 'prehurricane', 'volunteersafter', 'texas.during', 'clavulanate', 'piperacillin', 'tazobactam', 'zosyn', 'bagios', 'wapo', 'snowblue', 'attachedmentsi', 'nebank', 'saczynski', 'crosstabs', 'aa+', 'neratinib', 'maelstroms', 'nongroup', 'netback', 'ncib', 'misattuned', 'ctbs', 'erbb1', 'capecitabine', 'urdan', 'unsubsidised', '11,770', '47,080', 'plassat', 'turnround', '€2', '45bn', 'rebok', 'beicker', 'senatewhich', 'pressit', 'jcpoa', 'boustani', 'peterschmitt', 'guessous', 'mediawatch', 'biegelsen', 'dbkgn', 'jiankui', 'bsps', '88m', 'pzena', 'sisolak', 'unblinded', 'olypmic', 'nonconvertible', 'pelosis', '22bn', 'rb51', 'gipsa', 'uromedica', 'dupilumab', 'telogen', 'effluvium', 'totalis', 'carbon-free', 'sotu', 'nellen', 'peginesatide', 'derrough', 'polle', 'baltensperger', 'chondroplasty', 'convo', 'salveson', 'ccdoe', 'senblumenthal', 'nflcommish', 'rucaparib', 'watergoing', 'dabby', 'pfass', '•olden', '2°c', 'deppression', 'kalanick', 'nonpharmacological', 'nonallergic', 'galuccio', 'toweljune', 'knowmr', 'trenowden', 'portyghee', 'shulkin', 'nevertrumpers', '£1,000', '£7,000', 'giegold', 'ablations', 'matousek', 'shawkut', 'factualwhat', 'injectioncannot', '32,898', 'takethe', 'rylee', 'coene', 'guaidó', 'benisek', 'prwora', 'cimzia', 'singiser', 'mor208', '18,378', 'concernedthe', 'partiespaused', 'cellectis', 'government–', 'eargo', 'slavitt', 'cimziaâ®', 'triangulator', 'mnsure', 'gablofen', 'olness', 'marette', 'swetnick', 'pnk', 'crème', 'brûlée', 'behsudi', 'zacary', 'techeven', 'tromps', 'huttenhower', '79mr', 'takushi', 'ishikura', 'aledade', 'mostashari', 'costscosts', 'topx', 'enestro', 'parsortix', 'eskesen', 'azpia', 'economicsit', 'sweatily', 'dkr53bn', 'czapp', 'drrich', 'manchik', 'seriousconsequences', 'abbvie', 'bondholdings', 'costerg', 'cyence', 'kalobios', 'brainlessly', 'reify', 'oookaay', 'chatbots', 'ameringen', 'csalp', 'ttip', 'realdonaldtrump', 'undisbursed', 'playbookers', 'mutualization', 'cimzia®', 'bouhara', 'foroud', 'buoyant�', 'foolfest', 'toomeys', 'fsoc', 'kolhmann', 'sp+', 'paymentsnot', 'guglielmotti', 'multigenic', 'nidawi', 'wergin', 'minusma', 'swedo', 'lozman', 'putinwho', 'poehling', '9,580', '12,902', 'otherhand', 'car-t', 'ecigarettes', '1trn', 'luschini', 'preteenagers', '£1,381', '£9,300', 'famewave', 'albence', 'toldme', 'whatthey', 'thatonly', 'orher', 'forcompetence', 'ipaa', 'ungari', 'sageen', 'drive-left', 'beaverness', 'runkeeper', 'kalir', 'heartwire', 'hernández', 'avocadoes', 'garzotto', 'youdid', 'iley', 'umich', 'biggish', 'unrevoked', 'kohane', 'prewarned', 'rutqvist', 'coinbase', 'ehrsam', 'blockchain', 'tomdispatch', 'slate®', '£20', '£2', 'financialisation', 'housingthe', 'redicting', 'cemma', 'caitlinzemma', '134m', '346bn', 'washeteria', 'ishrak', 'hoogendyk', 'adheretech', 'abruptlyas', 'truty', 'withthemselves', 'andstrike', 'leiomyosarcomas', 'woulndt', 'sederberg', '2650bc', 'edro', 'gloviczki', 'jived', '5thbday', 'obamacarethe', 'democratsthat', 'krzanich', '10nm', 'weightsit', 'seasonand', 'scailex', 'scix', 'linora', 'consonery', 'brexiteers', 'hypertriglyceridemic', 'sequencingin', 'stefanek', 'cordovilla', 'mandrola', '6,584', 'chrystia', '聳', '25:44', '45and', 'boeckler', 'npdb', 'glausser', 'ricol', 'ovitt', 'cpri', 'aqlan', '9202', 'qan', 'epocrates', 'drummy', 'schnatter', 'hawkinberry', 'shuanghui', 'josé', '£50m', 'polkes', 'untether', 'interstim', 'liposuctioned', 'cirka', 'horsager', 'lowest-yielding', 'yaradua', 'vsoe', 'cisgendered', 'zoetis', 'nonproven', 'schlapp', '244m', 'posess', 'razaqzada', 'spanberger', 'inspra', 'elseplease', 'tothem', 'becausei', 'nechelput', 'esvelt', 'presidentgreens', 'forthare', 'restasis', 'intromission', 'dodick', 'ngdp', 'lnkd', 'panl', 'lubitz', 'skinactivist', '£300bn', 'wandoan', '200bn', 'bokkelen', 'thinkfor', 'thefinancial', 'tohim', 'fragmentedsystem', 'systemwould', 'treattheir', 'changethe', 'cispa', 'morethe', 'tmall', 'micrometastases', 'jumbomatic', 'dourson', 'dealbut', 'birtel', 'deplorables', 'weightsif', 'balwani', 'suahasil', 'nazara', '£22bn', 'zanny', 'flatto', 'culpritboth', 'unobligated', 'vilnerable', 'psychoanalyzing', 'ftfm', 'smartwatch', 'sabbs', 'coratti', 'omfif', 'loréal', 'nawana', 'wage-and-benefit', 'afib', 'acase', 'surgeonleft', 'publicityconcerning', 'iwatch', 'proco', 'zarzeczny', 'giventhough', 'hafle', 'demétrio', 'postuma', 'jannetta', 'chemed', 'glendevon', 'hampshirenew', 'nieca', 'abinbev', '£44', 'bakish', '15,302', '30am', '50u', '25u', 'symc', '13bn', '60bn', '100bn', 'preventedif', 'triatomine', 'broadridge', 'advicewhile', 'hodis', 'tabachnick', 'overcapitalized', 'insoll', 'fidessa', 'netbacks', 'sfr210', 'hummler', 'perfectionistic', 'progressivetokyo', 'bruera', '267,700', 'moscillo', 'effectuating', 'camuñez', 'michiro', '6tn', 'pão', 'açúcar', 'nesterczuk', '2891', 'rttgers', 'estis', 'dinamit', 'giuricin', 'sinofert', 'agraz', 'storiesweighing', 'cryptocurrencies', 'schmidle', 'cyberberkut', 'daign', 'ccilia', 'denverton', 'whohas', 'medicalert', 'abracelet', 'aboutthe', 'beallergic', 'diseasethat', 'factthat', 'diagnosedproperly', 'thyroidcancer', 'ryleigh', 'speechthere', 'answerscochlear', 'westerbeck', 'resect', 'roguishly', 'iex', 'nazarali', 'midwests', 'securequity', 'axovant', 'nelotanserin', 'roivant', 'kirschman', 'stormchaser', 'evofem', 'ocare', 'xeljanz', 'entyvio', 'arbitraged', 'cgus', 'tilray', 'lixivaptan', 'tlantic', 'atvi', 'schoenebaum', 'invensense', 'outsideif', '£750', '£1000', 'jiade', 'unipec', 'pericardiectomy', 'conibear', 'superscary', 'shulyatyeva', 'ceptaris', 'sadredin', 'coriell', 'terez', 'prismsport', 'worldvista', 'troughing', 'exfiltrated', 'badasses', 'mobihealthnews', 'kienitz', 'rebleed', 'presstime', 'ivd', 'bernik', 'mandateand', 'doesmr', 'andridge', 'eiopa', 'alphazero', 'schriock', 'tipirneni', 'priorities�', 'ucits', 'ex\\xadposed', 'oathbreaking', 'fxj', 'ochee', 'victrex', 'alll', 'inadvisably', '39m', 'upselling', 'emoji', 'zydelig', 'pillpack', 'outwhich', 'unlikelya', 'lebrikizumab', 'leaviss', 'silvinit', 'rpe65', 'percher', 'wanggaard', '13,919', 'fuct', 'burkhauser', 'olivetree', 'lbhi', 'muirhouse', 'anonymizing', 'reidentified', 'answerscataract', 'jarlov', 'hollowdweller', 'schonlein', 'overprotecting', 'urpilainen', 'pork-barrel', 'skarich', 'fdasia', 'hojat', '£100', 'tornier', 'supercommittee', 'termfiscal', '129:1815', 'ratpac', 'watchespn', 'faubion', 'landver', 'iscaro', 'landcolt', 'housea', 'choppering', 'malvey', 'metrokin', 'quirónsalud', 'pethokoukis', '£138', 'sicherer', 'waterminder', 'splunk', 'rangasamy', 'qga', 'missier', 'ioer', 'gaffed', 'vege', '£32bn']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yAN7rJ7YiN99","colab_type":"code","colab":{}},"source":["corpus_words = []\n","for sent in corpus['sentence']:\n","  for word in sent:\n","    if word not in not_in:\n","      if word not in corpus_words:\n","        corpus_words.append(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeK_vo9yjKQX","colab_type":"code","colab":{}},"source":["mapping = []\n","custom_mapping = {0 : 0}\n","idx = 1\n","for w in corpus_words:\n","  mapping.append(w2v[w2i[w]])\n","  custom_mapping[w] = idx\n","  idx+=1\n","\n","pad = [0]*300\n","mapping.insert(0,pad)\n","\n","mapping = np.array(mapping)\n","\n","for i in range(len(not_in)):\n","  rnd = np.array([np.random.uniform(-1,1,300)])\n","  mapping = np.append(mapping,rnd,axis = 0)\n","  custom_mapping[not_in[i]] = idx\n","  idx+=1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IchsZgsUDq7j","colab_type":"code","colab":{}},"source":["import pickle\n","\n","with open(\"custom_mapping.pickle\",'wb') as f:\n","  pickle.dump(custom_mapping, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IveYvoItpz5h","colab_type":"code","outputId":"8f64fa21-1465-4e7c-8938-c48531db0bf8","executionInfo":{"status":"ok","timestamp":1582809972138,"user_tz":-330,"elapsed":155732,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(mapping[0])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"VpSiph_13ddN","colab_type":"code","outputId":"7af568d0-9812-4e5a-a823-abdd71a12088","executionInfo":{"status":"ok","timestamp":1582809972140,"user_tz":-330,"elapsed":155554,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(mapping) == len(corpus_words) + len(not_in) + 1"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"dVvXmAtd50qN","colab_type":"code","colab":{}},"source":["max_len = 0\n","this = 0\n","for x in corpus['sentence']:\n","  if len(x)>max_len:\n","    max_len = len(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7UMMjt7p_iA","colab_type":"code","colab":{}},"source":["data = []\n","label = []\n","idx = 0\n","padding = 2\n","for sent in corpus['sentence']:\n","  s = [0]*padding\n","  for word in sent:\n","    s.append(custom_mapping[word])\n","  while(len(s) < max_len + 2*padding):\n","    s.append(0)\n","  data.append(s)\n","  label.append(corpus['gold_label'][idx])\n","  idx+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4qdMCmBx-0F","colab_type":"code","colab":{}},"source":["data = np.array(data)\n","label = np.array(label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"6aaee23e-c44c-4b69-8e9a-eead5710feb1","executionInfo":{"status":"ok","timestamp":1582809974546,"user_tz":-330,"elapsed":153553,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"id":"CQr1VsY74d7B","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(data[907])"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["593"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"7QXRglL818XD","colab_type":"code","colab":{}},"source":["data_indices = []\n","for i in range(len(label)):\n","  data_indices.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-50Bz2nODYbd","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GI1RkrQG7nu2","colab_type":"code","colab":{}},"source":["from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gsa0pLUK1Znq","colab_type":"code","colab":{}},"source":["seed = 2374"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxGnXUe1VQXi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":498},"outputId":"102c7070-7836-445a-c412-4bd1dee5850d","executionInfo":{"status":"ok","timestamp":1582809979956,"user_tz":-330,"elapsed":157304,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["!pip install transformers\n","\n","from transformers import *\n","import pandas as pd\n","import numpy as np\n","import random\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import tensorflow as tf\n","import torch.nn.functional as F\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchsummary import summary\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["There is/are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PfgEsvxyVdTp","colab_type":"code","colab":{}},"source":["MODELS = [#(BertModel,                           BertTokenizer,       'bert-base-uncased'),\n","          # (BertForSequenceClassification,       BertTokenizer,       'bert-large-uncased'),\n","          #(OpenAIGPTModel,                      OpenAIGPTTokenizer,  'openai-gpt'),\n","          #(GPT2Model,                           GPT2Tokenizer,       'gpt2'),\n","          #(CTRLModel,                           CTRLTokenizer,       'ctrl'),\n","          #(TransfoXLModel,                      TransfoXLTokenizer,  'transfo-xl-wt103'),\n","          #(XLNetModel,                          XLNetTokenizer,      'xlnet-base-cased'),\n","          #(XLNetForSequenceClassification,      XLNetTokenizer,      'xlnet-large-cased'),\n","          #(XLMModel,                            XLMTokenizer,        'xlm-mlm-enfr-1024'),\n","          #(XLMForSequenceClassification,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n","          #(RobertaModel,                        RobertaTokenizer,    'roberta-base'),\n","          (RobertaForSequenceClassification,    RobertaTokenizer,    'roberta-large'),\n","          #(XLMRobertaModel,                     XLMRobertaTokenizer, 'xlm-roberta-base'),\n","          #(XLMRobertaForSequenceClassification, XLMRobertaTokenizer, 'xlm-roberta-base'),\n","         ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EncyQ3a8Vg_b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b82d1480-c357-485d-fa14-08223ad4fecd","executionInfo":{"status":"ok","timestamp":1582809979959,"user_tz":-330,"elapsed":156115,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["len(label[train_indices])"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9750"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"A97D79lm_sIw","colab_type":"code","colab":{}},"source":["class text_dataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, data, label):\n","        self.data = data\n","        self.labels = label\n","\n","        \n","    def __len__(self):\n","        return len(self.labels)\n","        \n","    def __getitem__(self, idx):\n","\n","        l = torch.tensor(self.labels[idx]).long()\n","        \n","        sentence = torch.tensor(self.data[idx]).long()\n","\n","        return sentence, l\n","\n","\n","class ClassificationDataset(Dataset):\n","  def __init__(self, corpus, tokenizer_class, pretrained_weights, max_len, data, label):\n","    self.corpus = corpus.reset_index()\n","    self.corpus['sentence'].dropna(inplace=True)\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)# , do_lower_case=True)\n","    self.corpus['sentence'] = [self.tokenizer.encode(sent, add_special_tokens=True, max_length=max_len) for sent in self.corpus['sentence']]\n","    self.corpus['sentence'] = pad_sequences(self.corpus['sentence'], padding='post').tolist()\n","    self.weights = torch.tensor(self.corpus['gold_label'].value_counts(normalize=True).tolist()).to(device)\n","    # print(self.corpus['gold_label'].value_counts(normalize=True))\n","\n","    self.data = data\n","    self.labels = label\n","\n","  def __len__(self):\n","    return len(self.corpus)\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","    # print(type(self.corpus['sentence'][idx]))\n","    X = torch.tensor(self.corpus['sentence'][idx]).to(device)\n","    y = torch.tensor(self.corpus['gold_label'][idx]).to(device)\n","    sample = (X, y)\n","\n","    l = torch.tensor(self.labels[idx]).long()\n","        \n","    sentence = torch.tensor(self.data[idx]).long()\n","    \n","    return (sample, (sentence,l))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kqRal30eLlr","colab_type":"code","colab":{}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds.detach().cpu().numpy(), axis=1).flatten()\n","    labels_flat = labels.cpu().numpy().flatten()\n","\n","    # print(pred_flat, labels_flat)\n","    return np.sum(pred_flat == labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQz8NBi3eMlr","colab_type":"code","colab":{}},"source":["def set_worker_seed(worker_id):\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoMSOGKqDvps","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class CNN_Text(nn.Module):        \n","\n","    def __init__(self, args, weight_matrix, train_weights=False):\n","        super(CNN_Text, self).__init__()\n","        self.args = args\n","        self.embed, num_embeddings, embedding_dim = self.create_emb_layer(weight_matrix, train_weights)\n","\n","        V = num_embeddings\n","        D = embedding_dim\n","        C = args.class_num\n","        Ci = 1\n","        Co = args.kernel_num\n","        Ks = args.kernel_sizes\n","\n","        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n","        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n","        '''\n","        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n","        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n","        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n","        '''\n","        self.dropout = nn.Dropout(args.dropout)\n","        self.fc1 = nn.Linear(len(Ks)*Co, C)\n","    \n","    def create_emb_layer(self, weight_matrix, train_weights=False):\n","        num_embeddings, embedding_dim = weight_matrix.size()\n","        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n","        emb_layer.load_state_dict({'weight': weight_matrix})\n","        if train_weights:\n","            emb_layer.weight.requires_grad = True\n","        else:\n","            emb_layer.weight.requires_grad = False\n","\n","        return emb_layer, num_embeddings, embedding_dim\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.embed(x)  # (N, W, D)\n","        # print(x.size())\n","        \n","        if self.args.static:\n","            x = Variable(x)\n","\n","        x = x.unsqueeze(1)  # (N, Ci, W, D)\n","\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n","\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","\n","        x = torch.cat(x, 1)\n","\n","        '''\n","        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n","        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n","        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n","        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n","        '''\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-6-nHIRELWr","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import torch\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","\n","def train(train_iter, dev_iter, model, args):\n","    if args.cuda:\n","        model.cuda()\n","        print(\"On Cuda\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.1)\n","    weights = [1,10]\n","    weights = torch.FloatTensor(weights).cuda()\n","    criterion = nn.CrossEntropyLoss(weight=weights)\n","\n","    steps = 0\n","    best_acc = 0\n","    last_step = 0\n","    to_return = {}\n","    max_so_far = 0.0\n","    for epoch in range(1, args.epochs+1):\n","        model.train()\n","        train_preds = None\n","        train_labels = None\n","        running_loss = 0.0\n","        total_loss = 0.0\n","        mini_batch = 0\n","        print(\"epoch: \",epoch)\n","        scheduler.step()\n","\n","        for feature, labels in train_iter:\n","            mini_batch+=1\n","            \n","            if args.cuda:\n","                feature, labels = feature.cuda(), labels.cuda()\n","\n","            optimizer.zero_grad()\n","            outputs = model(feature)\n","\n","            # print('outputs size', outputs.size())\n","            # print('target vector', target.size())\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            steps += 1\n","            if train_preds is None or train_labels is None:\n","              train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","              train_labels = labels.cpu().numpy().flatten()\n","            else:\n","              train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","              train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n","\n","\n","            loss.backward()\n","            optimizer.step()\n","            # scheduler.step()\n","\n","            if mini_batch % 100 == 0:    # print every 100 mini-batches\n","              print('[%d, %5d] loss: %.3f' % (epoch , mini_batch , running_loss / 100))\n","              running_loss = 0.0\n","        \n","        print(\"Training loss in epoch %d is %.3f\" % (epoch , total_loss / len(train_loader)))\n","        print(\"Training accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(train_labels, train_preds) * 100))\n","        print(\"Training precision in epoch %d is %.3f\" % (epoch , precision_score(train_labels, train_preds) * 100))\n","        print(\"Training recall in epoch %d is %.3f\" % (epoch , recall_score(train_labels, train_preds) * 100))\n","        print(\"Training F1-score in epoch %d is %.3f\" % (epoch, f1_score(train_labels, train_preds) * 100))\n","        \n","        eval(dev_iter, model, criterion, epoch, args, to_return, max_so_far)\n","        print()\n","        print(\"-\"*90)\n","        print()\n","        \n","    return to_return\n","\n","\n","def eval(data_iter, model, criterion, epoch, args, to_return, max_so_far):\n","    \n","    model.eval()\n","    test_loss = 0.0\n","    test_preds = None\n","    test_labels = None\n","    test_inputs = None\n","    # corrects, avg_loss = 0, 0\n","    with torch.no_grad():\n","\n","        for data in data_iter:\n","            inputs, labels = data\n","            if args.cuda:\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","          \n","            test_loss += loss.item()\n","            if test_preds is None or test_labels is None:\n","                test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","                test_labels = labels.cpu().numpy().flatten()\n","                test_inputs = inputs.cpu().numpy()\n","            else:\n","                test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","                test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n","                test_inputs = np.concatenate((test_inputs, inputs.cpu().numpy()), axis = 0)\n","\n","            # test_accuracy += flat_accuracy(outputs[0], labels)\n","    \n","    print(\"Test loss in epoch %d is %.3f\" % (epoch, test_loss / len(val_loader)))\n","    print(\"Test accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(test_labels, test_preds) * 100))\n","    print(\"Test precision in epoch %d is %.3f\" % (epoch, precision_score(test_labels, test_preds) * 100))\n","    print(\"Test recall in epoch %d is %.3f\" % (epoch, recall_score(test_labels, test_preds) * 100))\n","    print(\"Test F1-score in epoch %d is %.3f\" % (epoch, f1_score(test_labels, test_preds) * 100))\n","    if f1_score(test_labels, test_preds)*100 > max_so_far:\n","      max_so_far = f1_score(test_labels, test_preds)*100\n","      to_return['pred'] = test_preds\n","      to_return['label'] = test_labels\n","      to_return['inputs'] = test_inputs\n","\n","\n","\n","def predict(text, model, text_field, label_feild, cuda_flag):\n","    assert isinstance(text, str)\n","    model.eval()\n","    # text = text_field.tokenize(text)\n","    text = text_field.preprocess(text)\n","    text = [[text_field.vocab.stoi[x] for x in text]]\n","    x = torch.tensor(text)\n","    x = autograd.Variable(x)\n","    if cuda_flag:\n","        x = x.cuda()\n","    print(x)\n","    output = model(x)\n","    _, predicted = torch.max(output, 1)\n","    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n","    return label_feild.vocab.itos[predicted.data[0]+1]\n","\n","\n","def save(model, save_dir, save_prefix, steps):\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    save_prefix = os.path.join(save_dir, save_prefix)\n","    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n","    torch.save(model.state_dict(), save_path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlKM4aZj4vH2","colab_type":"code","colab":{}},"source":["weight_matrix = torch.tensor(mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDSNmGqiEanf","colab_type":"code","outputId":"d6b67c1a-e1f3-4368-f60a-dcbcadba6799","executionInfo":{"status":"ok","timestamp":1582810058577,"user_tz":-330,"elapsed":18330,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":486}},"source":["import os\n","import argparse\n","import datetime\n","import torch\n","\n","\n","parser = argparse.ArgumentParser(description='CNN text classificer')\n","# learning\n","parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n","parser.add_argument('-epochs', type=int, default=30, help='number of epochs for train [default: 256]')\n","parser.add_argument('-batch-size', type=int, default=64, help='batch size for training [default: 64]')\n","parser.add_argument('-log-interval',  type=int, default=1,   help='how many steps to wait before logging training status [default: 1]')\n","parser.add_argument('-test-interval', type=int, default=100, help='how many steps to wait before testing [default: 100]')\n","parser.add_argument('-save-interval', type=int, default=500, help='how many steps to wait before saving [default:500]')\n","parser.add_argument('-save-dir', type=str, default='try1', help='where to save the snapshot')\n","parser.add_argument('-early-stop', type=int, default=1000, help='iteration numbers to stop without performance increasing')\n","parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n","# data \n","parser.add_argument('-shuffle', action='store_true', default=True, help='shuffle the data every epoch')\n","# model\n","parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n","parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n","parser.add_argument('-embed-dim''''  ''', type=int, default=300, help='number of embedding dimension [default: 128]')\n","parser.add_argument('-kernel-num', type=int, default=100, help='number of each kind of kernel')\n","parser.add_argument('-kernel-sizes', type=str, default='3,4,5', help='comma-separated kernel size to use for convolution')\n","parser.add_argument('-static', action='store_true', default=True, help='fix the embedding')\n","# device\n","parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n","parser.add_argument('-no-cuda', action='store_true', default=False, help='disable the gpu')\n","# option\n","parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n","parser.add_argument('-predict', type=str, default=None, help='predict the sentence given')\n","parser.add_argument('-test', action='store_true', default=False, help='train or test')\n","args = parser.parse_args(args=[])\n","\n","\n","# load data \n","print(\"\\nLoading data...\")\n","# text_field = data.Field(lower=True)\n","# label_field = data.Field(sequential=False)\n","# train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n","\n","for model_class, tokenizer_class, pretrained_weights in MODELS:\n","  \n","  train_corpus, test_corpus = train_corpus, test_corpus\n","\n","  train_dataset = ClassificationDataset(train_corpus, tokenizer_class, pretrained_weights, 128, data[train_indices], label[train_indices])\n","  test_dataset = ClassificationDataset(test_corpus, tokenizer_class, pretrained_weights, 128, data[test_indices], label[test_indices])\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, worker_init_fn=set_worker_seed)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True, worker_init_fn=set_worker_seed)\n","\n","  model = model_class.from_pretrained(pretrained_weights, num_labels=2, output_hidden_states=False, output_attentions=False)\n","  model.to(device)\n","  criterion = nn.CrossEntropyLoss(weight=train_dataset.weights)\n","\n","# train_sampler = SubsetRandomSampler(train_indices)\n","# valid_sampler = SubsetRandomSampler(test_indices)\n","\n","# dataset = text_dataset(data, label)\n","# train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","#                                            sampler = train_sampler)\n","# val_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","#                                            sampler = valid_sampler)\n","\n","# train_iter, dev_iter, test_iter = sst(text_field, label_field, device=-1, repeat=False)\n","\n","\n","# update args and print\n","args.embed_num = weight_matrix.size()[1]\n","args.class_num = 2 #binary\n","args.cuda = (not args.no_cuda) and torch.cuda.is_available(); del args.no_cuda\n","args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n","args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n","\n","print(\"\\nParameters:\")\n","for attr, value in sorted(args.__dict__.items()):\n","    print(\"\\t{}={}\".format(attr.upper(), value))\n","# print(\"Training sentences: \", int(len(dataset)*(1-split)))\n","# print(\"Validation sentences: \", int(len(dataset)*(split)))\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["\n","Loading data...\n","\n","Parameters:\n","\tBATCH_SIZE=64\n","\tCLASS_NUM=2\n","\tCUDA=True\n","\tDEVICE=-1\n","\tDROPOUT=0.5\n","\tEARLY_STOP=1000\n","\tEMBED_DIM  =300\n","\tEMBED_NUM=300\n","\tEPOCHS=30\n","\tKERNEL_NUM=100\n","\tKERNEL_SIZES=[3, 4, 5]\n","\tLOG_INTERVAL=1\n","\tLR=0.001\n","\tMAX_NORM=3.0\n","\tPREDICT=None\n","\tSAVE_BEST=True\n","\tSAVE_DIR=try1/2020-02-27_13-27-37\n","\tSAVE_INTERVAL=500\n","\tSHUFFLE=True\n","\tSNAPSHOT=None\n","\tSTATIC=True\n","\tTEST=False\n","\tTEST_INTERVAL=100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04O8R9EVJSDJ","colab_type":"code","colab":{}},"source":["# model\n","cnn = CNN_Text(args, weight_matrix)\n","if args.snapshot is not None:\n","    print('\\nLoading model from {}...'.format(args.snapshot))\n","    cnn.load_state_dict(torch.load(args.snapshot))\n","\n","if args.cuda:\n","    torch.cuda.set_device(args.device)\n","    cnn = cnn.cuda()\n","        \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgJtTMVPlQ20","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":382},"outputId":"48263eb3-5c2f-46d4-c7a1-3a9059fd63bf","executionInfo":{"status":"error","timestamp":1582810061510,"user_tz":-330,"elapsed":16575,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["epochs = 20\n","\n","optimizer = AdamW(model.parameters(),\n","                    lr = 1e-5, # args.learning_rate - default is 5e-5, 1e-5 worked best for me\n","                    eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_train_steps = len(train_loader) * epochs\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_train_steps)\n","\n","optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=args.lr)\n","scheduler_cnn = torch.optim.lr_scheduler.StepLR(optimizer_cnn, step_size=9, gamma=0.1)\n","# weights = [1,10]\n","# weights = torch.FloatTensor(weights).cuda()\n","# criterion_cnn = nn.CrossEntropyLoss(weight=weights)\n","\n","  \n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  total_loss = 0.0\n","  model.train()\n","  cnn.train()\n","\n","  train_preds = None\n","  train_labels = None\n","\n","  for i, data in enumerate(train_loader):\n","    data_trans , data_cnn = data\n","    inputs, labels = data_trans\n","    feature, labels_cnn = data_cnn\n","\n","    inputs, labels = inputs.cuda(), labels.cuda()\n","    feature, labels_cnn = feature.cuda(), labels_cnn.cuda()\n","\n","    optimizer.zero_grad()\n","    optimizer_cnn.zero_grad()\n","\n","    out_trans = model(inputs) # labels=b_labels)\n","    out_cnn = cnn(feature)\n","\n","    outputs = out_trans[0]*0.65 + out_cnn*0.35\n","\n","    loss = criterion(outputs, labels)\n","    \n","    running_loss += loss.item()\n","    total_loss += loss.item()\n","    # train_accuracy += flat_accuracy(outputs[0], labels)\n","\n","    if train_preds is None or train_labels is None:\n","      train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","      train_labels = labels.cpu().numpy().flatten()\n","    else:\n","      train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","      train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n","\n","    # Clip the norm of the gradients to 1.0.\n","    # This is to help prevent the \"exploding gradients\" problem.\n","    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    nn.utils.clip_grad_norm_(cnn.parameters(), 1.0)\n","\n","    loss.backward()\n","\n","    optimizer.step()\n","    optimizer_cnn.step()\n","    scheduler.step()\n","    scheduler_cnn.step()\n","\n","    if i % 100 == 99:    # print every 100 mini-batches\n","      print('[%d, %5d] loss: %.5f' % (epoch + 1, i + 1, running_loss / 100))\n","      running_loss = 0.0\n","  \n","  print(\"Training loss in epoch %d is %.5f\" % (epoch + 1, total_loss / len(train_loader)))\n","  print(\"Training accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(train_labels, train_preds) * 100))\n","  print(\"Training precision in epoch %d is %.5f\" % (epoch + 1, precision_score(train_labels, train_preds) * 100))\n","  print(\"Training recall in epoch %d is %.5f\" % (epoch + 1, recall_score(train_labels, train_preds) * 100))\n","  print(\"Training F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(train_labels, train_preds) * 100))\n","\n","  # Put the model in evaluation mode--the dropout layers behave differently\n","  # during evaluation.\n","  model.eval()\n","  cnn.eval()\n","  # Tracking variables \n","  test_loss = 0.0\n","\n","  test_preds = None\n","  test_labels = None\n","  \n","  with torch.no_grad():\n","    for data in test_loader:\n","      data_trans , data_cnn = data\n","      inputs, labels = data_trans\n","      feature, labels_cnn = data_cnn\n","\n","      inputs, labels = inputs.cuda(), labels.cuda()\n","      feature, labels_cnn = feature.cuda(), labels_cnn.cuda()\n","      # inputs, labels = data\n","      # outputs = model(inputs) # labels=b_labels)\n","      out_trans = model(inputs) # labels=b_labels)\n","      out_cnn = cnn(feature)\n","\n","      outputs = out_trans[0]*0.65 + out_cnn*0.35\n","\n","      loss = criterion(outputs, labels)\n","    \n","      test_loss += loss.item()\n","      if test_preds is None or test_labels is None:\n","        test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","        test_labels = labels.cpu().numpy().flatten()\n","      else:\n","        test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","        test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n","\n","      # test_accuracy += flat_accuracy(outputs[0], labels)\n","  # print(test_preds)\n","  print(\"Test loss in epoch %d is %.5f\" % (epoch + 1, test_loss / len(test_loader)))\n","  print(\"Test accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n","  print(\"Test precision in epoch %d is %.5f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n","  print(\"Test recall in epoch %d is %.5f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n","  print(\"Test F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))"],"execution_count":44,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-adf42bef36fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mout_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# labels=b_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mout_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         )\n\u001b[1;32m    792\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.43 GiB total capacity; 6.88 GiB already allocated; 8.94 MiB free; 6.90 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"_tJkLqT5xGDr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"7db76f9d-578c-46fd-b64c-95a91b484dd5","executionInfo":{"status":"ok","timestamp":1582809494202,"user_tz":-330,"elapsed":1034,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["a = outputs[0]*.6 + out_cnn*.4\n","a"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0095, -0.0762],\n","        [-0.2098,  0.0823],\n","        [-0.2847, -0.0768],\n","        [-0.1610, -0.0770],\n","        [ 0.0550,  0.1506],\n","        [-0.2612,  0.0017],\n","        [-0.0851, -0.1047],\n","        [ 0.1369,  0.0818],\n","        [-0.0771,  0.0568],\n","        [-0.1793,  0.0620],\n","        [-0.1783,  0.0473],\n","        [-0.0155, -0.0848],\n","        [-0.1317, -0.0090],\n","        [-0.1721, -0.0328],\n","        [-0.1708,  0.0993],\n","        [-0.1146, -0.0489]], device='cuda:0', grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"qI16xnRCJuB0","colab_type":"code","outputId":"9301a541-adc9-4d11-f88c-ef81fba68d12","executionInfo":{"status":"ok","timestamp":1582803334947,"user_tz":-330,"elapsed":181535,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","# train or predict\n","# static\n","if args.predict is not None:\n","    label = predict(args.predict, cnn, text_field, label_field, args.cuda)\n","    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n","elif args.test:\n","    try:\n","        eval(test_iter, cnn, args) \n","    except Exception as e:\n","        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n","else:\n","    print()\n","    try:\n","        val = train(train_loader, val_loader, cnn, args)\n","    except KeyboardInterrupt:\n","        print('\\n' + '-' * 89)\n","        print('Exiting from training early')"],"execution_count":87,"outputs":[{"output_type":"stream","text":["\n","On Cuda\n","epoch:  1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1,   100] loss: 0.552\n","Training loss in epoch 1 is 0.506\n","Training accuracy in epoch 1 is 71.292\n","Training precision in epoch 1 is 24.956\n","Training recall in epoch 1 is 78.002\n","Training F1-score in epoch 1 is 37.814\n","Test loss in epoch 1 is 0.384\n","Test accuracy in epoch 1 is 88.092\n","Test precision in epoch 1 is 47.909\n","Test recall in epoch 1 is 75.758\n","Test F1-score in epoch 1 is 58.698\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  2\n","[2,   100] loss: 0.358\n","Training loss in epoch 2 is 0.357\n","Training accuracy in epoch 2 is 83.221\n","Training precision in epoch 2 is 38.781\n","Training recall in epoch 2 is 86.343\n","Training F1-score in epoch 2 is 53.523\n","Test loss in epoch 2 is 0.452\n","Test accuracy in epoch 2 is 92.308\n","Test precision in epoch 2 is 66.006\n","Test recall in epoch 2 is 64.187\n","Test F1-score in epoch 2 is 65.084\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  3\n","[3,   100] loss: 0.288\n","Training loss in epoch 3 is 0.278\n","Training accuracy in epoch 3 is 87.036\n","Training precision in epoch 3 is 45.964\n","Training recall in epoch 3 is 90.284\n","Training F1-score in epoch 3 is 60.915\n","Test loss in epoch 3 is 0.325\n","Test accuracy in epoch 3 is 88.492\n","Test precision in epoch 3 is 49.094\n","Test recall in epoch 3 is 82.094\n","Test F1-score in epoch 3 is 61.443\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  4\n","[4,   100] loss: 0.202\n","Training loss in epoch 4 is 0.206\n","Training accuracy in epoch 4 is 90.513\n","Training precision in epoch 4 is 54.350\n","Training recall in epoch 4 is 95.050\n","Training F1-score in epoch 4 is 69.156\n","Test loss in epoch 4 is 0.296\n","Test accuracy in epoch 4 is 86.646\n","Test precision in epoch 4 is 44.979\n","Test recall in epoch 4 is 87.603\n","Test F1-score in epoch 4 is 59.439\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  5\n","[5,   100] loss: 0.162\n","Training loss in epoch 5 is 0.167\n","Training accuracy in epoch 5 is 92.513\n","Training precision in epoch 5 is 60.476\n","Training recall in epoch 5 is 95.509\n","Training F1-score in epoch 5 is 74.058\n","Test loss in epoch 5 is 0.303\n","Test accuracy in epoch 5 is 89.354\n","Test precision in epoch 5 is 51.443\n","Test recall in epoch 5 is 83.471\n","Test F1-score in epoch 5 is 63.655\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  6\n","[6,   100] loss: 0.120\n","Training loss in epoch 6 is 0.117\n","Training accuracy in epoch 6 is 94.810\n","Training precision in epoch 6 is 68.883\n","Training recall in epoch 6 is 97.800\n","Training F1-score in epoch 6 is 80.833\n","Test loss in epoch 6 is 0.384\n","Test accuracy in epoch 6 is 91.508\n","Test precision in epoch 6 is 59.560\n","Test recall in epoch 6 is 74.656\n","Test F1-score in epoch 6 is 66.259\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  7\n","[7,   100] loss: 0.093\n","Training loss in epoch 7 is 0.091\n","Training accuracy in epoch 7 is 95.949\n","Training precision in epoch 7 is 74.100\n","Training recall in epoch 7 is 98.075\n","Training F1-score in epoch 7 is 84.418\n","Test loss in epoch 7 is 0.530\n","Test accuracy in epoch 7 is 93.323\n","Test precision in epoch 7 is 72.256\n","Test recall in epoch 7 is 65.289\n","Test F1-score in epoch 7 is 68.596\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  8\n","[8,   100] loss: 0.081\n","Training loss in epoch 8 is 0.079\n","Training accuracy in epoch 8 is 96.738\n","Training precision in epoch 8 is 77.866\n","Training recall in epoch 8 is 98.992\n","Training F1-score in epoch 8 is 87.167\n","Test loss in epoch 8 is 0.393\n","Test accuracy in epoch 8 is 90.862\n","Test precision in epoch 8 is 56.707\n","Test recall in epoch 8 is 76.860\n","Test F1-score in epoch 8 is 65.263\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  9\n","[9,   100] loss: 0.069\n","Training loss in epoch 9 is 0.074\n","Training accuracy in epoch 9 is 96.913\n","Training precision in epoch 9 is 79.044\n","Training recall in epoch 9 is 98.533\n","Training F1-score in epoch 9 is 87.719\n","Test loss in epoch 9 is 0.649\n","Test accuracy in epoch 9 is 93.692\n","Test precision in epoch 9 is 77.055\n","Test recall in epoch 9 is 61.983\n","Test F1-score in epoch 9 is 68.702\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  10\n","[10,   100] loss: 0.054\n","Training loss in epoch 10 is 0.056\n","Training accuracy in epoch 10 is 98.000\n","Training precision in epoch 10 is 85.726\n","Training recall in epoch 10 is 98.533\n","Training F1-score in epoch 10 is 91.684\n","Test loss in epoch 10 is 0.509\n","Test accuracy in epoch 10 is 92.369\n","Test precision in epoch 10 is 64.339\n","Test recall in epoch 10 is 71.074\n","Test F1-score in epoch 10 is 67.539\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  11\n","[11,   100] loss: 0.048\n","Training loss in epoch 11 is 0.050\n","Training accuracy in epoch 11 is 98.021\n","Training precision in epoch 11 is 85.466\n","Training recall in epoch 11 is 99.175\n","Training F1-score in epoch 11 is 91.812\n","Test loss in epoch 11 is 0.436\n","Test accuracy in epoch 11 is 91.569\n","Test precision in epoch 11 is 59.329\n","Test recall in epoch 11 is 77.961\n","Test F1-score in epoch 11 is 67.381\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  12\n","[12,   100] loss: 0.037\n","Training loss in epoch 12 is 0.038\n","Training accuracy in epoch 12 is 98.585\n","Training precision in epoch 12 is 89.154\n","Training recall in epoch 12 is 99.450\n","Training F1-score in epoch 12 is 94.021\n","Test loss in epoch 12 is 0.505\n","Test accuracy in epoch 12 is 92.800\n","Test precision in epoch 12 is 66.329\n","Test recall in epoch 12 is 72.176\n","Test F1-score in epoch 12 is 69.129\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  13\n","[13,   100] loss: 0.031\n","Training loss in epoch 13 is 0.034\n","Training accuracy in epoch 13 is 98.708\n","Training precision in epoch 13 is 90.041\n","Training recall in epoch 13 is 99.450\n","Training F1-score in epoch 13 is 94.512\n","Test loss in epoch 13 is 0.549\n","Test accuracy in epoch 13 is 93.015\n","Test precision in epoch 13 is 68.681\n","Test recall in epoch 13 is 68.871\n","Test F1-score in epoch 13 is 68.776\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  14\n","[14,   100] loss: 0.030\n","Training loss in epoch 14 is 0.028\n","Training accuracy in epoch 14 is 99.097\n","Training precision in epoch 14 is 92.681\n","Training recall in epoch 14 is 99.817\n","Training F1-score in epoch 14 is 96.117\n","Test loss in epoch 14 is 0.612\n","Test accuracy in epoch 14 is 93.508\n","Test precision in epoch 14 is 72.485\n","Test recall in epoch 14 is 67.493\n","Test F1-score in epoch 14 is 69.900\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  15\n","[15,   100] loss: 0.026\n","Training loss in epoch 15 is 0.027\n","Training accuracy in epoch 15 is 98.995\n","Training precision in epoch 15 is 92.112\n","Training recall in epoch 15 is 99.542\n","Training F1-score in epoch 15 is 95.683\n","Test loss in epoch 15 is 0.556\n","Test accuracy in epoch 15 is 93.138\n","Test precision in epoch 15 is 69.022\n","Test recall in epoch 15 is 69.972\n","Test F1-score in epoch 15 is 69.494\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  16\n","[16,   100] loss: 0.025\n","Training loss in epoch 16 is 0.024\n","Training accuracy in epoch 16 is 99.200\n","Training precision in epoch 16 is 93.402\n","Training recall in epoch 16 is 99.908\n","Training F1-score in epoch 16 is 96.546\n","Test loss in epoch 16 is 0.552\n","Test accuracy in epoch 16 is 93.169\n","Test precision in epoch 16 is 68.700\n","Test recall in epoch 16 is 71.350\n","Test F1-score in epoch 16 is 70.000\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  17\n","[17,   100] loss: 0.023\n","Training loss in epoch 17 is 0.024\n","Training accuracy in epoch 17 is 99.251\n","Training precision in epoch 17 is 93.955\n","Training recall in epoch 17 is 99.725\n","Training F1-score in epoch 17 is 96.754\n","Test loss in epoch 17 is 0.563\n","Test accuracy in epoch 17 is 93.015\n","Test precision in epoch 17 is 67.895\n","Test recall in epoch 17 is 71.074\n","Test F1-score in epoch 17 is 69.448\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  18\n","[18,   100] loss: 0.022\n","Training loss in epoch 18 is 0.022\n","Training accuracy in epoch 18 is 99.272\n","Training precision in epoch 18 is 94.194\n","Training recall in epoch 18 is 99.633\n","Training F1-score in epoch 18 is 96.837\n","Test loss in epoch 18 is 0.598\n","Test accuracy in epoch 18 is 93.415\n","Test precision in epoch 18 is 71.347\n","Test recall in epoch 18 is 68.595\n","Test F1-score in epoch 18 is 69.944\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  19\n","[19,   100] loss: 0.022\n","Training loss in epoch 19 is 0.022\n","Training accuracy in epoch 19 is 99.118\n","Training precision in epoch 19 is 93.059\n","Training recall in epoch 19 is 99.542\n","Training F1-score in epoch 19 is 96.191\n","Test loss in epoch 19 is 0.677\n","Test accuracy in epoch 19 is 93.600\n","Test precision in epoch 19 is 74.603\n","Test recall in epoch 19 is 64.738\n","Test F1-score in epoch 19 is 69.322\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  20\n","[20,   100] loss: 0.024\n","Training loss in epoch 20 is 0.022\n","Training accuracy in epoch 20 is 99.251\n","Training precision in epoch 20 is 94.031\n","Training recall in epoch 20 is 99.633\n","Training F1-score in epoch 20 is 96.751\n","Test loss in epoch 20 is 0.652\n","Test accuracy in epoch 20 is 93.538\n","Test precision in epoch 20 is 73.112\n","Test recall in epoch 20 is 66.667\n","Test F1-score in epoch 20 is 69.741\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  21\n","[21,   100] loss: 0.019\n","Training loss in epoch 21 is 0.017\n","Training accuracy in epoch 21 is 99.559\n","Training precision in epoch 21 is 96.372\n","Training recall in epoch 21 is 99.817\n","Training F1-score in epoch 21 is 98.064\n","Test loss in epoch 21 is 0.672\n","Test accuracy in epoch 21 is 93.508\n","Test precision in epoch 21 is 73.602\n","Test recall in epoch 21 is 65.289\n","Test F1-score in epoch 21 is 69.197\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  22\n","[22,   100] loss: 0.018\n","Training loss in epoch 22 is 0.020\n","Training accuracy in epoch 22 is 99.333\n","Training precision in epoch 22 is 94.531\n","Training recall in epoch 22 is 99.817\n","Training F1-score in epoch 22 is 97.102\n","Test loss in epoch 22 is 0.703\n","Test accuracy in epoch 22 is 93.538\n","Test precision in epoch 22 is 74.919\n","Test recall in epoch 22 is 63.361\n","Test F1-score in epoch 22 is 68.657\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  23\n","[23,   100] loss: 0.018\n","Training loss in epoch 23 is 0.017\n","Training accuracy in epoch 23 is 99.487\n","Training precision in epoch 23 is 95.698\n","Training recall in epoch 23 is 99.908\n","Training F1-score in epoch 23 is 97.758\n","Test loss in epoch 23 is 0.621\n","Test accuracy in epoch 23 is 93.323\n","Test precision in epoch 23 is 71.345\n","Test recall in epoch 23 is 67.218\n","Test F1-score in epoch 23 is 69.220\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  24\n","[24,   100] loss: 0.018\n","Training loss in epoch 24 is 0.016\n","Training accuracy in epoch 24 is 99.559\n","Training precision in epoch 24 is 96.454\n","Training recall in epoch 24 is 99.725\n","Training F1-score in epoch 24 is 98.062\n","Test loss in epoch 24 is 0.634\n","Test accuracy in epoch 24 is 93.538\n","Test precision in epoch 24 is 72.973\n","Test recall in epoch 24 is 66.942\n","Test F1-score in epoch 24 is 69.828\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  25\n","[25,   100] loss: 0.015\n","Training loss in epoch 25 is 0.017\n","Training accuracy in epoch 25 is 99.549\n","Training precision in epoch 25 is 96.205\n","Training recall in epoch 25 is 99.908\n","Training F1-score in epoch 25 is 98.022\n","Test loss in epoch 25 is 0.634\n","Test accuracy in epoch 25 is 93.385\n","Test precision in epoch 25 is 72.699\n","Test recall in epoch 25 is 65.289\n","Test F1-score in epoch 25 is 68.795\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  26\n","[26,   100] loss: 0.018\n","Training loss in epoch 26 is 0.016\n","Training accuracy in epoch 26 is 99.508\n","Training precision in epoch 26 is 96.110\n","Training recall in epoch 26 is 99.633\n","Training F1-score in epoch 26 is 97.840\n","Test loss in epoch 26 is 0.652\n","Test accuracy in epoch 26 is 93.415\n","Test precision in epoch 26 is 72.923\n","Test recall in epoch 26 is 65.289\n","Test F1-score in epoch 26 is 68.895\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  27\n","[27,   100] loss: 0.016\n","Training loss in epoch 27 is 0.015\n","Training accuracy in epoch 27 is 99.641\n","Training precision in epoch 27 is 96.975\n","Training recall in epoch 27 is 99.908\n","Training F1-score in epoch 27 is 98.420\n","Test loss in epoch 27 is 0.660\n","Test accuracy in epoch 27 is 93.415\n","Test precision in epoch 27 is 72.923\n","Test recall in epoch 27 is 65.289\n","Test F1-score in epoch 27 is 68.895\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  28\n","[28,   100] loss: 0.014\n","Training loss in epoch 28 is 0.015\n","Training accuracy in epoch 28 is 99.600\n","Training precision in epoch 28 is 96.797\n","Training recall in epoch 28 is 99.725\n","Training F1-score in epoch 28 is 98.239\n","Test loss in epoch 28 is 0.653\n","Test accuracy in epoch 28 is 93.477\n","Test precision in epoch 28 is 72.948\n","Test recall in epoch 28 is 66.116\n","Test F1-score in epoch 28 is 69.364\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  29\n","[29,   100] loss: 0.015\n","Training loss in epoch 29 is 0.014\n","Training accuracy in epoch 29 is 99.590\n","Training precision in epoch 29 is 96.628\n","Training recall in epoch 29 is 99.817\n","Training F1-score in epoch 29 is 98.197\n","Test loss in epoch 29 is 0.653\n","Test accuracy in epoch 29 is 93.415\n","Test precision in epoch 29 is 72.923\n","Test recall in epoch 29 is 65.289\n","Test F1-score in epoch 29 is 68.895\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  30\n","[30,   100] loss: 0.015\n","Training loss in epoch 30 is 0.015\n","Training accuracy in epoch 30 is 99.600\n","Training precision in epoch 30 is 96.631\n","Training recall in epoch 30 is 99.908\n","Training F1-score in epoch 30 is 98.242\n","Test loss in epoch 30 is 0.648\n","Test accuracy in epoch 30 is 93.446\n","Test precision in epoch 30 is 72.727\n","Test recall in epoch 30 is 66.116\n","Test F1-score in epoch 30 is 69.264\n","\n","------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"STs-iLX_9qbY","colab_type":"code","colab":{}},"source":["import pickle\n","with open('val_glove.pickle','wb') as f:\n","  pickle.dump(val,f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2v2J_piwE4P","colab_type":"code","outputId":"cc710d45-738c-429a-8720-2d3321454175","executionInfo":{"status":"ok","timestamp":1580919251954,"user_tz":-330,"elapsed":1701,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["torch.save(cnn, 'model_glove_2.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CNN_Text. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]}]}