{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_glove.ipynb","provenance":[{"file_id":"1Om7GVDag19A4DMFLrCW1n1Lq1XjSr_j9","timestamp":1580323733414}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPZ5tJhsVlSbjI0HT4RLQig"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yi1b9Svhk-Qy","colab_type":"code","outputId":"33c2a91d-957f-4a0d-c1e2-5ca05bb906b1","executionInfo":{"status":"ok","timestamp":1581187406881,"user_tz":-330,"elapsed":33186,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WN6w7wL-b-eI","colab_type":"code","outputId":"e2c75664-01d5-44eb-f5e5-bc2bd67f27a9","executionInfo":{"status":"ok","timestamp":1581187406886,"user_tz":-330,"elapsed":33174,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/My\\ Drive/NLP/cnn-text-classification-pytorch/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP/cnn-text-classification-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1BWsbXx2sziz","colab_type":"code","outputId":"e1248e13-3d90-4324-c388-68e1d16e1c50","executionInfo":{"status":"ok","timestamp":1581187414969,"user_tz":-330,"elapsed":3474,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["custom_mapping.pickle  model.py                        rt-polaritydata.tar\n","LICENSE                mydatasets.py                   \u001b[0m\u001b[01;34msnapshot\u001b[0m/\n","main.py                \u001b[01;34m__pycache__\u001b[0m/                    train.py\n","mapping.pickle         README.md                       \u001b[01;34mtry1\u001b[0m/\n","model_glove_2.pth      \u001b[01;34mrt-polaritydata\u001b[0m/                val_glove.pickle\n","model_glove.pth        rt-polaritydata.README.1.0.txt  val_set.pickle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hy0G-z3qtS7W","colab_type":"code","outputId":"ecca9ea1-def0-48cc-a0b6-3329142951b0","executionInfo":{"status":"ok","timestamp":1581187418619,"user_tz":-330,"elapsed":3241,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from collections import defaultdict\n","from nltk.corpus import wordnet as wn\n","import numpy as np\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","\n","np.random.seed(500)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zeZbGHV0vqq1","colab_type":"code","colab":{}},"source":["path = \"../data/Subtask-1-master/train.csv\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MM8beogbv6Pp","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","corpus = pd.read_csv(path, encoding='utf-8')\n","corpus['sentence'].dropna(inplace=True)\n","corpus['sentence'] = [sent.lower() for sent in corpus['sentence']]\n","corpus['sentence'] = [word_tokenize(word) for word in corpus['sentence']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBFmeMBJwG-K","colab_type":"code","outputId":"b1ea5499-db12-4fcd-efa0-27bd9a0be225","executionInfo":{"status":"ok","timestamp":1581187422381,"user_tz":-330,"elapsed":4345,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["corpus['sentence']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        [goodfellow, 's, theory, has, been, questioned...\n","1        [however, ,, both, campaigners, and, pro-peopl...\n","2        [things, could, have, been, even, better, if, ...\n","3        [the, new, request, ,, if, approved, ,, would,...\n","4        [companies, in, financial, difficulty, can, cu...\n","                               ...                        \n","12995    [the, department, said, it, conducts, thorough...\n","12996    [the, mccain, episode, may, sound, largely, ha...\n","12997    [merkel, said, in, a, speech, to, her, conserv...\n","12998    [``, ``, the, entire, drug, scene, has, change...\n","12999    [in, 2009, ,, the, group, said, adults, should...\n","Name: sentence, Length: 13000, dtype: object"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"yGyKQsjswY-0","colab_type":"code","colab":{}},"source":["# glove\n","\n","w2v = []\n","w2i = {}\n","words = []\n","\n","idx = 0\n","with open('../data/glove.6B.300d.txt','rb') as f:\n","  for l in f:\n","    line = l.decode().split()\n","    word =  line[0]\n","    words.append(word)\n","    w2i[word] = idx\n","    idx +=1\n","    vect = np.array(line[1:]).astype(np.float)\n","    w2v.append(vect)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYvk9ssU4_mS","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '...' in sent[j]:\n","      corpus['sentence'][i][j] = sent[j].replace('...','')\n","    if '..' in sent[j]:\n","      corpus['sentence'][i][j] = sent[j].replace('..','')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kff8lmP5BThE","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '-' in sent[j]:\n","      x = sent[j].split('-')\n","      corpus['sentence'][i][j] = x[0]\n","      if j+1 == len(sent):\n","        corpus['sentence'][i].append(x[1])\n","      else:\n","        corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"668r-osICj4s","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if '/' in sent[j]:\n","      x = sent[j].split('/')\n","      corpus['sentence'][i][j] = x[0]\n","      if len(x)>1:\n","        corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8WVCaYeC_rl","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if \"'\" in sent[j]:\n","      if sent[j] not in words:\n","        corpus['sentence'][i][j] = sent[j].replace(\"'\",\"\")\n","      # x = sent[j].split('-')\n","      # corpus['sentence'][i][j] = x[0]\n","      # corpus['sentence'][i].insert(j+1,x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKHBegoNEXZa","colab_type":"code","colab":{}},"source":["for i in range(len(corpus['sentence'])):\n","  sent = corpus['sentence'][i]\n","  for j in range(len(sent)):\n","    if \".\" in sent[j]:\n","      if sent[j] not in words:\n","        if len(sent[j].split('.')) > 2 or len(sent[j].split('.')) == 1:\n","          corpus['sentence'][i][j] = sent[j].replace(\".\",\"\")\n","        else:\n","          if len(sent[j].split('.')) == 2:\n","            x = sent[j].split('.')\n","            corpus['sentence'][i][j] = x[0]\n","            corpus['sentence'][i].insert(j+1, x[1])\n","            # print(corpus['sentence'][i][j], corpus['sentence'][i][j+1], '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcYTE0JpxTs7","colab_type":"code","colab":{}},"source":["not_in = []\n","for sent in corpus['sentence']:\n","  for word in sent:\n","    if word not in words:\n","      if word not in not_in:\n","        not_in.append(word)\n","        # print(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dspSDLi08Lz","colab_type":"code","outputId":"1c038947-2d11-47fc-d194-880d346ca6d6","executionInfo":{"status":"ok","timestamp":1580918272324,"user_tz":-330,"elapsed":38,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(not_in)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1381"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"execute_result","data":{"text/plain":["1381"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"kicQpyKfE2Ps","colab_type":"code","outputId":"390e8ae3-e16a-4701-edc4-51293a1698c7","executionInfo":{"status":"ok","timestamp":1580918272327,"user_tz":-330,"elapsed":24,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["print(not_in)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['kingdomof', '', 'forwardnot', 'stakethe', 'nonemergencies', 'circumastance', 'r-nc', 'reynaga', 'irisin', 'cuntry', 'shkreli', 'thatdespite', 'forcehe', 'ntgr', 'bfsr', 'ceramides', 'otillar', 'scratchpad', 'mcritchie', 'enria', 'watchwordalthough', 'mosychuk', 'aedpa', 'dipierro', 'panelo', 'arconic', 'paynet', 'gorsuchs', 'suicidenow', 'kyrsten', 'freiborg', '5bn', 'depomed', 'amsellem', 'overperformances', 'thcb', 'privitera', 'legimately', 'burkan', 'zonszein', '10,554', 'brochez', 'pfapa', 'bingener', 'entresto', 'candidateand', 'zarxio', 'tdn2k', 'confounders', 'overcounted', 'rebirthed', 'altmaier', 'barcap', 'shithole', 'lignos', 'sciple', 'morens', 'reclosing', 'roadfor', 'unpeg', 'currencythe', 'somepart', 'allthat', 'staehr', 'baumblit', 'khorsand', 'innereye', 'topkill', '1+1', 'kilimnik', 'fralick', 'krautzberger', 'truog', 'luján', 'screenos', 'unbooked', 'ohions', 'sowanick', 'neurobiologically', 'killino', 'incel', 'nn27303398', 'rcep', 'piampongsant', 'oubina', 'nontraumatic', '21e', 'coeure', 'rabten', 'reichlin', 'klipp', 'theranos', 'brexit', 'ionis', 'msct', '≥25kg', '≥30kg', 'müller', 'issakainen', 'blasey', 'elugardo', 'unrigging', '40bn', '100tn', 'deprescribing', 'mulye', 'ilbd', 'torba', 'shiono', 'vemurafenib', 'dacarbazine', 'lynparza', 'cholnoky', 'jeoff', 'gangwisch', 'bernanke*', 'pended', 'rejoiceit', 'staikouras', 'lewandowskis', 'carcinoembryonic', 'bosiers', 'haridy', 'urodynamic', 'alembik', 'dvn', 'melland', 'florange', 'brexiteer', 'colectomy', 'pay+', 'rosanoff', 'knols', 'jobsbut', 'siloed', 'preauthorization', 'gaddie', 'mansplaining', 'esiner', 'beggaring', 'iezzi', '60tn', 'townswick', 'guidancesome', 'icagen', 'ecohelmet', 'world�', '�', 'fscai', 'scai', 'schnautz', 'proration', 'incentivised', 'mdv3100', 'zytiga', 'medivation', 'melsen', 'nunamaker', 'tudose', 'shingrix', 'indexuniverse', 'vxx', 'washpost', 'karoun', 'securityholder', 'yger', 'google+', '11,493', 'certifiied', 'orkambi', 'en+', 'pshg_p', '112bn', 'slavedriver', 'isrg', 'sharice', 'jfcom', 'woahhhh', 'nktr', 'opdivo', 'yervoy', 'immunotherapies', 'recapitalizating', 'dierksen', 'ambarella', 'herefor', 'gamblersor', 'contestantsthat', 'schreckreports', 'erdogans', 'beencooked', 'golbon', 'eubut', 'clintonness', 'giunchigliani', 'eteplirsen', 'storiesseasonal', 'viprinex', 'avenatti', 'jarris', 'gilholm', 'nontradeable', 'oanda', 'prizeroot', 'systemthe', 'pescanova', 'weekdo', 'unsterilised', 'them�', 'zemcik', 'immunochemical', 'papyracea', 'morcellation', 'houte', 'firdapse', 'kronthal', 'passporting', 'rotasiil', 'patrikis', 'cyad', 'comeyi', 'misdemeanourand', 'lomen', 'hoerth', 'baxalta', 'engag', 'percec', 'dovere', 'provention', 't1d', '**a', 'zavecz', 'hoepa', 'inquicker', 'baytex', 'do�', 'whcra', 'wharen', 'randomising', 'analogise', 'thoughtssome', '630m', 'assetswhich', 'banksthe', 'fibrillating', 'defibrillated', 'conlumino', 'slutkin', 'tjokrosaputro', 'dailyfx', 'ohiohealth', 'littlefinger', 'monologuing', 'dontos', 'lemtrada', 'presal', 'hematospermia', 'guindos', 'continueon', 'trueness', 'queered', 'nfld', 'venoplasty', 'unbuyable', 'btig', 'kopola', 'mäkinen', '33bn', '7bn', 'mythaholic', 'venge', 'scardina', 'isis8', 'whadya', '£200m', '£420m', '£7', '6bn', '£23bn', 'misgoverning', 'tyrannically', 'sobanko', 'filmalter', '£3', 'investmentwhile', 'wordelectoral', 'healthkit', 'parscale', 'ransomware', 'zulily', 'léonora', 'chiappelli', 'vestager', 'agentswho', 'jobfeared', 'dehejia', 'idfc', 'thepast', 'thatbehaviourright', '£500m', 'wauthier', 'cubicin', 'otezla', 'receptos', 'lunacies', 'coxibs', 'xme', 'thiswhile', 'hq2', 'catellanos', 'indivior', 'durata', 'sciutto', 'bigotted', 'lamestream', 'starndard', 'lifepoint', 'lpnt', 'ridaforolimus', 'mortifications', 'chozick', 'cyberespionage', 'alperovitch', 'crowdstrike', 'hackable', 'nkc', 'jcar015', '£69', '4bn', '13057', 'fornero', 'circumsized', 'pneumoliner', 'morcellated', 'inácio', 'bromiley', 'storiesempty', 'bakries', 'daca', 'kanefield', 'shugerman', 'strategised', 'reznikova', 'liptovsky', 'hradok', 'patheon', 'highertwitter', 'nyag', 'unremedied', 'molepost', 'solanezumab', 'pmis', 'believehigh', 'grexit', 'broemer', 'reince', '£28m', 'affo', 'puigdemont', 'trumpism', 'sportsafe', 'redtech', 'ursano', '16bn', 'warmbrodt', '204k', 'esformes', 'toigo', 'valvuloplasty', 'usmca', 'wrongi', 'klepin', 'worldchecking', 'amzn', 'tdjc', 'lockmaker', 'timesour', 'pinebridge', 'valuationsdiageo', 'themchinese', 'solarfun', 'curveglobal', 'margina', 'governmetn', 'tmunity', 'realised�', 'pr+', 'hospitalinspections', 'geltzeiler', 'bloomgren', 'nicolás', 'werent', 'okuwobi', 'riskscore', 'numberedat', 'premorbid', 'porchia', 'nifla', 'examination-rooms', 'overprescribe', 'espriet', 'irmat', 'optum', 'mfglobal', 'cadburys', 'earliestthough', 'verlinvest', 'mulacek', 'kotagal', 'schrder', 'perritano', 'flesvig', 'argumentsindeed', 'nymann', 'cuccinellis', 'lipus', 'vapers', 'akorn', 'putcha', 'sier', 'ultrastrong', 'kitakura', 'hntes', 'in�', 'wechat', 'wateringly', 'skylanders', 'entegra', 'nn25160928', 'prough', 'inzitari', 'yarros', 'charanpreet', 'dabhia', 'cellseven', 'licencethen', 'darwinistic', 'infinera', 'espp', '17bn', 'whaled', 'farkouh', 'restuccia', 'deconditioned', 'manouch', 'moshayedi', 'phylopatric', 'altarum', 'yieldco', 'gilvarry', 'docmorris', 'spokeo', 'retweeted', 'wasendorf', 'keytruda', 'non-executives', 'odendahl', 'saysthat', 'malvertising', 'camcorp', 'egzzz', 'medpage', 'ahier', 'generalisable', 'discussant', 'ticc', 'makuch', 'marketmakers', 'kolfage', 'hruksa', 'as�', 'overpromises', 'unspared', 'douda', 'spikily', 'egert', 'poloz', 'bolsonaro', 'ungallant', '7,042', 'litonjua', 'self-judgments', 'kondik', 'arshamian', 'lumentum', 'vcsels', 'rohret', 'capitala', 'amcap', 'emanuels', 'resolutionsmeaning', 'levelshave', 'semaya', 'healthtour', 't1c', 'vipshop', 'betterof', 'alreadyyou', 'cirmo', 'epidiolex', 'dravet', 'asnider', 'wyen', 'himselfwell', 'educatedthe', 'jbarro', 'trauttenberg', 'moonshoti', 'mmscf', 'lobley', 'point72', 'cakeshop', 'juurlink', 'qualifyand', 'mnn', 'gerspach', 'atypia', 'sefl', '55bn', '£69bn', '£14bn', 'rednecky', 'iiea', 'phenotyping', 'tijirit', 'preyss', 'plco', '2618', 'rebastinib', '3014', 'andhe', 'whatyou', 'thespecialist', 'takenplace', 'assistantsfollows', 'saikawa', 'inflam', 'workvery', 'anothervendor', 'trns', 'knighthead', 'rgy', 'obfr', 'enlightnment', 'doubleline', 'vidscription', 'ycmnet', 'yoshikami', 'mitigo', 'background-check', 'ap7', 'ca125', 'acquiror', 'nayda', 'pancioli', 'fischell', 'pcmh', 'leoning', 'threatsnixons', 'allegationseven', 'avoidably', 'under-estimation', 'obozo', 'blizzardy', 'entrec', 'treatmenteven', 'likelywould', 'weensure', 'oversimplifyingthe', 'thecataract', 'distanceor', 'matons', 'cithian', 'meiburg', 'mdbi', 'simps', 'flotus', 'stabbe', 'efsm', 'superinfection', 'sedgh', 'laclair', 'design\\xaded', 'rel\\xadeased', 'an\\xadnoying', 'bec\\xadause', 'lyxor', 'profitslindsay', 'jrgen', 'microgrid', 'vitalmedix', 'dexcom', 'missika', 'stefin', 'sherbin', 'serper', 'timbouctou', 'distributory', 'antiatherogenic', 'marinuzzi', '*minneapolis', '*fed', 'expiredmeaning', 'e004', 'emmanouilidis', 'devincow', 'doomsters', 'bowone', 'lowflationary', '456,789', 'witlessly', '65bn', 'pricings', 'dtegn', 'interspersal', 'nadella', 'ibrutinib', 'attackonly', 'hollyfrontier', '59,480', 'peña', 'deprescribe', 'tradereporter', 'goitein', 'iome', 'vissers', 'nppc', 'tanona', 'morbidities', 'biamonte', 'rmds', 'ellmers', 'airfarewatchdog', 'excisional', 'cryptocurrency', 'coincheck', 'cataltepe', 'biomarin', '2x1012', '6,862', '72,000,000', 'flowback', 'vbp', 'bisaro', 'hollak', 'leadiant', 'prevenion', 'newsnew', 'sederer', 'medicareon', 'aflappin', 'wesee', 'november1', '34,517,250', 'skierka', 'scavino', 'kobre', 'goate', 'younkin', 'vengroff', 'answersis', 'lbbb', '28bn', 'weekwant', 'steidley', 'gonzález', '58bn', 'protégés', 'marsupialization', 'trutina', 'delreal', 'aideven', 'leronlimab', 'cd02', 'pcatest', 'hemauer', 'savolitinib', 'maoa', 'luthman', 'liontrust', 'emph', 'trumponomics', 'dismuke', 'cassivi', 'insurancenot', 'intrapartum', 'shiarly', 'hepatologist', 'gracas', 'heran', 'gphin', 'tedprize', 'faltersremember', 'tmfdeej', 'gothamist', 'kashoggi', 'reguire', 'cringeworthy', 'underwhelm', 'flavanol', 'donationsthis', 'nccn', 'mcaleenan', 'coalitionperhaps', 'springloaded', '35:34', 'reluctances', 'orexo', 'ltip', 'indexiq', 'overexert', 'leftciti', 'lagorio', 'sedran', 'gevo', 'tarnef', 'atiqi', 'grandparental', 'trembowicz', 'wyderko', 'botheration', 'creuzfeldt', 'bauler', 'mainfirst', 'soleoutcome', 'whenall', 'anecklace', 'thebest', 'ourchest', 'feedingtube', 'regionrussia', 'itone', 'corpbanca', 'oxybate', '12bn', '9bn', 'forward‐looking', '+2349051208634when', 'srouji', 'pehub', 'corbat', 'corrollary', 'casamassimo', 'lnder', 'answershow', 'nondevelopmental', 'glitazones', 'hyperinfection', 'banaei', 'kartsotis', 'rerunor', 'sherzan', 'ezechiel', 'copic', 'bcma', 'haughom', '2000including', 'oklahomathe', 'sivignon', 'bdsi', 'detasseling', 'cornpicking', 'judgy', 'meetingwithout', 'evidencethat', 'dumain', 'kapla', 'frideres', 'ossoff', '£2,800', 'contentthis', 'romneyworld', 'bbby', 'partcipate', 'evalulation', 'nematicides', 'sowhile', 'plansout', 'orderto', 'furthermarket', 'lesscompetition', 'thelegislation', 'propafenone', 'sotalol', 'dotard', 'avanir', 'nuedexta', '7751', '7733', '8035', '8036', 'exercycle', 'linewhile', 'shitstains', 'nonrenewal', 'somebodyie', 'avav', 'lukashevich', 'oosterveld', 'awfulizing', 'janumet', 'dixhouse', 'ingel', 'hb56', 'kwarteng', 'tv+', 'nflx', 'medcity', 'traister', 'redocument', 'chondrocyte', 'loncon', '£903m', 'aquadvantage', 'glennbeck', 'gbtv', 'douchiness', 'policywise', 'friedmen', 'ferreyr', 'flexitime', 'shaub', 'posthurricane', 'prehurricane', 'volunteersafter', 'texas.during', 'clavulanate', 'piperacillin', 'tazobactam', 'zosyn', 'bagios', 'wapo', 'snowblue', 'attachedmentsi', 'nebank', 'saczynski', 'crosstabs', 'aa+', 'neratinib', 'maelstroms', 'nongroup', 'netback', 'ncib', 'misattuned', 'ctbs', 'erbb1', 'capecitabine', 'urdan', 'unsubsidised', '11,770', '47,080', 'plassat', 'turnround', '€2', '45bn', 'rebok', 'beicker', 'senatewhich', 'pressit', 'jcpoa', 'boustani', 'peterschmitt', 'guessous', 'mediawatch', 'biegelsen', 'dbkgn', 'jiankui', 'bsps', '88m', 'pzena', 'sisolak', 'unblinded', 'olypmic', 'nonconvertible', 'pelosis', '22bn', 'rb51', 'gipsa', 'uromedica', 'dupilumab', 'telogen', 'effluvium', 'totalis', 'carbon-free', 'sotu', 'nellen', 'peginesatide', 'derrough', 'polle', 'baltensperger', 'chondroplasty', 'convo', 'salveson', 'ccdoe', 'senblumenthal', 'nflcommish', 'rucaparib', 'watergoing', 'dabby', 'pfass', '•olden', '2°c', 'deppression', 'kalanick', 'nonpharmacological', 'nonallergic', 'galuccio', 'toweljune', 'knowmr', 'trenowden', 'portyghee', 'shulkin', 'nevertrumpers', '£1,000', '£7,000', 'giegold', 'ablations', 'matousek', 'shawkut', 'factualwhat', 'injectioncannot', '32,898', 'takethe', 'rylee', 'coene', 'guaidó', 'benisek', 'prwora', 'cimzia', 'singiser', 'mor208', '18,378', 'concernedthe', 'partiespaused', 'cellectis', 'government–', 'eargo', 'slavitt', 'cimziaâ®', 'triangulator', 'mnsure', 'gablofen', 'olness', 'marette', 'swetnick', 'pnk', 'crème', 'brûlée', 'behsudi', 'zacary', 'techeven', 'tromps', 'huttenhower', '79mr', 'takushi', 'ishikura', 'aledade', 'mostashari', 'costscosts', 'topx', 'enestro', 'parsortix', 'eskesen', 'azpia', 'economicsit', 'sweatily', 'dkr53bn', 'czapp', 'drrich', 'manchik', 'seriousconsequences', 'abbvie', 'bondholdings', 'costerg', 'cyence', 'kalobios', 'brainlessly', 'reify', 'oookaay', 'chatbots', 'ameringen', 'csalp', 'ttip', 'realdonaldtrump', 'undisbursed', 'playbookers', 'mutualization', 'cimzia®', 'bouhara', 'foroud', 'buoyant�', 'foolfest', 'toomeys', 'fsoc', 'kolhmann', 'sp+', 'paymentsnot', 'guglielmotti', 'multigenic', 'nidawi', 'wergin', 'minusma', 'swedo', 'lozman', 'putinwho', 'poehling', '9,580', '12,902', 'otherhand', 'car-t', 'ecigarettes', '1trn', 'luschini', 'preteenagers', '£1,381', '£9,300', 'famewave', 'albence', 'toldme', 'whatthey', 'thatonly', 'orher', 'forcompetence', 'ipaa', 'ungari', 'sageen', 'drive-left', 'beaverness', 'runkeeper', 'kalir', 'heartwire', 'hernández', 'avocadoes', 'garzotto', 'youdid', 'iley', 'umich', 'biggish', 'unrevoked', 'kohane', 'prewarned', 'rutqvist', 'coinbase', 'ehrsam', 'blockchain', 'tomdispatch', 'slate®', '£20', '£2', 'financialisation', 'housingthe', 'redicting', 'cemma', 'caitlinzemma', '134m', '346bn', 'washeteria', 'ishrak', 'hoogendyk', 'adheretech', 'abruptlyas', 'truty', 'withthemselves', 'andstrike', 'leiomyosarcomas', 'woulndt', 'sederberg', '2650bc', 'edro', 'gloviczki', 'jived', '5thbday', 'obamacarethe', 'democratsthat', 'krzanich', '10nm', 'weightsit', 'seasonand', 'scailex', 'scix', 'linora', 'consonery', 'brexiteers', 'hypertriglyceridemic', 'sequencingin', 'stefanek', 'cordovilla', 'mandrola', '6,584', 'chrystia', '聳', '25:44', '45and', 'boeckler', 'npdb', 'glausser', 'ricol', 'ovitt', 'cpri', 'aqlan', '9202', 'qan', 'epocrates', 'drummy', 'schnatter', 'hawkinberry', 'shuanghui', 'josé', '£50m', 'polkes', 'untether', 'interstim', 'liposuctioned', 'cirka', 'horsager', 'lowest-yielding', 'yaradua', 'vsoe', 'cisgendered', 'zoetis', 'nonproven', 'schlapp', '244m', 'posess', 'razaqzada', 'spanberger', 'inspra', 'elseplease', 'tothem', 'becausei', 'nechelput', 'esvelt', 'presidentgreens', 'forthare', 'restasis', 'intromission', 'dodick', 'ngdp', 'lnkd', 'panl', 'lubitz', 'skinactivist', '£300bn', 'wandoan', '200bn', 'bokkelen', 'thinkfor', 'thefinancial', 'tohim', 'fragmentedsystem', 'systemwould', 'treattheir', 'changethe', 'cispa', 'morethe', 'tmall', 'micrometastases', 'jumbomatic', 'dourson', 'dealbut', 'birtel', 'deplorables', 'weightsif', 'balwani', 'suahasil', 'nazara', '£22bn', 'zanny', 'flatto', 'culpritboth', 'unobligated', 'vilnerable', 'psychoanalyzing', 'ftfm', 'smartwatch', 'sabbs', 'coratti', 'omfif', 'loréal', 'nawana', 'wage-and-benefit', 'afib', 'acase', 'surgeonleft', 'publicityconcerning', 'iwatch', 'proco', 'zarzeczny', 'giventhough', 'hafle', 'demétrio', 'postuma', 'jannetta', 'chemed', 'glendevon', 'hampshirenew', 'nieca', 'abinbev', '£44', 'bakish', '15,302', '30am', '50u', '25u', 'symc', '13bn', '60bn', '100bn', 'preventedif', 'triatomine', 'broadridge', 'advicewhile', 'hodis', 'tabachnick', 'overcapitalized', 'insoll', 'fidessa', 'netbacks', 'sfr210', 'hummler', 'perfectionistic', 'progressivetokyo', 'bruera', '267,700', 'moscillo', 'effectuating', 'camuñez', 'michiro', '6tn', 'pão', 'açúcar', 'nesterczuk', '2891', 'rttgers', 'estis', 'dinamit', 'giuricin', 'sinofert', 'agraz', 'storiesweighing', 'cryptocurrencies', 'schmidle', 'cyberberkut', 'daign', 'ccilia', 'denverton', 'whohas', 'medicalert', 'abracelet', 'aboutthe', 'beallergic', 'diseasethat', 'factthat', 'diagnosedproperly', 'thyroidcancer', 'ryleigh', 'speechthere', 'answerscochlear', 'westerbeck', 'resect', 'roguishly', 'iex', 'nazarali', 'midwests', 'securequity', 'axovant', 'nelotanserin', 'roivant', 'kirschman', 'stormchaser', 'evofem', 'ocare', 'xeljanz', 'entyvio', 'arbitraged', 'cgus', 'tilray', 'lixivaptan', 'tlantic', 'atvi', 'schoenebaum', 'invensense', 'outsideif', '£750', '£1000', 'jiade', 'unipec', 'pericardiectomy', 'conibear', 'superscary', 'shulyatyeva', 'ceptaris', 'sadredin', 'coriell', 'terez', 'prismsport', 'worldvista', 'troughing', 'exfiltrated', 'badasses', 'mobihealthnews', 'kienitz', 'rebleed', 'presstime', 'ivd', 'bernik', 'mandateand', 'doesmr', 'andridge', 'eiopa', 'alphazero', 'schriock', 'tipirneni', 'priorities�', 'ucits', 'ex\\xadposed', 'oathbreaking', 'fxj', 'ochee', 'victrex', 'alll', 'inadvisably', '39m', 'upselling', 'emoji', 'zydelig', 'pillpack', 'outwhich', 'unlikelya', 'lebrikizumab', 'leaviss', 'silvinit', 'rpe65', 'percher', 'wanggaard', '13,919', 'fuct', 'burkhauser', 'olivetree', 'lbhi', 'muirhouse', 'anonymizing', 'reidentified', 'answerscataract', 'jarlov', 'hollowdweller', 'schonlein', 'overprotecting', 'urpilainen', 'pork-barrel', 'skarich', 'fdasia', 'hojat', '£100', 'tornier', 'supercommittee', 'termfiscal', '129:1815', 'ratpac', 'watchespn', 'faubion', 'landver', 'iscaro', 'landcolt', 'housea', 'choppering', 'malvey', 'metrokin', 'quirónsalud', 'pethokoukis', '£138', 'sicherer', 'waterminder', 'splunk', 'rangasamy', 'qga', 'missier', 'ioer', 'gaffed', 'vege', '£32bn']\n","['kingdomof', '', 'forwardnot', 'stakethe', 'nonemergencies', 'circumastance', 'r-nc', 'reynaga', 'irisin', 'cuntry', 'shkreli', 'thatdespite', 'forcehe', 'ntgr', 'bfsr', 'ceramides', 'otillar', 'scratchpad', 'mcritchie', 'enria', 'watchwordalthough', 'mosychuk', 'aedpa', 'dipierro', 'panelo', 'arconic', 'paynet', 'gorsuchs', 'suicidenow', 'kyrsten', 'freiborg', '5bn', 'depomed', 'amsellem', 'overperformances', 'thcb', 'privitera', 'legimately', 'burkan', 'zonszein', '10,554', 'brochez', 'pfapa', 'bingener', 'entresto', 'candidateand', 'zarxio', 'tdn2k', 'confounders', 'overcounted', 'rebirthed', 'altmaier', 'barcap', 'shithole', 'lignos', 'sciple', 'morens', 'reclosing', 'roadfor', 'unpeg', 'currencythe', 'somepart', 'allthat', 'staehr', 'baumblit', 'khorsand', 'innereye', 'topkill', '1+1', 'kilimnik', 'fralick', 'krautzberger', 'truog', 'luján', 'screenos', 'unbooked', 'ohions', 'sowanick', 'neurobiologically', 'killino', 'incel', 'nn27303398', 'rcep', 'piampongsant', 'oubina', 'nontraumatic', '21e', 'coeure', 'rabten', 'reichlin', 'klipp', 'theranos', 'brexit', 'ionis', 'msct', '≥25kg', '≥30kg', 'müller', 'issakainen', 'blasey', 'elugardo', 'unrigging', '40bn', '100tn', 'deprescribing', 'mulye', 'ilbd', 'torba', 'shiono', 'vemurafenib', 'dacarbazine', 'lynparza', 'cholnoky', 'jeoff', 'gangwisch', 'bernanke*', 'pended', 'rejoiceit', 'staikouras', 'lewandowskis', 'carcinoembryonic', 'bosiers', 'haridy', 'urodynamic', 'alembik', 'dvn', 'melland', 'florange', 'brexiteer', 'colectomy', 'pay+', 'rosanoff', 'knols', 'jobsbut', 'siloed', 'preauthorization', 'gaddie', 'mansplaining', 'esiner', 'beggaring', 'iezzi', '60tn', 'townswick', 'guidancesome', 'icagen', 'ecohelmet', 'world�', '�', 'fscai', 'scai', 'schnautz', 'proration', 'incentivised', 'mdv3100', 'zytiga', 'medivation', 'melsen', 'nunamaker', 'tudose', 'shingrix', 'indexuniverse', 'vxx', 'washpost', 'karoun', 'securityholder', 'yger', 'google+', '11,493', 'certifiied', 'orkambi', 'en+', 'pshg_p', '112bn', 'slavedriver', 'isrg', 'sharice', 'jfcom', 'woahhhh', 'nktr', 'opdivo', 'yervoy', 'immunotherapies', 'recapitalizating', 'dierksen', 'ambarella', 'herefor', 'gamblersor', 'contestantsthat', 'schreckreports', 'erdogans', 'beencooked', 'golbon', 'eubut', 'clintonness', 'giunchigliani', 'eteplirsen', 'storiesseasonal', 'viprinex', 'avenatti', 'jarris', 'gilholm', 'nontradeable', 'oanda', 'prizeroot', 'systemthe', 'pescanova', 'weekdo', 'unsterilised', 'them�', 'zemcik', 'immunochemical', 'papyracea', 'morcellation', 'houte', 'firdapse', 'kronthal', 'passporting', 'rotasiil', 'patrikis', 'cyad', 'comeyi', 'misdemeanourand', 'lomen', 'hoerth', 'baxalta', 'engag', 'percec', 'dovere', 'provention', 't1d', '**a', 'zavecz', 'hoepa', 'inquicker', 'baytex', 'do�', 'whcra', 'wharen', 'randomising', 'analogise', 'thoughtssome', '630m', 'assetswhich', 'banksthe', 'fibrillating', 'defibrillated', 'conlumino', 'slutkin', 'tjokrosaputro', 'dailyfx', 'ohiohealth', 'littlefinger', 'monologuing', 'dontos', 'lemtrada', 'presal', 'hematospermia', 'guindos', 'continueon', 'trueness', 'queered', 'nfld', 'venoplasty', 'unbuyable', 'btig', 'kopola', 'mäkinen', '33bn', '7bn', 'mythaholic', 'venge', 'scardina', 'isis8', 'whadya', '£200m', '£420m', '£7', '6bn', '£23bn', 'misgoverning', 'tyrannically', 'sobanko', 'filmalter', '£3', 'investmentwhile', 'wordelectoral', 'healthkit', 'parscale', 'ransomware', 'zulily', 'léonora', 'chiappelli', 'vestager', 'agentswho', 'jobfeared', 'dehejia', 'idfc', 'thepast', 'thatbehaviourright', '£500m', 'wauthier', 'cubicin', 'otezla', 'receptos', 'lunacies', 'coxibs', 'xme', 'thiswhile', 'hq2', 'catellanos', 'indivior', 'durata', 'sciutto', 'bigotted', 'lamestream', 'starndard', 'lifepoint', 'lpnt', 'ridaforolimus', 'mortifications', 'chozick', 'cyberespionage', 'alperovitch', 'crowdstrike', 'hackable', 'nkc', 'jcar015', '£69', '4bn', '13057', 'fornero', 'circumsized', 'pneumoliner', 'morcellated', 'inácio', 'bromiley', 'storiesempty', 'bakries', 'daca', 'kanefield', 'shugerman', 'strategised', 'reznikova', 'liptovsky', 'hradok', 'patheon', 'highertwitter', 'nyag', 'unremedied', 'molepost', 'solanezumab', 'pmis', 'believehigh', 'grexit', 'broemer', 'reince', '£28m', 'affo', 'puigdemont', 'trumpism', 'sportsafe', 'redtech', 'ursano', '16bn', 'warmbrodt', '204k', 'esformes', 'toigo', 'valvuloplasty', 'usmca', 'wrongi', 'klepin', 'worldchecking', 'amzn', 'tdjc', 'lockmaker', 'timesour', 'pinebridge', 'valuationsdiageo', 'themchinese', 'solarfun', 'curveglobal', 'margina', 'governmetn', 'tmunity', 'realised�', 'pr+', 'hospitalinspections', 'geltzeiler', 'bloomgren', 'nicolás', 'werent', 'okuwobi', 'riskscore', 'numberedat', 'premorbid', 'porchia', 'nifla', 'examination-rooms', 'overprescribe', 'espriet', 'irmat', 'optum', 'mfglobal', 'cadburys', 'earliestthough', 'verlinvest', 'mulacek', 'kotagal', 'schrder', 'perritano', 'flesvig', 'argumentsindeed', 'nymann', 'cuccinellis', 'lipus', 'vapers', 'akorn', 'putcha', 'sier', 'ultrastrong', 'kitakura', 'hntes', 'in�', 'wechat', 'wateringly', 'skylanders', 'entegra', 'nn25160928', 'prough', 'inzitari', 'yarros', 'charanpreet', 'dabhia', 'cellseven', 'licencethen', 'darwinistic', 'infinera', 'espp', '17bn', 'whaled', 'farkouh', 'restuccia', 'deconditioned', 'manouch', 'moshayedi', 'phylopatric', 'altarum', 'yieldco', 'gilvarry', 'docmorris', 'spokeo', 'retweeted', 'wasendorf', 'keytruda', 'non-executives', 'odendahl', 'saysthat', 'malvertising', 'camcorp', 'egzzz', 'medpage', 'ahier', 'generalisable', 'discussant', 'ticc', 'makuch', 'marketmakers', 'kolfage', 'hruksa', 'as�', 'overpromises', 'unspared', 'douda', 'spikily', 'egert', 'poloz', 'bolsonaro', 'ungallant', '7,042', 'litonjua', 'self-judgments', 'kondik', 'arshamian', 'lumentum', 'vcsels', 'rohret', 'capitala', 'amcap', 'emanuels', 'resolutionsmeaning', 'levelshave', 'semaya', 'healthtour', 't1c', 'vipshop', 'betterof', 'alreadyyou', 'cirmo', 'epidiolex', 'dravet', 'asnider', 'wyen', 'himselfwell', 'educatedthe', 'jbarro', 'trauttenberg', 'moonshoti', 'mmscf', 'lobley', 'point72', 'cakeshop', 'juurlink', 'qualifyand', 'mnn', 'gerspach', 'atypia', 'sefl', '55bn', '£69bn', '£14bn', 'rednecky', 'iiea', 'phenotyping', 'tijirit', 'preyss', 'plco', '2618', 'rebastinib', '3014', 'andhe', 'whatyou', 'thespecialist', 'takenplace', 'assistantsfollows', 'saikawa', 'inflam', 'workvery', 'anothervendor', 'trns', 'knighthead', 'rgy', 'obfr', 'enlightnment', 'doubleline', 'vidscription', 'ycmnet', 'yoshikami', 'mitigo', 'background-check', 'ap7', 'ca125', 'acquiror', 'nayda', 'pancioli', 'fischell', 'pcmh', 'leoning', 'threatsnixons', 'allegationseven', 'avoidably', 'under-estimation', 'obozo', 'blizzardy', 'entrec', 'treatmenteven', 'likelywould', 'weensure', 'oversimplifyingthe', 'thecataract', 'distanceor', 'matons', 'cithian', 'meiburg', 'mdbi', 'simps', 'flotus', 'stabbe', 'efsm', 'superinfection', 'sedgh', 'laclair', 'design\\xaded', 'rel\\xadeased', 'an\\xadnoying', 'bec\\xadause', 'lyxor', 'profitslindsay', 'jrgen', 'microgrid', 'vitalmedix', 'dexcom', 'missika', 'stefin', 'sherbin', 'serper', 'timbouctou', 'distributory', 'antiatherogenic', 'marinuzzi', '*minneapolis', '*fed', 'expiredmeaning', 'e004', 'emmanouilidis', 'devincow', 'doomsters', 'bowone', 'lowflationary', '456,789', 'witlessly', '65bn', 'pricings', 'dtegn', 'interspersal', 'nadella', 'ibrutinib', 'attackonly', 'hollyfrontier', '59,480', 'peña', 'deprescribe', 'tradereporter', 'goitein', 'iome', 'vissers', 'nppc', 'tanona', 'morbidities', 'biamonte', 'rmds', 'ellmers', 'airfarewatchdog', 'excisional', 'cryptocurrency', 'coincheck', 'cataltepe', 'biomarin', '2x1012', '6,862', '72,000,000', 'flowback', 'vbp', 'bisaro', 'hollak', 'leadiant', 'prevenion', 'newsnew', 'sederer', 'medicareon', 'aflappin', 'wesee', 'november1', '34,517,250', 'skierka', 'scavino', 'kobre', 'goate', 'younkin', 'vengroff', 'answersis', 'lbbb', '28bn', 'weekwant', 'steidley', 'gonzález', '58bn', 'protégés', 'marsupialization', 'trutina', 'delreal', 'aideven', 'leronlimab', 'cd02', 'pcatest', 'hemauer', 'savolitinib', 'maoa', 'luthman', 'liontrust', 'emph', 'trumponomics', 'dismuke', 'cassivi', 'insurancenot', 'intrapartum', 'shiarly', 'hepatologist', 'gracas', 'heran', 'gphin', 'tedprize', 'faltersremember', 'tmfdeej', 'gothamist', 'kashoggi', 'reguire', 'cringeworthy', 'underwhelm', 'flavanol', 'donationsthis', 'nccn', 'mcaleenan', 'coalitionperhaps', 'springloaded', '35:34', 'reluctances', 'orexo', 'ltip', 'indexiq', 'overexert', 'leftciti', 'lagorio', 'sedran', 'gevo', 'tarnef', 'atiqi', 'grandparental', 'trembowicz', 'wyderko', 'botheration', 'creuzfeldt', 'bauler', 'mainfirst', 'soleoutcome', 'whenall', 'anecklace', 'thebest', 'ourchest', 'feedingtube', 'regionrussia', 'itone', 'corpbanca', 'oxybate', '12bn', '9bn', 'forward‐looking', '+2349051208634when', 'srouji', 'pehub', 'corbat', 'corrollary', 'casamassimo', 'lnder', 'answershow', 'nondevelopmental', 'glitazones', 'hyperinfection', 'banaei', 'kartsotis', 'rerunor', 'sherzan', 'ezechiel', 'copic', 'bcma', 'haughom', '2000including', 'oklahomathe', 'sivignon', 'bdsi', 'detasseling', 'cornpicking', 'judgy', 'meetingwithout', 'evidencethat', 'dumain', 'kapla', 'frideres', 'ossoff', '£2,800', 'contentthis', 'romneyworld', 'bbby', 'partcipate', 'evalulation', 'nematicides', 'sowhile', 'plansout', 'orderto', 'furthermarket', 'lesscompetition', 'thelegislation', 'propafenone', 'sotalol', 'dotard', 'avanir', 'nuedexta', '7751', '7733', '8035', '8036', 'exercycle', 'linewhile', 'shitstains', 'nonrenewal', 'somebodyie', 'avav', 'lukashevich', 'oosterveld', 'awfulizing', 'janumet', 'dixhouse', 'ingel', 'hb56', 'kwarteng', 'tv+', 'nflx', 'medcity', 'traister', 'redocument', 'chondrocyte', 'loncon', '£903m', 'aquadvantage', 'glennbeck', 'gbtv', 'douchiness', 'policywise', 'friedmen', 'ferreyr', 'flexitime', 'shaub', 'posthurricane', 'prehurricane', 'volunteersafter', 'texas.during', 'clavulanate', 'piperacillin', 'tazobactam', 'zosyn', 'bagios', 'wapo', 'snowblue', 'attachedmentsi', 'nebank', 'saczynski', 'crosstabs', 'aa+', 'neratinib', 'maelstroms', 'nongroup', 'netback', 'ncib', 'misattuned', 'ctbs', 'erbb1', 'capecitabine', 'urdan', 'unsubsidised', '11,770', '47,080', 'plassat', 'turnround', '€2', '45bn', 'rebok', 'beicker', 'senatewhich', 'pressit', 'jcpoa', 'boustani', 'peterschmitt', 'guessous', 'mediawatch', 'biegelsen', 'dbkgn', 'jiankui', 'bsps', '88m', 'pzena', 'sisolak', 'unblinded', 'olypmic', 'nonconvertible', 'pelosis', '22bn', 'rb51', 'gipsa', 'uromedica', 'dupilumab', 'telogen', 'effluvium', 'totalis', 'carbon-free', 'sotu', 'nellen', 'peginesatide', 'derrough', 'polle', 'baltensperger', 'chondroplasty', 'convo', 'salveson', 'ccdoe', 'senblumenthal', 'nflcommish', 'rucaparib', 'watergoing', 'dabby', 'pfass', '•olden', '2°c', 'deppression', 'kalanick', 'nonpharmacological', 'nonallergic', 'galuccio', 'toweljune', 'knowmr', 'trenowden', 'portyghee', 'shulkin', 'nevertrumpers', '£1,000', '£7,000', 'giegold', 'ablations', 'matousek', 'shawkut', 'factualwhat', 'injectioncannot', '32,898', 'takethe', 'rylee', 'coene', 'guaidó', 'benisek', 'prwora', 'cimzia', 'singiser', 'mor208', '18,378', 'concernedthe', 'partiespaused', 'cellectis', 'government–', 'eargo', 'slavitt', 'cimziaâ®', 'triangulator', 'mnsure', 'gablofen', 'olness', 'marette', 'swetnick', 'pnk', 'crème', 'brûlée', 'behsudi', 'zacary', 'techeven', 'tromps', 'huttenhower', '79mr', 'takushi', 'ishikura', 'aledade', 'mostashari', 'costscosts', 'topx', 'enestro', 'parsortix', 'eskesen', 'azpia', 'economicsit', 'sweatily', 'dkr53bn', 'czapp', 'drrich', 'manchik', 'seriousconsequences', 'abbvie', 'bondholdings', 'costerg', 'cyence', 'kalobios', 'brainlessly', 'reify', 'oookaay', 'chatbots', 'ameringen', 'csalp', 'ttip', 'realdonaldtrump', 'undisbursed', 'playbookers', 'mutualization', 'cimzia®', 'bouhara', 'foroud', 'buoyant�', 'foolfest', 'toomeys', 'fsoc', 'kolhmann', 'sp+', 'paymentsnot', 'guglielmotti', 'multigenic', 'nidawi', 'wergin', 'minusma', 'swedo', 'lozman', 'putinwho', 'poehling', '9,580', '12,902', 'otherhand', 'car-t', 'ecigarettes', '1trn', 'luschini', 'preteenagers', '£1,381', '£9,300', 'famewave', 'albence', 'toldme', 'whatthey', 'thatonly', 'orher', 'forcompetence', 'ipaa', 'ungari', 'sageen', 'drive-left', 'beaverness', 'runkeeper', 'kalir', 'heartwire', 'hernández', 'avocadoes', 'garzotto', 'youdid', 'iley', 'umich', 'biggish', 'unrevoked', 'kohane', 'prewarned', 'rutqvist', 'coinbase', 'ehrsam', 'blockchain', 'tomdispatch', 'slate®', '£20', '£2', 'financialisation', 'housingthe', 'redicting', 'cemma', 'caitlinzemma', '134m', '346bn', 'washeteria', 'ishrak', 'hoogendyk', 'adheretech', 'abruptlyas', 'truty', 'withthemselves', 'andstrike', 'leiomyosarcomas', 'woulndt', 'sederberg', '2650bc', 'edro', 'gloviczki', 'jived', '5thbday', 'obamacarethe', 'democratsthat', 'krzanich', '10nm', 'weightsit', 'seasonand', 'scailex', 'scix', 'linora', 'consonery', 'brexiteers', 'hypertriglyceridemic', 'sequencingin', 'stefanek', 'cordovilla', 'mandrola', '6,584', 'chrystia', '聳', '25:44', '45and', 'boeckler', 'npdb', 'glausser', 'ricol', 'ovitt', 'cpri', 'aqlan', '9202', 'qan', 'epocrates', 'drummy', 'schnatter', 'hawkinberry', 'shuanghui', 'josé', '£50m', 'polkes', 'untether', 'interstim', 'liposuctioned', 'cirka', 'horsager', 'lowest-yielding', 'yaradua', 'vsoe', 'cisgendered', 'zoetis', 'nonproven', 'schlapp', '244m', 'posess', 'razaqzada', 'spanberger', 'inspra', 'elseplease', 'tothem', 'becausei', 'nechelput', 'esvelt', 'presidentgreens', 'forthare', 'restasis', 'intromission', 'dodick', 'ngdp', 'lnkd', 'panl', 'lubitz', 'skinactivist', '£300bn', 'wandoan', '200bn', 'bokkelen', 'thinkfor', 'thefinancial', 'tohim', 'fragmentedsystem', 'systemwould', 'treattheir', 'changethe', 'cispa', 'morethe', 'tmall', 'micrometastases', 'jumbomatic', 'dourson', 'dealbut', 'birtel', 'deplorables', 'weightsif', 'balwani', 'suahasil', 'nazara', '£22bn', 'zanny', 'flatto', 'culpritboth', 'unobligated', 'vilnerable', 'psychoanalyzing', 'ftfm', 'smartwatch', 'sabbs', 'coratti', 'omfif', 'loréal', 'nawana', 'wage-and-benefit', 'afib', 'acase', 'surgeonleft', 'publicityconcerning', 'iwatch', 'proco', 'zarzeczny', 'giventhough', 'hafle', 'demétrio', 'postuma', 'jannetta', 'chemed', 'glendevon', 'hampshirenew', 'nieca', 'abinbev', '£44', 'bakish', '15,302', '30am', '50u', '25u', 'symc', '13bn', '60bn', '100bn', 'preventedif', 'triatomine', 'broadridge', 'advicewhile', 'hodis', 'tabachnick', 'overcapitalized', 'insoll', 'fidessa', 'netbacks', 'sfr210', 'hummler', 'perfectionistic', 'progressivetokyo', 'bruera', '267,700', 'moscillo', 'effectuating', 'camuñez', 'michiro', '6tn', 'pão', 'açúcar', 'nesterczuk', '2891', 'rttgers', 'estis', 'dinamit', 'giuricin', 'sinofert', 'agraz', 'storiesweighing', 'cryptocurrencies', 'schmidle', 'cyberberkut', 'daign', 'ccilia', 'denverton', 'whohas', 'medicalert', 'abracelet', 'aboutthe', 'beallergic', 'diseasethat', 'factthat', 'diagnosedproperly', 'thyroidcancer', 'ryleigh', 'speechthere', 'answerscochlear', 'westerbeck', 'resect', 'roguishly', 'iex', 'nazarali', 'midwests', 'securequity', 'axovant', 'nelotanserin', 'roivant', 'kirschman', 'stormchaser', 'evofem', 'ocare', 'xeljanz', 'entyvio', 'arbitraged', 'cgus', 'tilray', 'lixivaptan', 'tlantic', 'atvi', 'schoenebaum', 'invensense', 'outsideif', '£750', '£1000', 'jiade', 'unipec', 'pericardiectomy', 'conibear', 'superscary', 'shulyatyeva', 'ceptaris', 'sadredin', 'coriell', 'terez', 'prismsport', 'worldvista', 'troughing', 'exfiltrated', 'badasses', 'mobihealthnews', 'kienitz', 'rebleed', 'presstime', 'ivd', 'bernik', 'mandateand', 'doesmr', 'andridge', 'eiopa', 'alphazero', 'schriock', 'tipirneni', 'priorities�', 'ucits', 'ex\\xadposed', 'oathbreaking', 'fxj', 'ochee', 'victrex', 'alll', 'inadvisably', '39m', 'upselling', 'emoji', 'zydelig', 'pillpack', 'outwhich', 'unlikelya', 'lebrikizumab', 'leaviss', 'silvinit', 'rpe65', 'percher', 'wanggaard', '13,919', 'fuct', 'burkhauser', 'olivetree', 'lbhi', 'muirhouse', 'anonymizing', 'reidentified', 'answerscataract', 'jarlov', 'hollowdweller', 'schonlein', 'overprotecting', 'urpilainen', 'pork-barrel', 'skarich', 'fdasia', 'hojat', '£100', 'tornier', 'supercommittee', 'termfiscal', '129:1815', 'ratpac', 'watchespn', 'faubion', 'landver', 'iscaro', 'landcolt', 'housea', 'choppering', 'malvey', 'metrokin', 'quirónsalud', 'pethokoukis', '£138', 'sicherer', 'waterminder', 'splunk', 'rangasamy', 'qga', 'missier', 'ioer', 'gaffed', 'vege', '£32bn']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yAN7rJ7YiN99","colab_type":"code","colab":{}},"source":["corpus_words = []\n","for sent in corpus['sentence']:\n","  for word in sent:\n","    if word not in not_in:\n","      if word not in corpus_words:\n","        corpus_words.append(word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeK_vo9yjKQX","colab_type":"code","colab":{}},"source":["mapping = []\n","custom_mapping = {0 : 0}\n","idx = 1\n","for w in corpus_words:\n","  mapping.append(w2v[w2i[w]])\n","  custom_mapping[w] = idx\n","  idx+=1\n","\n","pad = [0]*300\n","mapping.insert(0,pad)\n","\n","mapping = np.array(mapping)\n","\n","for i in range(len(not_in)):\n","  rnd = np.array([np.random.uniform(-1,1,300)])\n","  mapping = np.append(mapping,rnd,axis = 0)\n","  custom_mapping[not_in[i]] = idx\n","  idx+=1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IchsZgsUDq7j","colab_type":"code","colab":{}},"source":["import pickle\n","\n","with open(\"custom_mapping.pickle\",'wb') as f:\n","  pickle.dump(custom_mapping, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IveYvoItpz5h","colab_type":"code","outputId":"74525937-50c3-4d47-e211-638f9bbcf285","executionInfo":{"status":"ok","timestamp":1580819359122,"user_tz":-330,"elapsed":3143,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(mapping[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"VpSiph_13ddN","colab_type":"code","outputId":"8dcb80e9-c5e0-49b1-90f1-ee1be87adb36","executionInfo":{"status":"ok","timestamp":1580819360688,"user_tz":-330,"elapsed":4174,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(mapping) == len(corpus_words) + len(not_in) + 1"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"dVvXmAtd50qN","colab_type":"code","colab":{}},"source":["max_len = 0\n","this = 0\n","for x in corpus['sentence']:\n","  if len(x)>max_len:\n","    max_len = len(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7UMMjt7p_iA","colab_type":"code","colab":{}},"source":["data = []\n","label = []\n","idx = 0\n","padding = 2\n","for sent in corpus['sentence']:\n","  s = [0]*padding\n","  for word in sent:\n","    s.append(custom_mapping[word])\n","  while(len(s) < max_len + 2*padding):\n","    s.append(0)\n","  data.append(s)\n","  label.append(corpus['gold_label'][idx])\n","  idx+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4qdMCmBx-0F","colab_type":"code","colab":{}},"source":["data = np.array(data)\n","label = np.array(label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f98103e7-abd9-4efd-c547-409be379b4c1","executionInfo":{"status":"ok","timestamp":1580918310527,"user_tz":-330,"elapsed":38,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"id":"CQr1VsY74d7B","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(data[907])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["593"]},"metadata":{"tags":[]},"execution_count":22},{"output_type":"execute_result","data":{"text/plain":["593"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"7QXRglL818XD","colab_type":"code","colab":{}},"source":["data_indices = []\n","for i in range(len(label)):\n","  data_indices.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-50Bz2nODYbd","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GI1RkrQG7nu2","colab_type":"code","colab":{}},"source":["from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gsa0pLUK1Znq","colab_type":"code","colab":{}},"source":["seed = 2374"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQmFv-SV1Vsy","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","split = 0.2\n","splits = train_test_split(data_indices, test_size = split, random_state = seed, stratify = label)\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","train_sampler = SubsetRandomSampler(splits[0])\n","valid_sampler = SubsetRandomSampler(splits[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A97D79lm_sIw","colab_type":"code","colab":{}},"source":["class text_dataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, data, label):\n","        self.data = data\n","        self.labels = label\n","\n","        \n","    def __len__(self):\n","        return len(self.labels)\n","        \n","    def __getitem__(self, idx):\n","\n","        l = torch.tensor(self.labels[idx]).long()\n","        sentence = torch.tensor(self.data[idx]).long()\n","\n","        return sentence, l\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoMSOGKqDvps","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class CNN_Text(nn.Module):        \n","\n","    def __init__(self, args, weight_matrix, train_weights=False):\n","        super(CNN_Text, self).__init__()\n","        self.args = args\n","        self.embed, num_embeddings, embedding_dim = self.create_emb_layer(weight_matrix, train_weights)\n","\n","        V = num_embeddings\n","        D = embedding_dim\n","        C = args.class_num\n","        Ci = 1\n","        Co = args.kernel_num\n","        Ks = args.kernel_sizes\n","\n","        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n","        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n","        '''\n","        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n","        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n","        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n","        '''\n","        self.dropout = nn.Dropout(args.dropout)\n","        self.fc1 = nn.Linear(len(Ks)*Co, C)\n","    \n","    def create_emb_layer(self, weight_matrix, train_weights=False):\n","        num_embeddings, embedding_dim = weight_matrix.size()\n","        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n","        emb_layer.load_state_dict({'weight': weight_matrix})\n","        if train_weights:\n","            emb_layer.weight.requires_grad = True\n","        else:\n","            emb_layer.weight.requires_grad = False\n","\n","        return emb_layer, num_embeddings, embedding_dim\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.embed(x)  # (N, W, D)\n","        # print(x.size())\n","        \n","        if self.args.static:\n","            x = Variable(x)\n","\n","        x = x.unsqueeze(1)  # (N, Ci, W, D)\n","\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n","\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","\n","        x = torch.cat(x, 1)\n","\n","        '''\n","        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n","        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n","        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n","        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n","        '''\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-6-nHIRELWr","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import torch\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","\n","def train(train_iter, dev_iter, model, args):\n","    if args.cuda:\n","        model.cuda()\n","        print(\"On Cuda\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.1)\n","    weights = [1,10]\n","    weights = torch.FloatTensor(weights).cuda()\n","    criterion = nn.CrossEntropyLoss(weight=weights)\n","\n","    steps = 0\n","    best_acc = 0\n","    last_step = 0\n","    to_return = {}\n","    max_so_far = 0.0\n","    for epoch in range(1, args.epochs+1):\n","        model.train()\n","        train_preds = None\n","        train_labels = None\n","        running_loss = 0.0\n","        total_loss = 0.0\n","        mini_batch = 0\n","        print(\"epoch: \",epoch)\n","        scheduler.step()\n","\n","        for feature, labels in train_iter:\n","            mini_batch+=1\n","            \n","            if args.cuda:\n","                feature, labels = feature.cuda(), labels.cuda()\n","\n","            optimizer.zero_grad()\n","            outputs = model(feature)\n","\n","            # print('outputs size', outputs.size())\n","            # print('target vector', target.size())\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            steps += 1\n","            if train_preds is None or train_labels is None:\n","              train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","              train_labels = labels.cpu().numpy().flatten()\n","            else:\n","              train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","              train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n","\n","\n","            loss.backward()\n","            optimizer.step()\n","            # scheduler.step()\n","\n","            if mini_batch % 100 == 0:    # print every 100 mini-batches\n","              print('[%d, %5d] loss: %.3f' % (epoch , mini_batch , running_loss / 100))\n","              running_loss = 0.0\n","        \n","        print(\"Training loss in epoch %d is %.3f\" % (epoch , total_loss / len(train_loader)))\n","        print(\"Training accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(train_labels, train_preds) * 100))\n","        print(\"Training precision in epoch %d is %.3f\" % (epoch , precision_score(train_labels, train_preds) * 100))\n","        print(\"Training recall in epoch %d is %.3f\" % (epoch , recall_score(train_labels, train_preds) * 100))\n","        print(\"Training F1-score in epoch %d is %.3f\" % (epoch, f1_score(train_labels, train_preds) * 100))\n","        \n","        eval(dev_iter, model, criterion, epoch, args, to_return, max_so_far)\n","        print()\n","        print(\"-\"*90)\n","        print()\n","        \n","    return to_return\n","\n","\n","def eval(data_iter, model, criterion, epoch, args, to_return, max_so_far):\n","    \n","    model.eval()\n","    test_loss = 0.0\n","    test_preds = None\n","    test_labels = None\n","    test_inputs = None\n","    # corrects, avg_loss = 0, 0\n","    with torch.no_grad():\n","\n","        for data in data_iter:\n","            inputs, labels = data\n","            if args.cuda:\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","          \n","            test_loss += loss.item()\n","            if test_preds is None or test_labels is None:\n","                test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","                test_labels = labels.cpu().numpy().flatten()\n","                test_inputs = inputs.cpu().numpy()\n","            else:\n","                test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","                test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n","                test_inputs = np.concatenate((test_inputs, inputs.cpu().numpy()), axis = 0)\n","\n","            # test_accuracy += flat_accuracy(outputs[0], labels)\n","    \n","    print(\"Test loss in epoch %d is %.3f\" % (epoch, test_loss / len(val_loader)))\n","    print(\"Test accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(test_labels, test_preds) * 100))\n","    print(\"Test precision in epoch %d is %.3f\" % (epoch, precision_score(test_labels, test_preds) * 100))\n","    print(\"Test recall in epoch %d is %.3f\" % (epoch, recall_score(test_labels, test_preds) * 100))\n","    print(\"Test F1-score in epoch %d is %.3f\" % (epoch, f1_score(test_labels, test_preds) * 100))\n","    if f1_score(test_labels, test_preds)*100 > max_so_far:\n","      max_so_far = f1_score(test_labels, test_preds)*100\n","      to_return['pred'] = test_preds\n","      to_return['label'] = test_labels\n","      to_return['inputs'] = test_inputs\n","\n","\n","\n","def predict(text, model, text_field, label_feild, cuda_flag):\n","    assert isinstance(text, str)\n","    model.eval()\n","    # text = text_field.tokenize(text)\n","    text = text_field.preprocess(text)\n","    text = [[text_field.vocab.stoi[x] for x in text]]\n","    x = torch.tensor(text)\n","    x = autograd.Variable(x)\n","    if cuda_flag:\n","        x = x.cuda()\n","    print(x)\n","    output = model(x)\n","    _, predicted = torch.max(output, 1)\n","    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n","    return label_feild.vocab.itos[predicted.data[0]+1]\n","\n","\n","def save(model, save_dir, save_prefix, steps):\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    save_prefix = os.path.join(save_dir, save_prefix)\n","    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n","    torch.save(model.state_dict(), save_path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlKM4aZj4vH2","colab_type":"code","colab":{}},"source":["weight_matrix = torch.tensor(mapping)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDSNmGqiEanf","colab_type":"code","outputId":"28cacbf0-2f40-441c-9041-783b86626884","executionInfo":{"status":"ok","timestamp":1580919137791,"user_tz":-330,"elapsed":2101,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["import os\n","import argparse\n","import datetime\n","import torch\n","\n","\n","parser = argparse.ArgumentParser(description='CNN text classificer')\n","# learning\n","parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n","parser.add_argument('-epochs', type=int, default=30, help='number of epochs for train [default: 256]')\n","parser.add_argument('-batch-size', type=int, default=64, help='batch size for training [default: 64]')\n","parser.add_argument('-log-interval',  type=int, default=1,   help='how many steps to wait before logging training status [default: 1]')\n","parser.add_argument('-test-interval', type=int, default=100, help='how many steps to wait before testing [default: 100]')\n","parser.add_argument('-save-interval', type=int, default=500, help='how many steps to wait before saving [default:500]')\n","parser.add_argument('-save-dir', type=str, default='try1', help='where to save the snapshot')\n","parser.add_argument('-early-stop', type=int, default=1000, help='iteration numbers to stop without performance increasing')\n","parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n","# data \n","parser.add_argument('-shuffle', action='store_true', default=True, help='shuffle the data every epoch')\n","# model\n","parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n","parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n","parser.add_argument('-embed-dim''''  ''', type=int, default=300, help='number of embedding dimension [default: 128]')\n","parser.add_argument('-kernel-num', type=int, default=100, help='number of each kind of kernel')\n","parser.add_argument('-kernel-sizes', type=str, default='3,4,5', help='comma-separated kernel size to use for convolution')\n","parser.add_argument('-static', action='store_true', default=True, help='fix the embedding')\n","# device\n","parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n","parser.add_argument('-no-cuda', action='store_true', default=False, help='disable the gpu')\n","# option\n","parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n","parser.add_argument('-predict', type=str, default=None, help='predict the sentence given')\n","parser.add_argument('-test', action='store_true', default=False, help='train or test')\n","args = parser.parse_args(args=[])\n","\n","\n","# load data \n","print(\"\\nLoading data...\")\n","# text_field = data.Field(lower=True)\n","# label_field = data.Field(sequential=False)\n","# train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n","dataset = text_dataset(data, label)\n","train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","                                           sampler = train_sampler)\n","val_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","                                           sampler = valid_sampler)\n","\n","# train_iter, dev_iter, test_iter = sst(text_field, label_field, device=-1, repeat=False)\n","\n","\n","# update args and print\n","args.embed_num = weight_matrix.size()[1]\n","args.class_num = 2 #binary\n","args.cuda = (not args.no_cuda) and torch.cuda.is_available(); del args.no_cuda\n","args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n","args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n","\n","print(\"\\nParameters:\")\n","for attr, value in sorted(args.__dict__.items()):\n","    print(\"\\t{}={}\".format(attr.upper(), value))\n","print(\"Training sentences: \", int(len(dataset)*(1-split)))\n","print(\"Validation sentences: \", int(len(dataset)*(split)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Loading data...\n","\n","Parameters:\n","\tBATCH_SIZE=64\n","\tCLASS_NUM=2\n","\tCUDA=True\n","\tDEVICE=-1\n","\tDROPOUT=0.5\n","\tEARLY_STOP=1000\n","\tEMBED_DIM  =300\n","\tEMBED_NUM=300\n","\tEPOCHS=30\n","\tKERNEL_NUM=100\n","\tKERNEL_SIZES=[3, 4, 5]\n","\tLOG_INTERVAL=1\n","\tLR=0.001\n","\tMAX_NORM=3.0\n","\tPREDICT=None\n","\tSAVE_BEST=True\n","\tSAVE_DIR=try1/2020-02-05_16-12-16\n","\tSAVE_INTERVAL=500\n","\tSHUFFLE=True\n","\tSNAPSHOT=None\n","\tSTATIC=True\n","\tTEST=False\n","\tTEST_INTERVAL=100\n","Training sentences:  10400\n","Validation sentences:  2600\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04O8R9EVJSDJ","colab_type":"code","colab":{}},"source":["# model\n","cnn = CNN_Text(args, weight_matrix)\n","if args.snapshot is not None:\n","    print('\\nLoading model from {}...'.format(args.snapshot))\n","    cnn.load_state_dict(torch.load(args.snapshot))\n","\n","if args.cuda:\n","    torch.cuda.set_device(args.device)\n","    cnn = cnn.cuda()\n","        \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qI16xnRCJuB0","colab_type":"code","outputId":"6e6adfb6-8344-4903-b6a7-d6a896995a4f","executionInfo":{"status":"ok","timestamp":1580919238584,"user_tz":-330,"elapsed":99428,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","# train or predict\n","# static\n","if args.predict is not None:\n","    label = predict(args.predict, cnn, text_field, label_field, args.cuda)\n","    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n","elif args.test:\n","    try:\n","        eval(test_iter, cnn, args) \n","    except Exception as e:\n","        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n","else:\n","    print()\n","    try:\n","        val = train(train_loader, val_loader, cnn, args)\n","    except KeyboardInterrupt:\n","        print('\\n' + '-' * 89)\n","        print('Exiting from training early')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","On Cuda\n","epoch:  1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1,   100] loss: 0.529\n","Training loss in epoch 1 is 0.498\n","Training accuracy in epoch 1 is 72.394\n","Training precision in epoch 1 is 26.119\n","Training recall in epoch 1 is 80.310\n","Training F1-score in epoch 1 is 39.418\n","Test loss in epoch 1 is 0.343\n","Test accuracy in epoch 1 is 85.231\n","Test precision in epoch 1 is 42.390\n","Test recall in epoch 1 is 89.003\n","Test F1-score in epoch 1 is 57.428\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  2\n","[2,   100] loss: 0.359\n","Training loss in epoch 2 is 0.356\n","Training accuracy in epoch 2 is 83.000\n","Training precision in epoch 2 is 38.414\n","Training recall in epoch 2 is 86.242\n","Training F1-score in epoch 2 is 53.153\n","Test loss in epoch 2 is 0.308\n","Test accuracy in epoch 2 is 84.000\n","Test precision in epoch 2 is 40.516\n","Test recall in epoch 2 is 91.753\n","Test F1-score in epoch 2 is 56.211\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  3\n","[3,   100] loss: 0.256\n","Training loss in epoch 3 is 0.267\n","Training accuracy in epoch 3 is 87.673\n","Training precision in epoch 3 is 47.338\n","Training recall in epoch 3 is 90.972\n","Training F1-score in epoch 3 is 62.272\n","Test loss in epoch 3 is 0.323\n","Test accuracy in epoch 3 is 90.115\n","Test precision in epoch 3 is 53.899\n","Test recall in epoch 3 is 80.756\n","Test F1-score in epoch 3 is 64.649\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  4\n","[4,   100] loss: 0.211\n","Training loss in epoch 4 is 0.212\n","Training accuracy in epoch 4 is 90.096\n","Training precision in epoch 4 is 53.271\n","Training recall in epoch 4 is 93.121\n","Training F1-score in epoch 4 is 67.772\n","Test loss in epoch 4 is 0.284\n","Test accuracy in epoch 4 is 87.000\n","Test precision in epoch 4 is 45.927\n","Test recall in epoch 4 is 91.065\n","Test F1-score in epoch 4 is 61.060\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  5\n","[5,   100] loss: 0.157\n","Training loss in epoch 5 is 0.161\n","Training accuracy in epoch 5 is 92.394\n","Training precision in epoch 5 is 60.076\n","Training recall in epoch 5 is 95.357\n","Training F1-score in epoch 5 is 73.712\n","Test loss in epoch 5 is 0.386\n","Test accuracy in epoch 5 is 92.923\n","Test precision in epoch 5 is 66.361\n","Test recall in epoch 5 is 74.570\n","Test F1-score in epoch 5 is 70.227\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  6\n","[6,   100] loss: 0.130\n","Training loss in epoch 6 is 0.131\n","Training accuracy in epoch 6 is 94.308\n","Training precision in epoch 6 is 67.086\n","Training recall in epoch 6 is 96.389\n","Training F1-score in epoch 6 is 79.111\n","Test loss in epoch 6 is 0.317\n","Test accuracy in epoch 6 is 91.462\n","Test precision in epoch 6 is 58.234\n","Test recall in epoch 6 is 83.849\n","Test F1-score in epoch 6 is 68.732\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  7\n","[7,   100] loss: 0.106\n","Training loss in epoch 7 is 0.107\n","Training accuracy in epoch 7 is 95.356\n","Training precision in epoch 7 is 71.492\n","Training recall in epoch 7 is 97.248\n","Training F1-score in epoch 7 is 82.404\n","Test loss in epoch 7 is 0.300\n","Test accuracy in epoch 7 is 90.231\n","Test precision in epoch 7 is 53.978\n","Test recall in epoch 7 is 86.254\n","Test F1-score in epoch 7 is 66.402\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  8\n","[8,   100] loss: 0.080\n","Training loss in epoch 8 is 0.083\n","Training accuracy in epoch 8 is 96.471\n","Training precision in epoch 8 is 76.892\n","Training recall in epoch 8 is 97.850\n","Training F1-score in epoch 8 is 86.114\n","Test loss in epoch 8 is 0.493\n","Test accuracy in epoch 8 is 93.154\n","Test precision in epoch 8 is 69.550\n","Test recall in epoch 8 is 69.072\n","Test F1-score in epoch 8 is 69.310\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  9\n","[9,   100] loss: 0.064\n","Training loss in epoch 9 is 0.069\n","Training accuracy in epoch 9 is 97.058\n","Training precision in epoch 9 is 79.944\n","Training recall in epoch 9 is 98.366\n","Training F1-score in epoch 9 is 88.204\n","Test loss in epoch 9 is 0.379\n","Test accuracy in epoch 9 is 91.808\n","Test precision in epoch 9 is 59.750\n","Test recall in epoch 9 is 82.131\n","Test F1-score in epoch 9 is 69.175\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  10\n","[10,   100] loss: 0.062\n","Training loss in epoch 10 is 0.060\n","Training accuracy in epoch 10 is 97.500\n","Training precision in epoch 10 is 82.459\n","Training recall in epoch 10 is 98.624\n","Training F1-score in epoch 10 is 89.820\n","Test loss in epoch 10 is 0.435\n","Test accuracy in epoch 10 is 92.615\n","Test precision in epoch 10 is 64.431\n","Test recall in epoch 10 is 75.945\n","Test F1-score in epoch 10 is 69.716\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  11\n","[11,   100] loss: 0.049\n","Training loss in epoch 11 is 0.053\n","Training accuracy in epoch 11 is 97.923\n","Training precision in epoch 11 is 85.152\n","Training recall in epoch 11 is 98.624\n","Training F1-score in epoch 11 is 91.394\n","Test loss in epoch 11 is 0.414\n","Test accuracy in epoch 11 is 92.269\n","Test precision in epoch 11 is 62.097\n","Test recall in epoch 11 is 79.381\n","Test F1-score in epoch 11 is 69.683\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  12\n","[12,   100] loss: 0.040\n","Training loss in epoch 12 is 0.041\n","Training accuracy in epoch 12 is 98.538\n","Training precision in epoch 12 is 89.095\n","Training recall in epoch 12 is 99.054\n","Training F1-score in epoch 12 is 93.811\n","Test loss in epoch 12 is 0.485\n","Test accuracy in epoch 12 is 92.923\n","Test precision in epoch 12 is 67.657\n","Test recall in epoch 12 is 70.447\n","Test F1-score in epoch 12 is 69.024\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  13\n","[13,   100] loss: 0.032\n","Training loss in epoch 13 is 0.033\n","Training accuracy in epoch 13 is 98.904\n","Training precision in epoch 13 is 91.397\n","Training recall in epoch 13 is 99.570\n","Training F1-score in epoch 13 is 95.309\n","Test loss in epoch 13 is 0.498\n","Test accuracy in epoch 13 is 92.808\n","Test precision in epoch 13 is 67.105\n","Test recall in epoch 13 is 70.103\n","Test F1-score in epoch 13 is 68.571\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  14\n","[14,   100] loss: 0.028\n","Training loss in epoch 14 is 0.028\n","Training accuracy in epoch 14 is 99.048\n","Training precision in epoch 14 is 92.289\n","Training recall in epoch 14 is 99.828\n","Training F1-score in epoch 14 is 95.911\n","Test loss in epoch 14 is 0.500\n","Test accuracy in epoch 14 is 93.115\n","Test precision in epoch 14 is 68.792\n","Test recall in epoch 14 is 70.447\n","Test F1-score in epoch 14 is 69.610\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  15\n","[15,   100] loss: 0.028\n","Training loss in epoch 15 is 0.028\n","Training accuracy in epoch 15 is 99.096\n","Training precision in epoch 15 is 92.863\n","Training recall in epoch 15 is 99.570\n","Training F1-score in epoch 15 is 96.100\n","Test loss in epoch 15 is 0.467\n","Test accuracy in epoch 15 is 92.923\n","Test precision in epoch 15 is 66.877\n","Test recall in epoch 15 is 72.852\n","Test F1-score in epoch 15 is 69.737\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  16\n","[16,   100] loss: 0.024\n","Training loss in epoch 16 is 0.024\n","Training accuracy in epoch 16 is 99.279\n","Training precision in epoch 16 is 94.156\n","Training recall in epoch 16 is 99.742\n","Training F1-score in epoch 16 is 96.868\n","Test loss in epoch 16 is 0.476\n","Test accuracy in epoch 16 is 93.038\n","Test precision in epoch 16 is 67.516\n","Test recall in epoch 16 is 72.852\n","Test F1-score in epoch 16 is 70.083\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  17\n","[17,   100] loss: 0.024\n","Training loss in epoch 17 is 0.022\n","Training accuracy in epoch 17 is 99.317\n","Training precision in epoch 17 is 94.463\n","Training recall in epoch 17 is 99.742\n","Training F1-score in epoch 17 is 97.031\n","Test loss in epoch 17 is 0.520\n","Test accuracy in epoch 17 is 93.231\n","Test precision in epoch 17 is 69.492\n","Test recall in epoch 17 is 70.447\n","Test F1-score in epoch 17 is 69.966\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  18\n","[18,   100] loss: 0.022\n","Training loss in epoch 18 is 0.021\n","Training accuracy in epoch 18 is 99.423\n","Training precision in epoch 18 is 95.242\n","Training recall in epoch 18 is 99.828\n","Training F1-score in epoch 18 is 97.481\n","Test loss in epoch 18 is 0.486\n","Test accuracy in epoch 18 is 93.308\n","Test precision in epoch 18 is 69.180\n","Test recall in epoch 18 is 72.509\n","Test F1-score in epoch 18 is 70.805\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  19\n","[19,   100] loss: 0.022\n","Training loss in epoch 19 is 0.021\n","Training accuracy in epoch 19 is 99.337\n","Training precision in epoch 19 is 94.544\n","Training recall in epoch 19 is 99.828\n","Training F1-score in epoch 19 is 97.114\n","Test loss in epoch 19 is 0.523\n","Test accuracy in epoch 19 is 93.269\n","Test precision in epoch 19 is 69.728\n","Test recall in epoch 19 is 70.447\n","Test F1-score in epoch 19 is 70.085\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  20\n","[20,   100] loss: 0.021\n","Training loss in epoch 20 is 0.020\n","Training accuracy in epoch 20 is 99.375\n","Training precision in epoch 20 is 94.780\n","Training recall in epoch 20 is 99.914\n","Training F1-score in epoch 20 is 97.279\n","Test loss in epoch 20 is 0.500\n","Test accuracy in epoch 20 is 93.346\n","Test precision in epoch 20 is 69.667\n","Test recall in epoch 20 is 71.821\n","Test F1-score in epoch 20 is 70.728\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  21\n","[21,   100] loss: 0.017\n","Training loss in epoch 21 is 0.017\n","Training accuracy in epoch 21 is 99.490\n","Training precision in epoch 21 is 95.717\n","Training recall in epoch 21 is 99.914\n","Training F1-score in epoch 21 is 97.770\n","Test loss in epoch 21 is 0.495\n","Test accuracy in epoch 21 is 93.192\n","Test precision in epoch 21 is 68.387\n","Test recall in epoch 21 is 72.852\n","Test F1-score in epoch 21 is 70.549\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  22\n","[22,   100] loss: 0.016\n","Training loss in epoch 22 is 0.018\n","Training accuracy in epoch 22 is 99.433\n","Training precision in epoch 22 is 95.395\n","Training recall in epoch 22 is 99.742\n","Training F1-score in epoch 22 is 97.520\n","Test loss in epoch 22 is 0.491\n","Test accuracy in epoch 22 is 93.192\n","Test precision in epoch 22 is 68.506\n","Test recall in epoch 22 is 72.509\n","Test F1-score in epoch 22 is 70.451\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  23\n","[23,   100] loss: 0.018\n","Training loss in epoch 23 is 0.017\n","Training accuracy in epoch 23 is 99.510\n","Training precision in epoch 23 is 95.950\n","Training recall in epoch 23 is 99.828\n","Training F1-score in epoch 23 is 97.851\n","Test loss in epoch 23 is 0.574\n","Test accuracy in epoch 23 is 93.231\n","Test precision in epoch 23 is 69.896\n","Test recall in epoch 23 is 69.416\n","Test F1-score in epoch 23 is 69.655\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  24\n","[24,   100] loss: 0.015\n","Training loss in epoch 24 is 0.016\n","Training accuracy in epoch 24 is 99.587\n","Training precision in epoch 24 is 96.745\n","Training recall in epoch 24 is 99.656\n","Training F1-score in epoch 24 is 98.179\n","Test loss in epoch 24 is 0.561\n","Test accuracy in epoch 24 is 93.231\n","Test precision in epoch 24 is 69.896\n","Test recall in epoch 24 is 69.416\n","Test F1-score in epoch 24 is 69.655\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  25\n","[25,   100] loss: 0.014\n","Training loss in epoch 25 is 0.016\n","Training accuracy in epoch 25 is 99.577\n","Training precision in epoch 25 is 96.432\n","Training recall in epoch 25 is 99.914\n","Training F1-score in epoch 25 is 98.142\n","Test loss in epoch 25 is 0.535\n","Test accuracy in epoch 25 is 93.269\n","Test precision in epoch 25 is 69.863\n","Test recall in epoch 25 is 70.103\n","Test F1-score in epoch 25 is 69.983\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  26\n","[26,   100] loss: 0.017\n","Training loss in epoch 26 is 0.017\n","Training accuracy in epoch 26 is 99.471\n","Training precision in epoch 26 is 95.710\n","Training recall in epoch 26 is 99.742\n","Training F1-score in epoch 26 is 97.684\n","Test loss in epoch 26 is 0.569\n","Test accuracy in epoch 26 is 93.231\n","Test precision in epoch 26 is 69.896\n","Test recall in epoch 26 is 69.416\n","Test F1-score in epoch 26 is 69.655\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  27\n","[27,   100] loss: 0.018\n","Training loss in epoch 27 is 0.019\n","Training accuracy in epoch 27 is 99.471\n","Training precision in epoch 27 is 96.013\n","Training recall in epoch 27 is 99.398\n","Training F1-score in epoch 27 is 97.676\n","Test loss in epoch 27 is 0.520\n","Test accuracy in epoch 27 is 93.192\n","Test precision in epoch 27 is 69.257\n","Test recall in epoch 27 is 70.447\n","Test F1-score in epoch 27 is 69.847\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  28\n","[28,   100] loss: 0.017\n","Training loss in epoch 28 is 0.016\n","Training accuracy in epoch 28 is 99.567\n","Training precision in epoch 28 is 96.429\n","Training recall in epoch 28 is 99.828\n","Training F1-score in epoch 28 is 98.099\n","Test loss in epoch 28 is 0.548\n","Test accuracy in epoch 28 is 93.308\n","Test precision in epoch 28 is 69.966\n","Test recall in epoch 28 is 70.447\n","Test F1-score in epoch 28 is 70.205\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  29\n","[29,   100] loss: 0.016\n","Training loss in epoch 29 is 0.016\n","Training accuracy in epoch 29 is 99.558\n","Training precision in epoch 29 is 96.195\n","Training recall in epoch 29 is 100.000\n","Training F1-score in epoch 29 is 98.061\n","Test loss in epoch 29 is 0.546\n","Test accuracy in epoch 29 is 93.308\n","Test precision in epoch 29 is 69.966\n","Test recall in epoch 29 is 70.447\n","Test F1-score in epoch 29 is 70.205\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  30\n","[30,   100] loss: 0.016\n","Training loss in epoch 30 is 0.016\n","Training accuracy in epoch 30 is 99.567\n","Training precision in epoch 30 is 96.506\n","Training recall in epoch 30 is 99.742\n","Training F1-score in epoch 30 is 98.097\n","Test loss in epoch 30 is 0.559\n","Test accuracy in epoch 30 is 93.308\n","Test precision in epoch 30 is 69.966\n","Test recall in epoch 30 is 70.447\n","Test F1-score in epoch 30 is 70.205\n","\n","------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"STs-iLX_9qbY","colab_type":"code","colab":{}},"source":["import pickle\n","with open('val_glove.pickle','wb') as f:\n","  pickle.dump(val,f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2v2J_piwE4P","colab_type":"code","outputId":"cc710d45-738c-429a-8720-2d3321454175","executionInfo":{"status":"ok","timestamp":1580919251954,"user_tz":-330,"elapsed":1701,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["torch.save(cnn, 'model_glove_2.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CNN_Text. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]}]}