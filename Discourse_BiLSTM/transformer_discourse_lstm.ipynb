{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"discourse_lstm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","outputId":"137fa7dd-a96c-448f-ef51-75172440a6b0","executionInfo":{"status":"ok","timestamp":1582638992759,"user_tz":-330,"elapsed":25959,"user":{"displayName":"Shashank Gupta","photoUrl":"","userId":"02574039063615610097"}},"id":"UNZvp5E3EQ-7","colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M-wdomItEQqV","outputId":"b4563a3c-f0b0-4aa4-844a-24c6e3bcb5eb","executionInfo":{"status":"ok","timestamp":1582639001192,"user_tz":-330,"elapsed":34369,"user":{"displayName":"Shashank Gupta","photoUrl":"","userId":"02574039063615610097"}},"colab":{"base_uri":"https://localhost:8080/","height":721}},"source":["%cd drive/My Drive/NLP\n","!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n","\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 13.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 19.1MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 42.3MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=45231d71624793f86290672bc7a4af383b7bfb443b16e288cdc6cde60043a18a\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"feee0f9f-e686-4e41-bc99-56b3591c795b","executionInfo":{"status":"ok","timestamp":1582639006230,"user_tz":-330,"elapsed":39396,"user":{"displayName":"Shashank Gupta","photoUrl":"","userId":"02574039063615610097"}},"id":"chcSk9xNEP_E","colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["import gc\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import *\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["There is/are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1JJ-U5yzwdqX","colab_type":"code","colab":{}},"source":["TRAIN_DATAPATH = 'data/processed_train.npy'\n","MODELS = [#(BertModel,                           BertTokenizer,       'bert-base-uncased'),\n","          #(BertForSequenceClassification,       BertTokenizer,       'bert-base-uncased'),\n","          #(OpenAIGPTModel,                      OpenAIGPTTokenizer,  'openai-gpt'),\n","          #(GPT2Model,                           GPT2Tokenizer,       'gpt2'),\n","          #(CTRLModel,                           CTRLTokenizer,       'ctrl'),\n","          #(TransfoXLModel,                      TransfoXLTokenizer,  'transfo-xl-wt103'),\n","          (XLNetModel,                          XLNetTokenizer,      'xlnet-large-cased'),\n","          #(XLNetForSequenceClassification,      XLNetTokenizer,      'xlnet-base-cased'),\n","          #(XLMModel,                            XLMTokenizer,        'xlm-mlm-enfr-1024'),\n","          #(XLMForSequenceClassification,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n","          #(RobertaModel,                        RobertaTokenizer,    'roberta-base'),\n","          #(RobertaForSequenceClassification,    RobertaTokenizer,    'roberta-base'),\n","          #(XLMRobertaModel,                     XLMRobertaTokenizer, 'xlm-roberta-base'),\n","          #(XLMRobertaForSequenceClassification, XLMRobertaTokenizer, 'xlm-roberta-base'),\n","         ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6aralFdxgxT","colab_type":"code","colab":{}},"source":["class DiscourseDataset(Dataset):\n","  def __init__(self, corpus, tokenizer_class, pretrained_weights):\n","    self.corpus = corpus.reset_index()\n","    self.corpus['label'] = self.corpus['label'].astype(int)\n","    self.corpus['sentence'].dropna(inplace=True)\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","    self.weights = torch.tensor(self.corpus['label'].value_counts(normalize=True).tolist()).to(device)\n","\n","  def __len__(self):\n","    return(len(self.corpus))\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","    sent = self.corpus['sentence'][idx]\n","    args = self.corpus['args'][idx]\n","    enc_sent = torch.tensor(self.tokenizer.encode(sent, add_special_tokens=True, max_length=128)).to(device)\n","    enc_sent = F.pad(enc_sent, (0, 128 - enc_sent.shape[0])).type(torch.LongTensor).to(device)\n","    enc_args = [self.tokenizer.encode(a, add_special_tokens=False, max_length=32) for a in args]\n","    for enc_arg in enc_args:\n","      while len(enc_arg) < 32:\n","        enc_arg.append(0)\n","    # print(enc_args)\n","    enc_args = torch.tensor(enc_args, dtype=torch.long).to(device)\n","    # print(enc_args)\n","    enc_a = torch.zeros((8, 32), dtype=torch.long).to(device)\n","    if enc_args.shape[0] <= 8:\n","      enc_a[0:enc_args.shape[0], :] = enc_args\n","    else:\n","      enc_a = enc_args[0:8, :]\n","    label = torch.tensor(self.corpus['label'][idx], dtype=torch.long).to(device)\n","    return (enc_sent, enc_a, label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVk6qYas8171","colab_type":"code","colab":{}},"source":["class CustomModel(nn.Module):\n","  def __init__(self, model_class, pretrained_weights):\n","    super(CustomModel, self).__init__()\n","    self.transformer = model_class.from_pretrained(pretrained_weights, output_hidden_states=False, output_attentions=False)\n","    self.gru = nn.GRU(1024, 1024, batch_first=True, bidirectional=True)\n","    self.dropout = nn.Dropout(0.3)\n","    # self.lin_1 = nn.Linear(in_features=768, out_features=64)\n","    self.lin = nn.Linear(in_features=1024, out_features=2)\n","\n","  def forward(self, x, args):\n","    h = self.transformer(x)[0]\n","    h = torch.sum(h, dim=1) / 128 #index_select(h, dim=1, index=torch.tensor(127).to(device))\n","    h = h.repeat(2, 1, 1)\n","    arg_seq = torch.empty(x.shape[0], 8, 1024).to(device)\n","    for i in range(args.shape[0]):\n","      arg = torch.index_select(args, dim=0, index=torch.tensor(i).to(device)).squeeze()\n","      a = self.transformer(arg)[0]\n","      a = torch.sum(a, dim=1) / 32\n","      arg_seq[i, :, :] = a\n","    _, h = self.gru(arg_seq, h)\n","    print(h.shape)\n","    h = torch.sum(h, dim=1)\n","    x = self.dropout(h)\n","    x = self.lin(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHx-QZxavc55","colab_type":"code","colab":{}},"source":["def set_worker_seed(worker_id):\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57Mg2Jlh8AoQ","colab_type":"code","colab":{}},"source":["model_class, tokenizer_class, pretrained_weights = MODELS[0]\n","\n","# Loading the data and splitting it\n","\n","master_corpus = np.load(TRAIN_DATAPATH, allow_pickle=True)\n","master_corpus = pd.DataFrame(list(master_corpus))\n","# master_corpus['arg_len'] = [len(arg) for arg in master_corpus['args']]\n","# print(lsorted(master_corpus['arg_len'], reverse=True))\n","\n","train_corpus, test_corpus = train_test_split(master_corpus, random_state=seed_val, stratify=master_corpus['label'])\n","\n","train_dataset = DiscourseDataset(train_corpus, tokenizer_class, pretrained_weights)\n","test_dataset = DiscourseDataset(test_corpus, tokenizer_class, pretrained_weights)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, worker_init_fn=set_worker_seed)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, worker_init_fn=set_worker_seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5r3jzpyWLgXR","colab_type":"code","outputId":"a143d253-cd3a-4709-9962-555e2f41c850","executionInfo":{"status":"ok","timestamp":1582633113345,"user_tz":-330,"elapsed":3874,"user":{"displayName":"Shashank Gupta","photoUrl":"","userId":"02574039063615610097"}},"colab":{"base_uri":"https://localhost:8080/","height":325}},"source":["for obj in gc.get_objects():\n","  try:\n","    if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n","      del obj\n","  except:\n","    pass"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n","  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: astor.all_symbols is deprecated.  Please use astor.symbol_data.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: astor.treewalk is deprecated.  Please use astor.tree_walk.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: astor.codegen is deprecated.  Please use astor.code_gen.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Series.data is deprecated and will be removed in a future version\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: RangeIndex.data is deprecated and will be removed in a future version\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Index.data is deprecated and will be removed in a future version\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Int64Index.data is deprecated and will be removed in a future version\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"obsSQTKh2ZjI","colab_type":"code","outputId":"842dd964-3bfa-406e-c22d-6eaf7cd1e6fd","executionInfo":{"status":"error","timestamp":1582633127431,"user_tz":-330,"elapsed":10160,"user":{"displayName":"Shashank Gupta","photoUrl":"","userId":"02574039063615610097"}},"colab":{"base_uri":"https://localhost:8080/","height":468}},"source":["print('Memory Usage:')\n","print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","model = CustomModel(model_class, pretrained_weights).to(device)\n","criterion = nn.CrossEntropyLoss(weight=train_dataset.weights)\n","\n","\"\"\" For XLNet \n","  param_optimizer = list(model.named_parameters())\n","  no_decay = ['bias', 'gamma', 'beta']\n","  optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","  ]\n","  # This variable contains all of the hyperparemeter information our training loop needs\n","  optimizer = AdamW(optimizer_grouped_parameters,\n","                    lr=2e-5)\n","\"\"\"\n","\n","\"\"\" For BERT \"\"\"\n","optimizer = AdamW(model.parameters(),\n","                    lr = 1e-5, # args.learning_rate - default is 5e-5, best is 1e-5 so far\n","                    eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n","\n","\n","  # Number of training epochs (authors recommend between 2 and 4)\n","epochs = 20\n","# Total number of training steps is number of batches * number of epochs.\n","total_train_steps = len(train_loader) * epochs\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_train_steps)\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  total_loss = 0.0\n","  model.train()\n","\n","  train_preds = None\n","  train_labels = None\n","\n","  for i, data in enumerate(train_loader):\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","    optimizer.zero_grad()\n","    \n","    enc_sents, enc_args, labels = data\n","    outputs = model(enc_sents, enc_args)\n","    loss = criterion(outputs, labels)\n","      \n","    running_loss += loss.item()\n","    total_loss += loss.item()\n","\n","\n","    if train_preds is None or train_labels is None:\n","      train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","      train_labels = labels.cpu().numpy().flatten()\n","    else:\n","      train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","      train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n","\n","    # Clip the norm of the gradients to 1.0.\n","    # This is to help prevent the \"exploding gradients\" problem.\n","    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    if i % 100 == 99:    # print every 100 mini-batches\n","      print('[%d, %5d] loss: %.5f' % (epoch + 1, i + 1, running_loss / 100))\n","      running_loss = 0.0\n","    \n","  print(\"Training loss in epoch %d is %.5f\" % (epoch + 1, total_loss / len(train_loader)))\n","  print(\"Training accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(train_labels, train_preds) * 100))\n","  print(\"Training precision in epoch %d is %.5f\" % (epoch + 1, precision_score(train_labels, train_preds) * 100))\n","  print(\"Training recall in epoch %d is %.5f\" % (epoch + 1, recall_score(train_labels, train_preds) * 100))\n","  print(\"Training F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(train_labels, train_preds) * 100))\n","\n","  # Put the model in evaluation mode--the dropout layers behave differently\n","  # during evaluation.\n","  model.eval()\n","  # Tracking variables \n","  test_loss = 0.0\n","\n","  test_preds = None\n","  test_labels = None\n","\n","  with torch.no_grad():\n","    for data in test_loader:\n","      enc_sents, enc_args, labels = data\n","      outputs = model(enc_sents, enc_args)\n","      loss = criterion(outputs, labels)\n","      test_loss += loss.item()\n","      if test_preds is None or test_labels is None:\n","        test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","        test_labels = labels.cpu().numpy().flatten()\n","      else:\n","        test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","        test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n","\n","  print(\"Test loss in epoch %d is %.5f\" % (epoch + 1, test_loss / len(test_loader)))\n","  print(\"Test accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n","  print(\"Test precision in epoch %d is %.5f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n","  print(\"Test recall in epoch %d is %.5f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n","  print(\"Test F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Memory Usage:\n","Allocated: 15.2 GB\n","Cached:    15.2 GB\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-6cfa7e11ccc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Allocated:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cached:   '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 15.17 GiB already allocated; 1.88 MiB free; 15.20 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"95FH70jb2Y-E","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}