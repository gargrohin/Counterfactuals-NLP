{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "discourse_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzTjsk_Ftvn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jVF6Tlevy4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My Drive/NLP\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EFwaysvwR--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import *\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JJ-U5yzwdqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATAPATH = 'data/processed_train.npy'\n",
        "MODELS = [#(BertModel,                           BertTokenizer,       'bert-base-uncased'),\n",
        "          #(BertForSequenceClassification,       BertTokenizer,       'bert-base-uncased'),\n",
        "          #(OpenAIGPTModel,                      OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          #(GPT2Model,                           GPT2Tokenizer,       'gpt2'),\n",
        "          #(CTRLModel,                           CTRLTokenizer,       'ctrl'),\n",
        "          #(TransfoXLModel,                      TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
        "          #(XLNetModel,                          XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          #(XLNetForSequenceClassification,      XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          #(XLMModel,                            XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          #(XLMForSequenceClassification,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          (RobertaModel,                        RobertaTokenizer,    'roberta-large'),\n",
        "          #(RobertaForSequenceClassification,    RobertaTokenizer,    'roberta-base'),\n",
        "          #(XLMRobertaModel,                     XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "          #(XLMRobertaForSequenceClassification, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "         ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6aralFdxgxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscourseDataset(Dataset):\n",
        "  def __init__(self, corpus, tokenizer_class, pretrained_weights):\n",
        "    corpus.dropna(subset=['sentence'], inplace=True)\n",
        "    self.corpus = corpus.reset_index()\n",
        "    self.corpus['label'] = self.corpus['label'].astype(int)\n",
        "    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    self.weights = torch.tensor(self.corpus['label'].value_counts(normalize=True).tolist()).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return(len(self.corpus))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    sent = self.corpus['sentence'][idx]\n",
        "    args = self.corpus['args'][idx]\n",
        "    enc_sent = torch.tensor(self.tokenizer.encode(sent, add_special_tokens=True, max_length=128)).to(device)\n",
        "    enc_sent = F.pad(enc_sent, (0, 128 - enc_sent.shape[0])).type(torch.cuda.LongTensor)\n",
        "    enc_args = [self.tokenizer.encode(a, add_special_tokens=False, max_length=32) for a in args]\n",
        "    for enc_arg in enc_args:\n",
        "      while len(enc_arg) < 32:\n",
        "        enc_arg.append(0)\n",
        "    enc_args = torch.tensor(enc_args, dtype=torch.long).to(device)\n",
        "    enc_a = torch.zeros((8, 32), dtype=torch.long).to(device)\n",
        "    if enc_args.shape[0] <= 8:\n",
        "      enc_a[0:enc_args.shape[0], :] = enc_args\n",
        "    else:\n",
        "      enc_a = enc_args[0:8, :]\n",
        "    label = torch.tensor(self.corpus['label'][idx], dtype=torch.long).to(device)\n",
        "    return (enc_sent, enc_a, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVk6qYas8171",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(CustomModel, self).__init__()\n",
        "    self.transformer = model\n",
        "    self.gru = nn.GRUCell(1024, 1024)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lin = nn.Linear(in_features=1024, out_features=2)\n",
        "\n",
        "  def forward(self, x, args):\n",
        "    h = self.transformer(x)[1]\n",
        "    for i in range(args.shape[1]):\n",
        "      arg = torch.index_select(args, dim=1, index=torch.tensor(i).to(device))\n",
        "      arg = torch.squeeze(arg)\n",
        "      a = self.transformer(arg)\n",
        "      h = self.gru(a[1], h)\n",
        "    x = self.dropout(h)\n",
        "    x = self.lin(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHx-QZxavc55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_worker_seed(worker_id):\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Mg2Jlh8AoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = MODELS[0]\n",
        "\n",
        "# Loading the data and splitting it\n",
        "\n",
        "master_corpus = np.load(TRAIN_DATAPATH, allow_pickle=True)\n",
        "master_corpus = pd.DataFrame(list(master_corpus))\n",
        "# master_corpus['arg_len'] = [len(arg) for arg in master_corpus['args']]\n",
        "# print(lsorted(master_corpus['arg_len'], reverse=True))\n",
        "\n",
        "train_corpus, test_corpus = train_test_split(master_corpus, random_state=seed_val, stratify=master_corpus['label'])\n",
        "\n",
        "train_dataset = DiscourseDataset(train_corpus, tokenizer_class, pretrained_weights)\n",
        "test_dataset = DiscourseDataset(test_corpus, tokenizer_class, pretrained_weights)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, worker_init_fn=set_worker_seed)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, worker_init_fn=set_worker_seed)\n",
        "\n",
        "base_model = model_class.from_pretrained(pretrained_weights, output_hidden_states=False, output_attentions=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obsSQTKh2ZjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CustomModel(base_model)\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight=train_dataset.weights)\n",
        "\n",
        "\"\"\" For XLNet \n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "  ]\n",
        "  # This variable contains all of the hyperparemeter information our training loop needs\n",
        "  optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                    lr=2e-5)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" For BERT \"\"\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                    lr = 1e-5, # args.learning_rate - default is 5e-5, best is 1e-5 so far\n",
        "                    eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n",
        "\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 10\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_train_steps = len(train_loader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_train_steps)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  total_loss = 0.0\n",
        "  model.train()\n",
        "\n",
        "  train_preds = None\n",
        "  train_labels = None\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    enc_sents, enc_args, labels = data\n",
        "    outputs = model(enc_sents, enc_args)\n",
        "    loss = criterion(outputs, labels)\n",
        "      \n",
        "    running_loss += loss.item()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "\n",
        "    if train_preds is None or train_labels is None:\n",
        "      train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
        "      train_labels = labels.cpu().numpy().flatten()\n",
        "    else:\n",
        "      train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
        "      train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "      print('[%d, %5d] loss: %.5f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "      running_loss = 0.0\n",
        "    \n",
        "  print(\"Training loss in epoch %d is %.5f\" % (epoch + 1, total_loss / len(train_loader)))\n",
        "  print(\"Training accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training precision in epoch %d is %.5f\" % (epoch + 1, precision_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training recall in epoch %d is %.5f\" % (epoch + 1, recall_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(train_labels, train_preds) * 100))\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  test_loss = 0.0\n",
        "\n",
        "  test_preds = None\n",
        "  test_labels = None\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      enc_sents, enc_args, labels = data\n",
        "      outputs = model(enc_sents, enc_args)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      if test_preds is None or test_labels is None:\n",
        "        test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
        "        test_labels = labels.cpu().numpy().flatten()\n",
        "      else:\n",
        "        test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
        "        test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "  print(\"Test loss in epoch %d is %.5f\" % (epoch + 1, test_loss / len(test_loader)))\n",
        "  print(\"Test accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test precision in epoch %d is %.5f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test recall in epoch %d is %.5f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95FH70jb2Y-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"roberta-base-discourse-gru/model.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4zvNglKGJ0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "model = CustomModel(base_model)\n",
        "x = torch.load(\"roberta-base-discourse-gru/model.pkl\")\n",
        "model.load_state_dict(state_dict=x)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}