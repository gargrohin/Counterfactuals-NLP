{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hVjFTt4jRAgW",
        "outputId": "b0738d7e-5f23-4922-a380-6c9e78aaddee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vexauTnBQ-eq",
        "outputId": "4821b644-2429-445e-c483-b45b544df9fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!pip install transformers\n",
        "%cd drive/My\\ Drive/NLP"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n",
            "/content/drive/My Drive/NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKb4S7vqyepJ",
        "colab_type": "code",
        "outputId": "7aa0dda7-9f33-44b3-8a60-c4f95e682a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "from transformers import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "There is/are 1 GPU(s) available.\n",
            "Using GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vwMaus01WpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment the model(s) you want to use\n",
        "MODELS = [#(BertModel,                           BertTokenizer,       'bert-base-uncased'),\n",
        "          #(BertForSequenceClassification,       BertTokenizer,       'bert-large-uncased'),\n",
        "          #(OpenAIGPTModel,                      OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          #(GPT2Model,                           GPT2Tokenizer,       'gpt2'),\n",
        "          #(CTRLModel,                           CTRLTokenizer,       'ctrl'),\n",
        "          #(TransfoXLModel,                      TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
        "          #(XLNetModel,                          XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          #(XLNetForSequenceClassification,      XLNetTokenizer,      'xlnet-large-cased'),\n",
        "          #(XLMModel,                            XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          #(XLMForSequenceClassification,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          #(RobertaModel,                        RobertaTokenizer,    'roberta-base'),\n",
        "          (RobertaForSequenceClassification,    RobertaTokenizer,    'roberta-base'),\n",
        "          #(XLMRobertaModel,                     XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "          #(XLMRobertaForSequenceClassification, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "         ]\n",
        "DATAPATH = \"data/FC/full_train_1.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85NYnlk6Kcj",
        "colab_type": "text"
      },
      "source": [
        "# For the first sub-task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a495uYyLX4di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "  def __init__(self, corpus, tokenizer_class, pretrained_weights, max_len):\n",
        "    # Dropping NaN rows\n",
        "    corpus.dropna(subset=['Text'], inplace=True)\n",
        "    self.corpus = corpus.reset_index()\n",
        "    # Tokenising sentences\n",
        "    self.corpus = self.corpus.dropna(subset=['Text'])\n",
        "    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    self.corpus['Enc_Text'] = [self.tokenizer.encode_plus(sent, max_length=max_len, \n",
        "                                                      pad_to_max_length='right')['input_ids'] \n",
        "                           for sent in self.corpus['Text']]\n",
        "    # Weights for cross-entropy loss\n",
        "    self.weights = torch.tensor(self.corpus['Gold'].value_counts(normalize=True).tolist()).to(device)\n",
        "    print(self.corpus['Gold'].value_counts(normalize=True))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    S = self.corpus['Text'][idx]\n",
        "    X = torch.tensor(self.corpus['Enc_Text'][idx]).to(device)\n",
        "    y = torch.tensor(self.corpus['Gold'][idx]).to(device)\n",
        "    # Sample is sentence, embedding, gold label\n",
        "    sample = (S, X, y)\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi9-WYaUPTpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_worker_seed(worker_id):\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLoAWt5wdpzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "master_corpus = pd.read_csv(DATAPATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRPj8SbxwTOR",
        "colab_type": "code",
        "outputId": "e736705f-1a16-4441-85aa-b9f88d37c4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "master_corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Text</th>\n",
              "      <th>Gold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00001</td>\n",
              "      <td>Third Democratic presidential debate  Septembe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00002</td>\n",
              "      <td>On the policy front, Bernie Sanders claimed hi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.00003</td>\n",
              "      <td>Joe Biden misrepresented recent history when h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.00004</td>\n",
              "      <td>Here's a look at some of the assertions in the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.00005</td>\n",
              "      <td>It killed 22 people, and injured many more, we...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22170</th>\n",
              "      <td>590.00061</td>\n",
              "      <td>Contact transpo@gmu.edu with questions.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22171</th>\n",
              "      <td>590.00062</td>\n",
              "      <td>Campus Fire Safety Month  September is Campus ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22172</th>\n",
              "      <td>590.00063</td>\n",
              "      <td>Review the university's Fire Safety Plan, whic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22173</th>\n",
              "      <td>590.00064</td>\n",
              "      <td>Contact Meredith Muckerman at 703-993-9715 or ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22174</th>\n",
              "      <td>590.00065</td>\n",
              "      <td>Topics: Faculty/Staff Announcements</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22175 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Index                                               Text  Gold\n",
              "0        1.00001  Third Democratic presidential debate  Septembe...     0\n",
              "1        1.00002  On the policy front, Bernie Sanders claimed hi...     0\n",
              "2        1.00003  Joe Biden misrepresented recent history when h...     0\n",
              "3        1.00004  Here's a look at some of the assertions in the...     0\n",
              "4        1.00005  It killed 22 people, and injured many more, we...     0\n",
              "...          ...                                                ...   ...\n",
              "22170  590.00061            Contact transpo@gmu.edu with questions.     0\n",
              "22171  590.00062  Campus Fire Safety Month  September is Campus ...     0\n",
              "22172  590.00063  Review the university's Fire Safety Plan, whic...     0\n",
              "22173  590.00064  Contact Meredith Muckerman at 703-993-9715 or ...     0\n",
              "22174  590.00065                Topics: Faculty/Staff Announcements     0\n",
              "\n",
              "[22175 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBbDyT_loYta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus, test_corpus = train_test_split(master_corpus, stratify=master_corpus['Gold'])\n",
        "train_corpus.to_csv(\"data/FC/train_train_1.csv\", index=False)\n",
        "test_corpus.to_csv(\"data/FC/train_val_1.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUhh5kHXyL-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus = pd.read_csv(\"data/FC/train_train_1.csv\")\n",
        "test_corpus = pd.read_csv(\"data/FC/train_val_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2dA2Hht79CF",
        "colab_type": "code",
        "outputId": "87ed4cc9-7918-4a39-f90e-e86d2f133a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
        "  # Loading the data and splitting it\n",
        "  # master_corpus = master_corpus\n",
        "  # train_corpus, test_corpus = master_corpus, master_corpus\n",
        "\n",
        "  train_dataset = ClassificationDataset(train_corpus, tokenizer_class, pretrained_weights, max_len=512)\n",
        "  test_dataset = ClassificationDataset(test_corpus, tokenizer_class, pretrained_weights, max_len=512)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, worker_init_fn=set_worker_seed)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False, worker_init_fn=set_worker_seed)\n",
        "\n",
        "  model = model_class.from_pretrained(pretrained_weights, num_labels=2, output_hidden_states=False, output_attentions=False)\n",
        "  model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight=train_dataset.weights)\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "  epochs = 20\n",
        "\n",
        "  \"\"\"For XLNet\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "  ]\n",
        "  # This variable contains all of the hyperparemeter information our training loop needs\n",
        "  optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                    lr=1e-5)\n",
        "  \"\"\"\n",
        "  \"\"\" For BERT \"\"\"\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 1e-5, # args.learning_rate - default is 5e-5, 1e-5 worked best for me\n",
        "                    eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n",
        "\n",
        "  # Total number of training steps is number of batches * number of epochs.\n",
        "  total_train_steps = len(train_loader) * epochs\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_train_steps)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_preds = None\n",
        "    train_labels = None\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      _, inputs, labels = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs) # labels=b_labels)\n",
        "      loss = criterion(outputs[0], labels)\n",
        "      \n",
        "      running_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if train_preds is None or train_labels is None:\n",
        "        train_preds = np.argmax(outputs[0].detach().cpu().numpy(), axis=1).flatten()\n",
        "        train_labels = labels.cpu().numpy().flatten()\n",
        "      else:\n",
        "        train_preds = np.concatenate((train_preds, np.argmax(outputs[0].detach().cpu().numpy(), axis=1).flatten()))\n",
        "        train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "      # Clip the norm of the gradients to 1.0.\n",
        "      # This is to help prevent the \"exploding gradients\" problem.\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      if i % 100 == 99:    # print every 100 mini-batches\n",
        "        print('[%d, %5d] loss: %.5f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "        running_loss = 0.0\n",
        "    \n",
        "    print(\"Training loss in epoch %d is %.5f\" % (epoch + 1, total_loss / len(train_loader)))\n",
        "    print(\"Training accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(train_labels, train_preds) * 100))\n",
        "    print(\"Training precision in epoch %d is %.5f\" % (epoch + 1, precision_score(train_labels, train_preds) * 100))\n",
        "    print(\"Training recall in epoch %d is %.5f\" % (epoch + 1, recall_score(train_labels, train_preds) * 100))\n",
        "    print(\"Training F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(train_labels, train_preds) * 100))\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables \n",
        "    test_loss = 0.0\n",
        "\n",
        "    test_preds = None\n",
        "    test_labels = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "        _, inputs, labels = data\n",
        "        outputs = model(inputs) # labels=b_labels)\n",
        "        loss = criterion(outputs[0], labels)\n",
        "      \n",
        "        test_loss += loss.item()\n",
        "        if test_preds is None or test_labels is None:\n",
        "          test_preds = np.argmax(outputs[0].detach().cpu().numpy(), axis=1).flatten()\n",
        "          test_labels = labels.cpu().numpy().flatten()\n",
        "        else:\n",
        "          test_preds = np.concatenate((test_preds, np.argmax(outputs[0].detach().cpu().numpy(), axis=1).flatten()))\n",
        "          test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "    print(\"Test loss in epoch %d is %.5f\" % (epoch + 1, test_loss / len(test_loader)))\n",
        "    print(\"Test accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n",
        "    print(\"Test precision in epoch %d is %.5f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n",
        "    print(\"Test recall in epoch %d is %.5f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n",
        "    print(\"Test F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    0.927949\n",
            "1    0.072051\n",
            "Name: Gold, dtype: float64\n",
            "0    0.92803\n",
            "1    0.07197\n",
            "Name: Gold, dtype: float64\n",
            "[1,   100] loss: 0.07751\n",
            "[1,   200] loss: 0.03603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ZkSpHUQ1mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving model\n",
        "model.save_pretrained('fc-models/roberta-base-fc-32b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS8YelOP_Nff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlnet-base-cf-all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwzZcHbHK_OG",
        "colab_type": "code",
        "outputId": "a6a8feeb-171f-40c5-fd2a-e8a5decf60f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# To generate labels for final model\n",
        "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
        "  # Loading the data and splitting it\n",
        "  master_corpus = pd.read_csv(DATAPATH)\n",
        "  master_dataset = ClassificationDataset(master_corpus, tokenizer_class, pretrained_weights, max_len=512)\n",
        "  master_loader = torch.utils.data.DataLoader(master_dataset, batch_size=8, shuffle=False)\n",
        "  # criterion = nn.CrossEntropyLoss(weight=master_dataset.weights)\n",
        "\n",
        "  model.to(device)\n",
        "  \n",
        "  FP = []\n",
        "  FN = []\n",
        "\n",
        "  test_loss = 0\n",
        "  test_preds = None\n",
        "  test_labels = None\n",
        "  for i, data in enumerate(master_loader):\n",
        "    sent, inp, _ = data\n",
        "    outputs = model(inp)\n",
        "    # loss = criterion(outputs[0], labels)\n",
        "      \n",
        "    # test_loss += loss.item()\n",
        "    preds = np.argmax(outputs[0].detach().cpu().numpy(), axis=1).flatten()\n",
        "    # labels = labels.cpu().numpy().flatten()\n",
        "    if test_preds is None:# or test_labels is None:\n",
        "      test_preds = preds.copy()\n",
        "      # test_labels = labels.copy()\n",
        "    else:\n",
        "      test_preds = np.concatenate((test_preds, preds))\n",
        "      # test_labels = np.concatenate((test_labels, labels))\n",
        "\n",
        "    #for i in range(preds.shape[0]):\n",
        "    #  if preds[i].item() is 0 and labels[i].item() is 1:\n",
        "    #    FN.append(sent[i])\n",
        "    #  elif preds[i].item() is 1 and labels[i].item() is 0:\n",
        "    #    FP.append(sent[i])\n",
        "\n",
        "  #print(\"Test loss in epoch %d is %.5f\" % (1, test_loss / len(master_loader)))\n",
        "  #print(\"Test accuracy in epoch %d is %.5f\" % (1, accuracy_score(test_labels, test_preds) * 100))\n",
        "  #print(\"Test precision in epoch %d is %.5f\" % (1, precision_score(test_labels, test_preds) * 100))\n",
        "  #print(\"Test recall in epoch %d is %.5f\" % (1, recall_score(test_labels, test_preds) * 100))\n",
        "  #print(\"Test F1-score in epoch %d is %.5f\" % (1, f1_score(test_labels, test_preds) * 100))\n",
        "\n",
        "  np.save(\"xlnet-base-cf-all/xlnet-base-cf-all-preds.npy\", test_preds)\n",
        "  for i in FP:\n",
        "    print(i)\n",
        "  print()\n",
        "  for i in FN:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2cZqgtOjNM4",
        "colab_type": "code",
        "outputId": "e3a014df-e49b-4406-87fd-ca48b8708314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(FP))\n",
        "print(len(FN))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGQTsy0VYsfH",
        "colab_type": "code",
        "outputId": "698ee988-8988-47c0-8de7-890ea3c4b8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "print(FP[np.argmin([len(x) for x in FP])])\n",
        "print(FP[np.argmax([len(x) for x in FP])])\n",
        "print(FN[np.argmin([len(x) for x in FN])])\n",
        "print(FN[np.argmax([len(x) for x in FN])])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I thought that if I was just doing what the doctor said, I'd be fine.\n",
            "As part of his vetting in late 2014, in which he and his wife almost leased a home in the trendy Penn Quarter neighborhood of Washington D.C., Landon said he \"felt an obligation to help\" when he realized the sitting Fed governors were doing the jobs of two or three people. \"You wish there were a more certain path to the job,\" he said..\n",
            "Even if they could, I would still oppose them on moral grounds.\n",
            "It's become fashionable to tell a disability story in a hopeful arc, where the heroine may have moments of discouragement or fear, but comes out into full life at the end - into mainstream schools, love and romance, full participation in the social world, and these stories have become so pervasive that if they were to spread to aliens they'd find them familiar.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9XaLda-zHVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}