{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMSp6YuJB8Uvy5Wo6srLRL9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yi1b9Svhk-Qy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"0c158bb4-d1b9-4fdb-b74c-5a8708440e0b","executionInfo":{"status":"ok","timestamp":1580572539979,"user_tz":-330,"elapsed":23601,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WN6w7wL-b-eI","colab_type":"code","outputId":"05597630-7bdd-4b6e-871f-38d8817d7f48","executionInfo":{"status":"ok","timestamp":1580572543796,"user_tz":-330,"elapsed":1432,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/My\\ Drive/NLP/cnn-text-classification-pytorch/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP/cnn-text-classification-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1BWsbXx2sziz","colab_type":"code","outputId":"ab0dbb20-82d6-48ef-9c94-f585428671a2","executionInfo":{"status":"ok","timestamp":1580572548401,"user_tz":-330,"elapsed":5783,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["LICENSE   mydatasets.py  \u001b[0m\u001b[01;34mrt-polaritydata\u001b[0m/                \u001b[01;34msnapshot\u001b[0m/\n","main.py   \u001b[01;34m__pycache__\u001b[0m/   rt-polaritydata.README.1.0.txt  train.py\n","model.py  README.md      rt-polaritydata.tar             \u001b[01;34mtry1\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"33Gc00uhupOJ","colab_type":"code","colab":{}},"source":["import h5py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_6KujgZs7Jh","colab_type":"code","colab":{}},"source":["path_to_h5py = \"../sent-conv-torch/custom.hdf5\"\n","hf = h5py.File(path_to_h5py,'r')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueErB-UG3vAP","colab_type":"code","colab":{}},"source":["keys = list(hf.keys())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pv2wViTX34U8","colab_type":"code","colab":{}},"source":["w2v = hf['w2v']\n","data = hf['train']\n","label = hf['train_label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJm4dsGr4D1P","colab_type":"code","colab":{}},"source":["w2v = hf.get('w2v').value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilGkYDlR4U-s","colab_type":"code","colab":{}},"source":["\n","data = hf.get('train').value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrGQSFTg6iO3","colab_type":"code","colab":{}},"source":["label = hf.get('train_label').value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMJIORYt6rcO","colab_type":"code","colab":{}},"source":["max_len = 0\n","this = 0\n","for x in data:\n","  c = 0\n","  l = len(x) - 1\n","  while(x[l]==1 and l>=0):\n","    c+=1\n","    l-=1\n","  c = len(x) - c\n","  if c > max_len:\n","    max_len = c\n","    this = x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6rQ0MX59hT4","colab_type":"code","outputId":"0dbcbcde-be23-4ba9-f8ef-fd028cda5958","executionInfo":{"status":"ok","timestamp":1580332872597,"user_tz":-330,"elapsed":12352,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["max_len"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["551"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"c6jUjLBl-3R4","colab_type":"code","colab":{}},"source":["for i in range(data.shape[0]):\n","  for j in range(len(data[i])):\n","    data[i][j] = data[i][j] - 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3Ku-rfHDyDP","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-50Bz2nODYbd","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GI1RkrQG7nu2","colab_type":"code","colab":{}},"source":["from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UeKQ7CbDefO","colab_type":"code","colab":{}},"source":["ones = []\n","zeros = []\n","for i in range(len(label)):\n","  if label[i]==1:\n","    ones.append(i)\n","  else:\n","    zeros.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wg2KNVC2VBBp","colab_type":"code","colab":{}},"source":["ones = np.array(ones)\n","zeros = np.array(zeros)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJYBNw76VI3D","colab_type":"code","colab":{}},"source":["np.random.seed(0)\n","np.random.shuffle(ones)\n","np.random.shuffle(zeros)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4gOqsUBVIEd","colab_type":"code","colab":{}},"source":["np.random.seed(0)\n","\n","split = .8\n","train_indices = []\n","val_indices = []\n","count = 0\n","for i in range(len(ones)):\n","  if count<= split*len(ones)+1:\n","    train_indices.append(ones[i])\n","    count+=1\n","  else:\n","    val_indices.append(ones[i])\n","\n","count = 0\n","for i in range(len(zeros)):\n","  if count<= split*len(zeros) +1:\n","    train_indices.append(zeros[i])\n","    count+=1\n","  else:\n","    val_indices.append(zeros[i])  \n","\n","val_indices = np.array(val_indices)\n","train_indices = np.array(train_indices)\n","np.random.shuffle(val_indices)\n","np.random.shuffle(train_indices)\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kPsQBUW_lBd","colab_type":"code","outputId":"2e37b81c-5c2b-414e-a8bf-98a67cff8cfd","executionInfo":{"status":"ok","timestamp":1580572598689,"user_tz":-330,"elapsed":1332,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(train_indices)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10403"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Z9QmVAOU_rPu","colab_type":"code","outputId":"7dabf41b-b01e-4d7d-e86c-e55ecfd749f4","executionInfo":{"status":"ok","timestamp":1580572598693,"user_tz":-330,"elapsed":1086,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(val_indices)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2597"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"A97D79lm_sIw","colab_type":"code","colab":{}},"source":["class text_dataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, data, label):\n","        # Begin\n","        # self.train = train  # boolean to check if train or test\n","        # if train:\n","        #     self.root_dir = os.path.join(root_dir , 'trainval')\n","        #     labels_path = os.path.join(self.root_dir, 'labels2.txt')\n","        #     self.names = [(name.split(\" \")[0].strip() , name.split(\" \")[1].strip()) for name in open(labels_path)]\n","        #     self.names = sorted(self.names , key=lambda x: x[1])\n","        # else:\n","        #     self.root_dir = os.path.join(root_dir , 'test/VOCdevkit/VOC2007/')\n","        #     test_path = os.path.join(self.root_dir, 'list.txt')\n","        #     self.names = [name.strip() for name in open(test_path)]\n","        #     self.names = sorted(self.names , key=lambda x: x[1])\n","                    \n","        \n","        # self.transform = transform            # transforms to be done \n","\n","        self.data = data\n","        self.labels = label\n","\n","        \n","    def __len__(self):\n","        # Begin\n","        return len(self.labels)\n","        \n","    def __getitem__(self, idx):\n","\n","        l = torch.tensor(self.labels[idx]).long()\n","        sentence = torch.tensor(self.data[idx]).long()\n","\n","        return sentence, l\n","\n","        # Begin\n","        # if self.train:\n","        #     name = self.names[idx][0]\n","        #     label = classes.index(self.names[idx][1])\n","        #     image_path = os.path.join(self.root_dir , 'croped2/' + name)  # load image\n","        #     img = image_loader(image_path)\n","            \n","        #     if self.transform is not None:\n","        #         img = self.transform(img)  # apply transforms\n","            \n","        #     return img, label   # return image's tensor and label\n","\n","        \n","    # def test_iter(self, idx):    # only for Testing.\n","    #     '''This function loads test image with index = idx, and return's  it's path and the list of bounding boxes:\n","    #     [class, x1,y1,x2,y2]'''\n","        \n","    #     image_name = self.names[idx]\n","    #     annot_path = os.path.join(self.root_dir , 'Annotations')\n","    #     image_path = os.path.join(self.root_dir , 'Images/' + image_name)\n","    #     img = image_path\n","\n","    #     tree = ET.parse(os.path.join(annot_path , image_name.split('.')[0] + '.xml'))\n","    #     root = tree.getroot()\n","    #     id =0\n","    #     bounding_boxes=[]\n","    #     for child in root.iter('object'):\n","    #         label = []\n","    #         name = child.find('name').text\n","    #         if name != 'aeroplane' and name != 'chair' and name != 'bottle':\n","    #             continue\n","    #         label.append(classes.index(name))\n","    #         bndbox = child.find('bndbox')   # load bounding box\n","    #         label.append(int(bndbox.find('xmin').text))\n","    #         label.append(int(bndbox.find('ymin').text))\n","    #         label.append(int(bndbox.find('xmax').text))\n","    #         label.append(int(bndbox.find('ymax').text))\n","    #         bounding_boxes.append(np.array(label))\n","\n","    #     return img, np.array(bounding_boxes)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoMSOGKqDvps","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class CNN_Text(nn.Module):        \n","\n","    def __init__(self, args, weight_matrix):\n","        super(CNN_Text, self).__init__()\n","        self.args = args\n","        self.embed, num_embeddings, embedding_dim = self.create_emb_layer(weight_matrix, args.non_static)\n","\n","        V = num_embeddings\n","        D = embedding_dim\n","        C = args.class_num\n","        Ci = 1\n","        Co = args.kernel_num\n","        Ks = args.kernel_sizes\n","\n","        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n","        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n","        '''\n","        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n","        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n","        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n","        '''\n","        self.dropout = nn.Dropout(args.dropout)\n","        self.fc1 = nn.Linear(len(Ks)*Co, C)\n","    \n","    def create_emb_layer(self, weight_matrix, train_weights=False):\n","        num_embeddings, embedding_dim = weight_matrix.size()\n","        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n","        emb_layer.load_state_dict({'weight': weight_matrix})\n","        if train_weights:\n","            emb_layer.weight.requires_grad = True\n","        else:\n","            emb_layer.weight.requires_grad = False\n","\n","        return emb_layer, num_embeddings, embedding_dim\n","\n","    def conv_and_pool(self, x, conv):\n","        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n","        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.embed(x)  # (N, W, D)\n","        # print(x.size())\n","        \n","        if self.args.non_static == False:\n","            x = Variable(x)\n","\n","        x = x.unsqueeze(1)  # (N, Ci, W, D)\n","\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n","\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","\n","        x = torch.cat(x, 1)\n","\n","        '''\n","        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n","        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n","        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n","        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n","        '''\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-6-nHIRELWr","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import torch\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","\n","def train(train_iter, dev_iter, model, args):\n","    if args.cuda:\n","        model.cuda()\n","        print(\"On Cuda\")\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","    weights = [1,10]\n","    weights = torch.FloatTensor(weights).cuda()\n","    criterion = nn.CrossEntropyLoss(weight=weights)\n","\n","    steps = 0\n","    best_acc = 0\n","    last_step = 0\n","    to_return = {}\n","    max_so_far = 0.0\n","    for epoch in range(1, args.epochs+1):\n","        model.train()\n","        train_preds = None\n","        train_labels = None\n","        running_loss = 0.0\n","        total_loss = 0.0\n","        mini_batch = 0\n","        print(\"epoch: \",epoch)\n","        scheduler.step()\n","\n","        for feature, labels in train_iter:\n","            mini_batch+=1\n","            \n","            if args.cuda:\n","                feature, labels = feature.cuda(), labels.cuda()\n","\n","            optimizer.zero_grad()\n","            outputs = model(feature)\n","\n","            # print('outputs size', outputs.size())\n","            # print('target vector', target.size())\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            steps += 1\n","            # if steps % args.log_interval == 0:\n","                # corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","                # accuracy = 100.0 * corrects/args.batch_size\n","                # sys.stdout.write(\n","                #     '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n","                #                                                              loss.data, \n","                #                                                              accuracy,\n","                #                                                              corrects,\n","                #                                                              args.batch_size))\n","            if train_preds is None or train_labels is None:\n","              train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","              train_labels = labels.cpu().numpy().flatten()\n","            else:\n","              train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","              train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n","\n","\n","            loss.backward()\n","            optimizer.step()\n","            # scheduler.step()\n","\n","            if mini_batch % 100 == 0:    # print every 100 mini-batches\n","              print('[%d, %5d] loss: %.3f' % (epoch , mini_batch , running_loss / 100))\n","              running_loss = 0.0\n","            \n","            # if steps % args.test_interval == 0:\n","            #     dev_acc = eval(dev_iter, model, args)\n","            #     if dev_acc > best_acc:\n","            #         best_acc = dev_acc\n","            #         last_step = steps\n","            #         if args.save_best:\n","            #             save(model, args.save_dir, 'best', steps)\n","            #     else:\n","            #         if steps - last_step >= args.early_stop:\n","            #             print('early stop by {} steps.'.format(args.early_stop))\n","            # elif steps % args.save_interval == 0:\n","            #     save(model, args.save_dir, 'snapshot', steps)\n","        \n","        print(\"Training loss in epoch %d is %.3f\" % (epoch , total_loss / len(train_loader)))\n","        print(\"Training accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(train_labels, train_preds) * 100))\n","        print(\"Training precision in epoch %d is %.3f\" % (epoch , precision_score(train_labels, train_preds) * 100))\n","        print(\"Training recall in epoch %d is %.3f\" % (epoch , recall_score(train_labels, train_preds) * 100))\n","        print(\"Training F1-score in epoch %d is %.3f\" % (epoch, f1_score(train_labels, train_preds) * 100))\n","        \n","        eval(dev_iter, model, criterion, epoch, args, to_return, max_so_far)\n","        print()\n","        print(\"-\"*90)\n","        print()\n","        \n","    return to_return\n","\n","\n","def eval(data_iter, model, criterion, epoch, args, to_return, max_so_far):\n","    \n","    model.eval()\n","    test_loss = 0.0\n","    test_preds = None\n","    test_labels = None\n","    test_inputs = None\n","    # corrects, avg_loss = 0, 0\n","    with torch.no_grad():\n","\n","        for data in data_iter:\n","            inputs, labels = data\n","            if args.cuda:\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","          \n","            test_loss += loss.item()\n","            if test_preds is None or test_labels is None:\n","                test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n","                test_labels = labels.cpu().numpy().flatten()\n","                test_inputs = inputs.cpu().numpy()\n","            else:\n","                test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n","                test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n","                test_inputs = np.concatenate((test_inputs, inputs.cpu().numpy()), axis = 0)\n","\n","            # test_accuracy += flat_accuracy(outputs[0], labels)\n","    \n","    print(\"Test loss in epoch %d is %.3f\" % (epoch, test_loss / len(val_loader)))\n","    print(\"Test accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(test_labels, test_preds) * 100))\n","    print(\"Test precision in epoch %d is %.3f\" % (epoch, precision_score(test_labels, test_preds) * 100))\n","    print(\"Test recall in epoch %d is %.3f\" % (epoch, recall_score(test_labels, test_preds) * 100))\n","    print(\"Test F1-score in epoch %d is %.3f\" % (epoch, f1_score(test_labels, test_preds) * 100))\n","    if f1_score(test_labels, test_preds)*100 > max_so_far:\n","      max_so_far = f1_score(test_labels, test_preds)*100\n","      to_return['pred'] = test_preds\n","      to_return['label'] = test_labels\n","      to_return['inputs'] = test_inputs\n","\n","    # for feature, target in data_iter:\n","    #     # feature, target = batch.text, batch.label\n","    #     # feature.data.t_(), target.data.sub_(1)  # batch first, index align\n","    #     if args.cuda:\n","    #         feature, target = feature.cuda(), target.cuda()\n","\n","    #     logit = model(feature)\n","    #     loss = criterion(logit, target)\n","\n","    #     avg_loss += loss.data\n","    #     corrects += (torch.max(logit, 1)\n","    #                  [1].view(target.size()).data == target.data).sum()\n","\n","    # size = len(data_iter.dataset)*0.2\n","    # avg_loss /= size\n","    # accuracy = 100.0 * corrects/size\n","    # print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n","    #                                                                    accuracy, \n","    #                                                                    corrects, \n","    #                                                                    size))\n","\n","\n","    # return accuracy\n","\n","    # print(\"Val loss in epoch %d is %.3f\" % (epoch + 1, test_loss / len(test_loader)))\n","    # print(\"Val accuracy in epoch %d is %.3f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n","    # print(\"Val precision in epoch %d is %.3f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n","    # print(\"Val recall in epoch %d is %.3f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n","    # print(\"Val   F1-score in epoch %d is %.3f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))\n","\n","\n","def predict(text, model, text_field, label_feild, cuda_flag):\n","    assert isinstance(text, str)\n","    model.eval()\n","    # text = text_field.tokenize(text)\n","    text = text_field.preprocess(text)\n","    text = [[text_field.vocab.stoi[x] for x in text]]\n","    x = torch.tensor(text)\n","    x = autograd.Variable(x)\n","    if cuda_flag:\n","        x = x.cuda()\n","    print(x)\n","    output = model(x)\n","    _, predicted = torch.max(output, 1)\n","    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n","    return label_feild.vocab.itos[predicted.data[0]+1]\n","\n","\n","def save(model, save_dir, save_prefix, steps):\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    save_prefix = os.path.join(save_dir, save_prefix)\n","    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n","    torch.save(model.state_dict(), save_path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDSNmGqiEanf","colab_type":"code","outputId":"d7fb3dd6-ffc5-4573-ec25-e1381aa67791","executionInfo":{"status":"ok","timestamp":1580573529343,"user_tz":-330,"elapsed":1101,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["import os\n","import argparse\n","import datetime\n","import torch\n","# import torchtext.data as data\n","# import torchtext.datasets as datasets\n","# import model\n","# import train\n","# import mydatasets\n","\n","\n","parser = argparse.ArgumentParser(description='CNN text classificer')\n","# learning\n","parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n","parser.add_argument('-epochs', type=int, default=15, help='number of epochs for train [default: 256]')\n","parser.add_argument('-batch-size', type=int, default=32, help='batch size for training [default: 64]')\n","parser.add_argument('-log-interval',  type=int, default=1,   help='how many steps to wait before logging training status [default: 1]')\n","parser.add_argument('-test-interval', type=int, default=100, help='how many steps to wait before testing [default: 100]')\n","parser.add_argument('-save-interval', type=int, default=500, help='how many steps to wait before saving [default:500]')\n","parser.add_argument('-save-dir', type=str, default='try1', help='where to save the snapshot')\n","parser.add_argument('-early-stop', type=int, default=1000, help='iteration numbers to stop without performance increasing')\n","parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n","# data \n","parser.add_argument('-shuffle', action='store_true', default=True, help='shuffle the data every epoch')\n","# model\n","parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n","parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n","parser.add_argument('-embed-dim''''  ''', type=int, default=300, help='number of embedding dimension [default: 128]')\n","parser.add_argument('-kernel-num', type=int, default=100, help='number of each kind of kernel')\n","parser.add_argument('-kernel-sizes', type=str, default='3,4,5', help='comma-separated kernel size to use for convolution')\n","parser.add_argument('-non_static', action='store_true', default=False, help='fix the embedding')\n","# device\n","parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n","parser.add_argument('-no-cuda', action='store_true', default=False, help='disable the gpu')\n","# option\n","parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n","parser.add_argument('-predict', type=str, default=None, help='predict the sentence given')\n","parser.add_argument('-test', action='store_true', default=False, help='train or test')\n","args = parser.parse_args(args=[])\n","\n","\n","# load data \n","print(\"\\nLoading data...\")\n","# text_field = data.Field(lower=True)\n","# label_field = data.Field(sequential=False)\n","# train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n","dataset = text_dataset(data, label)\n","train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","                                           sampler = train_sampler)\n","val_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n","                                           sampler = valid_sampler)\n","\n","# train_iter, dev_iter, test_iter = sst(text_field, label_field, device=-1, repeat=False)\n","\n","\n","# update args and print\n","args.embed_num = weight_matrix.size()[1]\n","args.class_num = 2 #binary\n","args.cuda = (not args.no_cuda) and torch.cuda.is_available(); del args.no_cuda\n","args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n","args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n","\n","print(\"\\nParameters:\")\n","for attr, value in sorted(args.__dict__.items()):\n","    print(\"\\t{}={}\".format(attr.upper(), value))\n","print(\"Training sentences: \", int(len(dataset)*split))\n","print(\"Validation sentences: \", int(len(dataset)*(1-split)))\n"],"execution_count":73,"outputs":[{"output_type":"stream","text":["\n","Loading data...\n","\n","Parameters:\n","\tBATCH_SIZE=32\n","\tCLASS_NUM=2\n","\tCUDA=True\n","\tDEVICE=-1\n","\tDROPOUT=0.5\n","\tEARLY_STOP=1000\n","\tEMBED_DIM  =300\n","\tEMBED_NUM=300\n","\tEPOCHS=15\n","\tKERNEL_NUM=100\n","\tKERNEL_SIZES=[3, 4, 5]\n","\tLOG_INTERVAL=1\n","\tLR=0.001\n","\tMAX_NORM=3.0\n","\tNON_STATIC=False\n","\tPREDICT=None\n","\tSAVE_BEST=True\n","\tSAVE_DIR=try1/2020-02-01_16-12-10\n","\tSAVE_INTERVAL=500\n","\tSHUFFLE=True\n","\tSNAPSHOT=None\n","\tTEST=False\n","\tTEST_INTERVAL=100\n","Training sentences:  10400\n","Validation sentences:  2599\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04O8R9EVJSDJ","colab_type":"code","colab":{}},"source":["# model \n","cnn = CNN_Text(args, weight_matrix)\n","if args.snapshot is not None:\n","    print('\\nLoading model from {}...'.format(args.snapshot))\n","    cnn.load_state_dict(torch.load(args.snapshot))\n","\n","if args.cuda:\n","    torch.cuda.set_device(args.device)\n","    cnn = cnn.cuda()\n","        \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSvXvKkJROCO","colab_type":"code","outputId":"404c5c14-eada-434d-97fd-ef27a6c8bada","executionInfo":{"status":"ok","timestamp":1580574244368,"user_tz":-330,"elapsed":57688,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","# train or predict\n","# static\n","if args.predict is not None:\n","    label = predict(args.predict, cnn, text_field, label_field, args.cuda)\n","    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n","elif args.test:\n","    try:\n","        eval(test_iter, cnn, args) \n","    except Exception as e:\n","        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n","else:\n","    print()\n","    try:\n","        val = train(train_loader, val_loader, cnn, args)\n","    except KeyboardInterrupt:\n","        print('\\n' + '-' * 89)\n","        print('Exiting from training early')"],"execution_count":86,"outputs":[{"output_type":"stream","text":["\n","On Cuda\n","epoch:  1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1,   100] loss: 0.606\n","[1,   200] loss: 0.458\n","[1,   300] loss: 0.471\n","Training loss in epoch 1 is 0.503\n","Training accuracy in epoch 1 is 73.267\n","Training precision in epoch 1 is 26.151\n","Training recall in epoch 1 is 76.052\n","Training F1-score in epoch 1 is 38.919\n","Test loss in epoch 1 is 0.418\n","Test accuracy in epoch 1 is 86.292\n","Test precision in epoch 1 is 43.366\n","Test recall in epoch 1 is 75.779\n","Test F1-score in epoch 1 is 55.164\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  2\n","[2,   100] loss: 0.380\n","[2,   200] loss: 0.357\n","[2,   300] loss: 0.337\n","Training loss in epoch 2 is 0.354\n","Training accuracy in epoch 2 is 84.774\n","Training precision in epoch 2 is 41.389\n","Training recall in epoch 2 is 86.438\n","Training F1-score in epoch 2 is 55.976\n","Test loss in epoch 2 is 0.359\n","Test accuracy in epoch 2 is 84.521\n","Test precision in epoch 2 is 40.722\n","Test recall in epoch 2 is 85.813\n","Test F1-score in epoch 2 is 55.234\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  3\n","[3,   100] loss: 0.271\n","[3,   200] loss: 0.248\n","[3,   300] loss: 0.278\n","Training loss in epoch 3 is 0.266\n","Training accuracy in epoch 3 is 88.138\n","Training precision in epoch 3 is 48.415\n","Training recall in epoch 3 is 90.472\n","Training F1-score in epoch 3 is 63.076\n","Test loss in epoch 3 is 0.374\n","Test accuracy in epoch 3 is 89.488\n","Test precision in epoch 3 is 51.860\n","Test recall in epoch 3 is 77.163\n","Test F1-score in epoch 3 is 62.031\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  4\n","[4,   100] loss: 0.202\n","[4,   200] loss: 0.222\n","[4,   300] loss: 0.192\n","Training loss in epoch 4 is 0.201\n","Training accuracy in epoch 4 is 91.474\n","Training precision in epoch 4 is 57.240\n","Training recall in epoch 4 is 94.335\n","Training F1-score in epoch 4 is 71.248\n","Test loss in epoch 4 is 0.376\n","Test accuracy in epoch 4 is 89.911\n","Test precision in epoch 4 is 53.207\n","Test recall in epoch 4 is 77.509\n","Test F1-score in epoch 4 is 63.099\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  5\n","[5,   100] loss: 0.148\n","[5,   200] loss: 0.164\n","[5,   300] loss: 0.179\n","Training loss in epoch 5 is 0.165\n","Training accuracy in epoch 5 is 93.117\n","Training precision in epoch 5 is 62.705\n","Training recall in epoch 5 is 95.107\n","Training F1-score in epoch 5 is 75.580\n","Test loss in epoch 5 is 0.482\n","Test accuracy in epoch 5 is 92.722\n","Test precision in epoch 5 is 67.241\n","Test recall in epoch 5 is 67.474\n","Test F1-score in epoch 5 is 67.358\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  6\n","[6,   100] loss: 0.126\n","[6,   200] loss: 0.141\n","[6,   300] loss: 0.132\n","Training loss in epoch 6 is 0.131\n","Training accuracy in epoch 6 is 94.425\n","Training precision in epoch 6 is 67.484\n","Training recall in epoch 6 is 96.910\n","Training F1-score in epoch 6 is 79.563\n","Test loss in epoch 6 is 0.406\n","Test accuracy in epoch 6 is 90.412\n","Test precision in epoch 6 is 54.975\n","Test recall in epoch 6 is 76.471\n","Test F1-score in epoch 6 is 63.965\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  7\n","[7,   100] loss: 0.088\n","[7,   200] loss: 0.083\n","[7,   300] loss: 0.086\n","Training loss in epoch 7 is 0.088\n","Training accuracy in epoch 7 is 96.780\n","Training precision in epoch 7 is 78.117\n","Training recall in epoch 7 is 98.970\n","Training F1-score in epoch 7 is 87.315\n","Test loss in epoch 7 is 0.429\n","Test accuracy in epoch 7 is 91.336\n","Test precision in epoch 7 is 58.889\n","Test recall in epoch 7 is 73.356\n","Test F1-score in epoch 7 is 65.331\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  8\n","[8,   100] loss: 0.086\n","[8,   200] loss: 0.079\n","[8,   300] loss: 0.077\n","Training loss in epoch 8 is 0.082\n","Training accuracy in epoch 8 is 96.905\n","Training precision in epoch 8 is 78.969\n","Training recall in epoch 8 is 98.627\n","Training F1-score in epoch 8 is 87.710\n","Test loss in epoch 8 is 0.423\n","Test accuracy in epoch 8 is 90.951\n","Test precision in epoch 8 is 57.181\n","Test recall in epoch 8 is 74.394\n","Test F1-score in epoch 8 is 64.662\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  9\n","[9,   100] loss: 0.074\n","[9,   200] loss: 0.067\n","[9,   300] loss: 0.071\n","Training loss in epoch 9 is 0.070\n","Training accuracy in epoch 9 is 97.260\n","Training precision in epoch 9 is 80.769\n","Training recall in epoch 9 is 99.142\n","Training F1-score in epoch 9 is 89.017\n","Test loss in epoch 9 is 0.475\n","Test accuracy in epoch 9 is 92.491\n","Test precision in epoch 9 is 64.968\n","Test recall in epoch 9 is 70.588\n","Test F1-score in epoch 9 is 67.662\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  10\n","[10,   100] loss: 0.064\n","[10,   200] loss: 0.073\n","[10,   300] loss: 0.075\n","Training loss in epoch 10 is 0.070\n","Training accuracy in epoch 10 is 97.366\n","Training precision in epoch 10 is 81.351\n","Training recall in epoch 10 is 99.227\n","Training F1-score in epoch 10 is 89.404\n","Test loss in epoch 10 is 0.463\n","Test accuracy in epoch 10 is 92.414\n","Test precision in epoch 10 is 64.650\n","Test recall in epoch 10 is 70.242\n","Test F1-score in epoch 10 is 67.330\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  11\n","[11,   100] loss: 0.061\n","[11,   200] loss: 0.063\n","[11,   300] loss: 0.065\n","Training loss in epoch 11 is 0.063\n","Training accuracy in epoch 11 is 97.712\n","Training precision in epoch 11 is 83.514\n","Training recall in epoch 11 is 99.142\n","Training F1-score in epoch 11 is 90.659\n","Test loss in epoch 11 is 0.483\n","Test accuracy in epoch 11 is 92.530\n","Test precision in epoch 11 is 65.273\n","Test recall in epoch 11 is 70.242\n","Test F1-score in epoch 11 is 67.667\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  12\n","[12,   100] loss: 0.065\n","[12,   200] loss: 0.056\n","[12,   300] loss: 0.055\n","Training loss in epoch 12 is 0.058\n","Training accuracy in epoch 12 is 97.943\n","Training precision in epoch 12 is 85.092\n","Training recall in epoch 12 is 98.970\n","Training F1-score in epoch 12 is 91.508\n","Test loss in epoch 12 is 0.476\n","Test accuracy in epoch 12 is 92.068\n","Test precision in epoch 12 is 62.462\n","Test recall in epoch 12 is 71.972\n","Test F1-score in epoch 12 is 66.881\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  13\n","[13,   100] loss: 0.058\n","[13,   200] loss: 0.057\n","[13,   300] loss: 0.054\n","Training loss in epoch 13 is 0.057\n","Training accuracy in epoch 13 is 97.895\n","Training precision in epoch 13 is 84.375\n","Training recall in epoch 13 is 99.657\n","Training F1-score in epoch 13 is 91.381\n","Test loss in epoch 13 is 0.521\n","Test accuracy in epoch 13 is 92.761\n","Test precision in epoch 13 is 66.450\n","Test recall in epoch 13 is 70.588\n","Test F1-score in epoch 13 is 68.456\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  14\n","[14,   100] loss: 0.057\n","[14,   200] loss: 0.054\n","[14,   300] loss: 0.048\n","Training loss in epoch 14 is 0.052\n","Training accuracy in epoch 14 is 98.404\n","Training precision in epoch 14 is 87.927\n","Training recall in epoch 14 is 99.399\n","Training F1-score in epoch 14 is 93.312\n","Test loss in epoch 14 is 0.484\n","Test accuracy in epoch 14 is 92.414\n","Test precision in epoch 14 is 64.465\n","Test recall in epoch 14 is 70.934\n","Test F1-score in epoch 14 is 67.545\n","\n","------------------------------------------------------------------------------------------\n","\n","epoch:  15\n","[15,   100] loss: 0.049\n","[15,   200] loss: 0.052\n","[15,   300] loss: 0.043\n","Training loss in epoch 15 is 0.049\n","Training accuracy in epoch 15 is 98.270\n","Training precision in epoch 15 is 86.947\n","Training recall in epoch 15 is 99.485\n","Training F1-score in epoch 15 is 92.794\n","Test loss in epoch 15 is 0.482\n","Test accuracy in epoch 15 is 92.607\n","Test precision in epoch 15 is 65.595\n","Test recall in epoch 15 is 70.588\n","Test F1-score in epoch 15 is 68.000\n","\n","------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S15Ij_rxeR10","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"676d1ff8-39fd-4fd3-8b3c-a9b1a3f3c7a5","executionInfo":{"status":"ok","timestamp":1580573748235,"user_tz":-330,"elapsed":2567,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}}},"source":["val['inputs'].shape"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2597, 555)"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"4AMm2zTCiXq_","colab_type":"code","colab":{}},"source":["import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwNP0W7jJqmE","colab_type":"code","colab":{}},"source":["with open('val_set.pickle', 'wb') as f:\n","  pickle.dump(val, f )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xaH223FV5NK","colab_type":"code","outputId":"30d129ab-242e-4975-f0cc-8939c6e33136","executionInfo":{"status":"ok","timestamp":1580573924899,"user_tz":-330,"elapsed":4111,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["ls"],"execution_count":81,"outputs":[{"output_type":"stream","text":["LICENSE        README.md                       train.py\n","main.py        \u001b[0m\u001b[01;34mrt-polaritydata\u001b[0m/                \u001b[01;34mtry1\u001b[0m/\n","model.py       rt-polaritydata.README.1.0.txt  val_set.pickle\n","mydatasets.py  rt-polaritydata.tar\n","\u001b[01;34m__pycache__\u001b[0m/   \u001b[01;34msnapshot\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kVu4exJFSgp3","colab_type":"code","colab":{}},"source":["input = torch.randn(3, 5, requires_grad=True)\n","target = torch.empty(3, dtype=torch.long).random_(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sMRF5amSil4","colab_type":"code","outputId":"6f1374e7-c509-4109-e0e2-d85ca9802cdd","executionInfo":{"status":"ok","timestamp":1580318008404,"user_tz":-330,"elapsed":3123,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["input"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.0909e+00, -4.3674e-01, -1.5669e+00, -1.3722e+00,  2.7839e-01],\n","        [-3.6084e-01, -1.5469e-01, -3.0640e-04,  4.9972e-01,  1.5021e+00],\n","        [-8.6501e-01, -9.9743e-01, -8.3489e-01,  9.9229e-01,  6.1073e-01]],\n","       requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"oP0BW_qCSmnb","colab_type":"code","outputId":"28f5b045-d33f-43f4-a89c-6b22541a3c35","executionInfo":{"status":"ok","timestamp":1580318008407,"user_tz":-330,"elapsed":2957,"user":{"displayName":"sronin iitk","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCphT4jhrPBxWQA7llMy2jFYOE6znf8RIqDKiQr=s64","userId":"11615430726484299494"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["target"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 4, 1])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"89kW2PmISn63","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}