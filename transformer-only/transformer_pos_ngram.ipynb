{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_pos_ngram.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5W49kAaCLLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jczSOgyjyEhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "%cd drive/My\\ Drive/NLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKb4S7vqyepJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import *\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible. Somehow this isn't working!\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vwMaus01WpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS = [#(BertModel,                           BertTokenizer,       'bert-large-uncased'),\n",
        "          #(BertForSequenceClassification,       BertTokenizer,       'bert-base-uncased'),\n",
        "          #(OpenAIGPTMode/l,                      OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          #(GPT2Model,                           GPT2Tokenizer,       'gpt2'),\n",
        "          #(CTRLModel,                           CTRLTokenizer,       'ctrl'),\n",
        "          #(TransfoXLModel,                      TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
        "          #(XLNetModel,                          XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          #(XLNetForSequenceClassification,      XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          #(XLMModel,                            XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          #(XLMForSequenceClassification,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          (RobertaModel,                        RobertaTokenizer,    'roberta-large'),\n",
        "          #(RobertaForSequenceClassification,    RobertaTokenizer,    'roberta-base'),\n",
        "          #(XLMRobertaModel,                     XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "          #(XLMRobertaForSequenceClassification, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "         ]\n",
        "FIRST_DATAPATH = \"data/train_1.csv\"\n",
        "SECOND_DATAPATH = \"data/train_2.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85NYnlk6Kcj",
        "colab_type": "text"
      },
      "source": [
        "# For the first sub-task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a495uYyLX4di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "  def __init__(self, corpus, tokenizer_class, pretrained_weights, pos_vectorizer, ngram_vectorizer, max_len):\n",
        "    self.corpus = corpus.reset_index()\n",
        "    self.corpus['sentence'].dropna(inplace=True)\n",
        "    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    self.corpus['enc_sentence'] = [self.tokenizer.encode(sent, add_special_tokens=True, max_length=max_len) for sent in self.corpus['sentence']]\n",
        "    self.corpus['enc_sentence'] = pad_sequences(self.corpus['enc_sentence'], padding='post').tolist()\n",
        "    self.weights = torch.tensor(self.corpus['gold_label'].value_counts(normalize=True).tolist()).to(device)\n",
        "    self.pos_vectorizer = pos_vectorizer\n",
        "    self.ngram_vectorizer = ngram_vectorizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    S = self.corpus['sentence'][idx]\n",
        "    X = torch.tensor(self.corpus['enc_sentence'][idx]).to(device)\n",
        "    y = torch.tensor(self.corpus['gold_label'][idx]).to(device)\n",
        "    P = torch.tensor(self.pos_vectorizer.transform([self.corpus['pos_string'][idx]]).toarray(), dtype=torch.float32).flatten().to(device)\n",
        "    N = torch.tensor(self.ngram_vectorizer.transform([S]).toarray(), dtype=torch.float32).flatten().to(device)\n",
        "    sample = (X, y, P, N, S)\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNGHBUddT_J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(CustomModel, self).__init__()\n",
        "    self.transformer = model\n",
        "    \"\"\"self.conv_1 = nn.Conv2d(1, 4, 3, padding_mode='same')\n",
        "    self.pool_1 = nn.MaxPool2d(2, stride=2)\n",
        "    self.conv_2 = nn.Conv2d(4, 1, 3, padding_mode='same')\n",
        "    self.pool_2 = nn.MaxPool2d(2, stride=2)\n",
        "    self.lin_1 = nn.Linear(in_features=5700, out_features=64)\n",
        "    self.lin_2 = nn.Linear(in_features=64, out_features=2)\"\"\"\n",
        "    self.dropout_1 = nn.Dropout(0.3)\n",
        "    self.dropout_2 = nn.Dropout(0.3)\n",
        "    self.lin_1 = nn.Linear(in_features=3024, out_features=512)\n",
        "    self.lin_2 = nn.Linear(in_features=512, out_features=64)\n",
        "    self.lin_3 = nn.Linear(in_features=64, out_features=2)\n",
        "\n",
        "  def forward(self, x, p, n):\n",
        "    x = self.transformer(x)\n",
        "    \"\"\"\n",
        "    x = x[0].unsqueeze(1)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv_1(x))\n",
        "    x = self.pool_1(x)\n",
        "    x = F.relu(self.conv_2(x))\n",
        "    x = self.pool_2(x)\n",
        "    x = x.squeeze()\n",
        "    x = x.flatten(start_dim=1)\n",
        "    # x = self.dropout_1(x)\n",
        "    x = F.relu(self.lin_1(x))\n",
        "    x = self.dropout_2(x)\"\"\"\n",
        "    x = torch.cat((p, n, x[1]), dim=-1)\n",
        "    x = self.lin_1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout_1(x)\n",
        "    x = self.lin_2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout_2(x)\n",
        "    x = self.lin_3(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvcl2kGvo-TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_worker_seed(worker_id):\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2dA2Hht79CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = MODELS[0]\n",
        "\n",
        "# Loading the data and splitting it\n",
        "master_corpus = pd.read_csv(FIRST_DATAPATH, encoding='utf-8')\n",
        "# master_corpus['pred_label'] = master_corpus['gold_label']\n",
        "# submission = master_corpus[['sentenceID', 'pred_label']]\n",
        "# submission.to_csv('subtask1.csv', index=False)\n",
        "\n",
        "master_corpus['pos_list'] = [[pairs[1] for pairs in pos_tag(word_tokenize(sent))] for sent in master_corpus['sentence']]\n",
        "master_corpus['pos_string'] = [\" \".join(pos_list) for pos_list in master_corpus['pos_list']]\n",
        "pos_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=1000)\n",
        "pos_vectorizer.fit(master_corpus['pos_string'])\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=1000)\n",
        "ngram_vectorizer.fit(master_corpus['sentence'])\n",
        "\n",
        "# train_corpus, test_corpus = train_test_split(master_corpus, random_state=seed_val, stratify=master_corpus['gold_label'])\n",
        "train_corpus = pd.read_csv('train_train_1.csv', encoding='utf-8')\n",
        "test_corpus = pd.read_csv('train_va;_1.csv', encoding='utf-8')\n",
        "\n",
        "train_dataset = ClassificationDataset(train_corpus, tokenizer_class, pretrained_weights, pos_vectorizer, ngram_vectorizer, max_len=128)\n",
        "test_dataset = ClassificationDataset(test_corpus, tokenizer_class, pretrained_weights, pos_vectorizer, ngram_vectorizer, max_len=128)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, worker_init_fn=set_worker_seed)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True, worker_init_fn=set_worker_seed)\n",
        "\n",
        "base_model = model_class.from_pretrained(pretrained_weights, output_hidden_states=False, output_attentions=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDpFwjv6Bd5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CustomModel(base_model)\n",
        "model.cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight=train_dataset.weights)\n",
        "\n",
        "\"\"\" For XLNet \n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "  ]\n",
        "  # This variable contains all of the hyperparemeter information our training loop needs\n",
        "  optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                    lr=2e-5)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" For BERT \"\"\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                    lr = 1e-5, # args.learning_rate - default is 5e-5, best is 1e-5 so far\n",
        "                    eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n",
        "\n",
        "\n",
        "  # Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 20\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_train_steps = len(train_loader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_train_steps)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  total_loss = 0.0\n",
        "  model.train()\n",
        "\n",
        "  train_preds = None\n",
        "  train_labels = None\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "      \n",
        "    inputs, labels, pos_feats, ngram_feats, sentences = data\n",
        "    outputs = model(inputs, pos_feats, ngram_feats)\n",
        "    loss = criterion(outputs, labels)\n",
        "      \n",
        "    running_loss += loss.item()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if train_preds is None or train_labels is None:\n",
        "      train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
        "      train_labels = labels.cpu().numpy().flatten()\n",
        "    else:\n",
        "      train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
        "      train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "      print('[%d, %5d] loss: %.5f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "      running_loss = 0.0\n",
        "    \n",
        "  print(\"Training loss in epoch %d is %.5f\" % (epoch + 1, total_loss / len(train_loader)))\n",
        "  print(\"Training accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training precision in epoch %d is %.5f\" % (epoch + 1, precision_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training recall in epoch %d is %.5f\" % (epoch + 1, recall_score(train_labels, train_preds) * 100))\n",
        "  print(\"Training F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(train_labels, train_preds) * 100))\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  test_loss = 0.0\n",
        "\n",
        "  test_preds = None\n",
        "  test_labels = None\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      inputs, labels, pos_feats, ngram_feats, sentences = data\n",
        "      outputs = model(inputs, pos_feats, ngram_feats)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      if test_preds is None or test_labels is None:\n",
        "        test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
        "        test_labels = labels.cpu().numpy().flatten()\n",
        "      else:\n",
        "        test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
        "        test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n",
        "\n",
        "  print(\"Test loss in epoch %d is %.5f\" % (epoch + 1, test_loss / len(test_loader)))\n",
        "  print(\"Test accuracy in epoch %d is %.5f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test precision in epoch %d is %.5f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test recall in epoch %d is %.5f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n",
        "  print(\"Test F1-score in epoch %d is %.5f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCcjdZ75KGtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, \"roberta_pos_ngram.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}